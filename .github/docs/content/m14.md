# Módulo 14 - K-Means

> [voltar](../../../README.md) para a página anterior.

## Sumário

- [Módulo 14 - K-Means](#módulo-14---k-means)
  - [Sumário](#sumário)
  - [Bloco 1 - Introdução](#bloco-1---introdução)
    - [Aula 1 - Introdução](#aula-1---introdução)
      - [Transcrição](#transcrição)
    - [Aula 2 - O Que É Clusterização?](#aula-2---o-que-é-clusterização)
      - [Transcrição](#transcrição-1)
    - [Aula 3 - Um Passeio Pelos Algoritmos de Clusterização](#aula-3---um-passeio-pelos-algoritmos-de-clusterização)
      - [Transcrição](#transcrição-2)
  - [Bloco 2 - Conceito K-Means](#bloco-2---conceito-k-means)
    - [Aula 4 - O Que É O Algoritmo K-Means?](#aula-4---o-que-é-o-algoritmo-k-means)
      - [Transcrição](#transcrição-3)
    - [Aula 5 - Como Definir a Quantidade de Clusters](#aula-5---como-definir-a-quantidade-de-clusters)
      - [Transcrição](#transcrição-4)
  - [Bloco 3 - Demais Algoritmos para Algoritmos de Clusterização](#bloco-3---demais-algoritmos-para-algoritmos-de-clusterização)
    - [Aula 6 - Métricas de Algoritmos de Clusterização](#aula-6---métricas-de-algoritmos-de-clusterização)
      - [Transcrição](#transcrição-5)
  - [Bloco 4 - Projeto Prático](#bloco-4---projeto-prático)
    - [Aula 7 - Apresentação do Projeto Prático](#aula-7---apresentação-do-projeto-prático)
      - [Transcrição](#transcrição-6)
  - [Bloco 5 - Análise Exploratória de Dados](#bloco-5---análise-exploratória-de-dados)
    - [Aula 8 - Setup e Carga de Dados](#aula-8---setup-e-carga-de-dados)
      - [Transcrição](#transcrição-7)
    - [Aula 9 - EDA](#aula-9---eda)
      - [Transcrição](#transcrição-8)
    - [Aula 10 - Introdução ANOVA e Testes Estatísticos](#aula-10---introdução-anova-e-testes-estatísticos)
      - [Transcrição](#transcrição-9)
    - [Aula 11 - Teste de ANOVA](#aula-11---teste-de-anova)
      - [Transcrição](#transcrição-10)
  - [Bloco 6 - Treinamento do Modelo](#bloco-6---treinamento-do-modelo)
    - [Aula 12 - Preparação dos Dados para o KMeans](#aula-12---preparação-dos-dados-para-o-kmeans)
      - [Transcrição](#transcrição-11)
    - [Aula 13 - Executando o KMeans com Optuna](#aula-13---executando-o-kmeans-com-optuna)
      - [Transcrição](#transcrição-12)
  - [Bloco 7 - Análise do Resultados](#bloco-7---análise-do-resultados)
    - [Aula 14 - Analisando Resultados do K-Means](#aula-14---analisando-resultados-do-k-means)
      - [Transcrição](#transcrição-13)
    - [Aula 15 - Visualização de Decisões do KMeans](#aula-15---visualização-de-decisões-do-kmeans)
      - [Transcrição](#transcrição-14)
  - [Bloco 8 - Entrega do Modelo com App em Batch](#bloco-8---entrega-do-modelo-com-app-em-batch)
    - [Aula 16 - Aplicação Batch com Gradio](#aula-16---aplicação-batch-com-gradio)
      - [Transcrição](#transcrição-15)

> [Apresentação em slides](../assets/pdf/ppts_m14.pdf) do conteúdo teórico

## Bloco 1 - Introdução

### Aula 1 - Introdução

#### Transcrição

Fala Dev, começaremos agora uma nova fase da nossa trilha de A, explorando agora os algoritmos não supervisionados, o primeiro deles o Caminhos. Pra dar um refresh na questão de algoritmos não supervisionados, a ideia do algoritmo não supervisionado é que a gente possa extrair padrões dos dados que a gente está lançando para o algoritmo, ou seja, a gente não tem previamente um target a ser atingido, nosso objetivo é ou agrupar ou eventualmente reduzir a dimensionalidade dos dados através dos padrões que a gente captura desses dados, então a gente não faz um treinamento, separa os dados em treino e teste para poder avaliar se a nossa performance do modelo está boa frente a uma verdade absoluta que os dados nos trazem, então essa é uma das principais diferenças dos algoritmos supervisionados para os algoritmos não supervisionados. O objetivo desse módulo especificamente é que a gente vai apresentar conceitualmente os principais algoritmos de clusterização, ou seja, dentro dos algoritmos não supervisionados há uma série de classes de algoritmos, e a primeira que a gente vai explorar são os algoritmos de clusterização ou agrupamento, de forma que a gente possa desenvolver um projeto de machine learning que realize agrupamento de objetos ou de itens, e nós faremos um projeto explorando o primeiro desses algoritmos, que é o CAMINS, onde a gente vai fazer o processo completo, desde o EDA até a entrega do modelo através de uma aplicação para a inferência Batch. Aqui falar um pouco da agenda, do que a gente vai repassar nesse módulo, a gente vai falar primeiro do que é clusterização, fazer um passeio pelos principais algoritmos de clusterização, falar sobre o algoritmo CAMINS, falar de medidas de distância, que são conceitos importantes para a gente entender um pouco como funcionam os algoritmos de clusterização e até outros algoritmos de machine learning, falar de quais métricas a gente pode usar para validar ou avaliar a performance dos nossos algoritmos de clusterização, e aí vou falar um pouco sobre o projeto que a gente vai fazer usando e explorando esse algoritmo de CAMINS.

### Aula 2 - O Que É Clusterização?

#### Transcrição

E o que é um algoritmo de clusterização, o que são esses algoritmos? Eles são técnicas de aprendizado de máquina e de mineração de dados que agrupam um conjunto de dados em clusters ou grupos com base nas similaridades que esses itens possuem. Como pode ter visto aqui nesse gráfico aqui do lado, uma representação bem simplificada, a ideia aqui que neste plano cartesiano XY, imagina que a gente só tenha duas variáveis para clusterizar, que a gente consiga de uma certa forma, com base na similaridade desses objetos, definir agrupamentos ou clusters por conta dessas características comuns. Existem diversos tipos de algoritmos de classificação, eu vou passar conceitualmente por cada um deles, mas todos têm o mesmo objetivo fundamental, a gente identificar um padrão nos dados ou nas características dos dados e organizar esses dados em grupos significativos. Aqui quais são alguns usos que a gente pode fazer quando a gente trabalha com algoritmo de clusterização. Um dos usos que a gente pode fazer bastante frequente é o caso de segmentação de mercado ou segmentação de clientes, então em geral as empresas usam a clusterização para segmentar seus clientes com base em características demográficas, comportamentais ou de consumo dos seus clientes. Isso permite que elas possam direcionar estratégias de marketing que sejam mais eficazes e mais personalizadas para cada segmento de clientes. Um outro tipo de uso para algoritmo de clusterização é para análise de redes sociais, a gente pode ter uma rede social onde a gente pode identificar comunidades de usuários com interesses semelhantes e a gente também pode, com isso, personalizar o conteúdo ou recomendação para esses grupos ou essas comunidades de usuários. Um terceiro uso para um algoritmo de clusterização, dentre os diversos usos, é agrupamento de documentos, então imagina que você tem uma rotina de mineração de texto, usando processamento de linguagem natural, de forma que os algoritmos possam ser usados para agrupar documentos semelhantes, facilitando assim a organização e a recuperação de informações. Então esses são alguns dos usos, conceituando um pouco o que é clusterização e alguns dos usos de clusterização no mercado.

### Aula 3 - Um Passeio Pelos Algoritmos de Clusterização

#### Transcrição

Vamos fazer agora um passeio pelos principais algoritmos de clusterização, começando do algoritmo de particionamento. O algoritmo de particionamento divide os dados em um número pré-definido de clusters ou grupos, geralmente por meio da minimização de algum tipo de métrica de distância. Alguns exemplos de algoritmos de clustering de particionamento são o K-Means e o CAMEDOIDS. Para o clustering do tipo hierárquico, ele constrói uma hierarquia de clusters, onde os clusters menores são subconjuntos dos clusters maiores. Eles podem ser aglomerativos, onde eles começam com clusters individuais e vão se mesclando, ou divisivos, onde eles começam com um cluster único e vão se dividindo. Então eu tenho neste caso, clusters do tipo aglomerativos e do tipo divisivos. Eu tenho também clusters do tipo densidade, algoritmos de clustering de densidade, que eles identificam regiões densas de pontos e agrupam pontos dentro dessas regiões, adaptando-se à forma e ao tamanho dos clusters. Dois tipos que a gente conhece, o DBSCAN e o MEANSHIFT. A gente tem também um outro tipo de algoritmo de clusterização, que é distribuição, que ele modela a distribuição dos dados em clusters probabilísticos, geralmente assumindo que os dados são gerados a partir de uma mistura de distribuições probabilísticas. Um dos exemplos principais deste modelo, este tipo de modelo de algoritmo, é o GMM, que é o Gaussian Mixture Models. Temos também um outro tipo chamado clustering de grade, que ele divide o espaço de dados em células de uma grade e atribui pontos de dados nessas células correspondentes. Isso facilita a identificação de clusters em uma estrutura de grade. O principal exemplo desse tipo de algoritmo é o chamado CLARA, que na verdade é um acrônimo para Clustering Large Applications. Por último, existe também o tipo de partitioning de clusters chamado Particionamento Probabilístico, onde eles modelam os clusters como distribuições de probabilidade e atribuem pontos de dados a clusters com base nessas probabilidades. Um dos principais exemplos desse é o Fuzz C-Means. Explanando agora um pouco mais dos algoritmos propriamente ditos, o C-Means agrupa os dados em k-clusters, onde o k é definido pelo usuário, por exemplo, pelo cientista de dados. Ele atribui pontos de dados aos clusters com base na proximidade do que a gente chama de centroides. A gente vai entrar mais um detalhe nisso, já que esse módulo é focado no C-Means. E ele vai recalculando esses centroides iterativamente para minimizar a variância intra-cluster, ou seja, dentro de cada um dos clusters ou dos grupos. Já o K-Medoids, ele é uma variação do C-Means, que na verdade ele usa um conceito de medoid que é um dado real. Enquanto o centroide, na verdade, é um protótipo de dado, ele cria um ponto que pode ser um ponto, na verdade, um ponto no meio do plano cartesiano de N dimensões, ele cria um ponto ali que é virtual, o medoid é um dado real que eu tenho na base e ele é o representante do cluster. O cluster faz também o mesmo processo, ele vai atribuindo medoids inicialmente e vai reajustando para minimizar alguma função de dissimilaridade e ele é mais robusto ao tilássio. Falando do hierárquico aglomerativo, a ideia é que a gente começa considerando como cada ponto de dado como um cluster separado e a gente vai mesclando esses pontos de dados com os clusters mais próximos para formar uma hierarquia até que todos os pontos estejam em um único cluster. No ideal divisível, é o contrário, a gente começa considerando todos os pontos de dados como um único cluster e vai dividindo interativamente em subclusters menores até que cada ponto de dado esteja em seu próprio cluster individual. Vamos falar um pouco do DBSCAN, o DBSCAN, que é um dos tipos de algoritmos de densidade, ele agrupa pontos em regiões densas, identificando clusters de diversas formas e tamanhos. Os principais parâmetros deles são Y, que é a distância máxima entre pontos vizinhos, e minpts, que é o número mínimo de pontos para se formar um cluster. Outro algoritmo de densidade é o minshift, que busca os máximos locais da função de densidade de probabilidade para encontrar os centros dos clusters. Ele vai ajustando interativamente os centros dos clusters em direção às regiões de maior densidade de pontos, até convergir para os centros finais. Vamos falar agora de um tipo de algoritmo de clusterização de distribuição, o Gaussian Mixture Model, ou GMM. Ele assume que os pontos de dados são gerados a partir de uma mistura de várias distribuições gaussianas, ou distribuições normais. Ele estima os parâmetros dessas distribuições, como médias, covarianças e pesos, e assim modela os clusters usados. Por último, um exemplo de um algoritmo de clusterização, desta vez de GRAD, que é uma adaptação do Kamedoids para grandes datasets, que é o CLARA, que é o Clustering para Grandes Datasets. Ele usa uma amostrage para criar um subconjunto, aplica o Kamedoids nesse subconjunto e ajusta os clusters para os dados completos, mantendo apenas os Medoids ótimos, tornando o processo eficiente para grandes datasets.

## Bloco 2 - Conceito K-Means

### Aula 4 - O Que É O Algoritmo K-Means?

#### Transcrição

Agora eu vou falar um pouco do que é o algoritmo do K-Means. O K-Means, como comentei, é um algoritmo de clusterização do tipo particionamento que é amplamente utilizado em análise de dados e aprendizado de máquina. Ele agrupa dados em k-clusters, que são definidos pelos cientistas de dados, e o algoritmo segue os seguintes passos, a seguinte estrutura. Primeiro ele inicia os centroides, que são esses pontos virtuais dentro de um espaço dimensional, e ele coloca aleatoriamente esses pontos como centroides centrais. Esses centroides são os representantes do cluster. Então imagina que você vai ter três clusters, e ele vai definir nesse plano dimensional três pontos aleatórios como centroides iniciais. Depois que ele faz isso, ele começa a atribuir os pontos a cada um desses clusters com base em uma medida de distância, geralmente a distância euclidiana. Eu vou falar um pouco mais sobre distância, sobre os tipos de distância. Mas a ideia é que cada centroide começa a atrair os pontos que estão mais próximos dele com base em uma medida de distância. Depois que isso é feito, os centroides dos clusters são recalculados como a média de todos os pontos atribuídos nesse cluster. Então agora que eu tenho os pontos que foram atribuídos com base nesses clusters virtuais distribuídos aleatoriamente, eles começam a recalcular novamente cada um desses centroides. Com base nesse recálculo, é feita uma reatribuição dos pontos com base nesses novos centroides. Ou seja, quais pontos agora estão próximos, considerando uma medida de distância, desses novos centroides. Isso é feito, tanto o passo 3, de recalcular os centroides, quanto o passo 4, que reatribui os pontos, eles são repetidos até que não haja mudanças significativas na atribuição dos pontos aos clusters ou então se um número máximo de interações for alcançado, que é um hiperparâmetro do modelo. Então se ele começa a perceber, se o algoritmo percebe que não está tendo muita mudança entre os pontos estarem se movendo de um cluster para outro, o algoritmo para. Ou você pode determinar a quantidade máxima de interações que aquele algoritmo vai fazer. O algoritmo converge quando os centroides não mudam significativamente ou quando atinge a quantidade máxima de interações. No entanto, o resultado final pode variar dependendo da inicialização aleatória dos centroides e do número de clusters escolhidos. Para ter uma ideia mais visual, eu trouxe dois charts, um mostrando de forma passo a passo e um mostrando, na verdade, uma animação. Então no primeiro passo, imaginando que eu tenho um dataset com dois parâmetros, ou seja, eu tenho um plano cartesiano de X e Y, tenho aqui alguns pontos, que são os pontos de dados que eu tenho no meu dataset. Com base nesses dois pontos, ele determina, ele aleatoriamente distribui, três centroides. Imagina que a gente colocou um K igual a 3, que é o que está aqui, nesse caso escolhemos o K igual a 3, então atribuímos três centroides aqui no meio desse plano. No primeiro passo, o que ele vai fazer? Ele vai calcular a medida de distância de cada um desses pontos com esses centroides que foram determinados e vai fazer uma definição inicial de cada ponto desses a qual centroide ou cluster ele pertence. Feito isso, ele recalcula os centroides, já que agora eu tenho os pontos aproximados, e define um novo centroide. Com base nesse novo centroide, ele recalcula os pontos para avaliar em que cluster esses pontos vão fazer parte. Isso vai sendo feito interativamente, até que ele perceba que não está havendo mais mudança de cluster entre os pontos, ou se chegar a um limite máximo de interações. Nesse caso, ele percebeu que não houve mais mudanças significativas e conseguiu atribuir os três clusters de forma independente. Eu tenho aqui um cluster vermelho, onde o centroide está aqui, um cluster verde, onde o centroide está aqui, e um cluster roxo, onde o centroide está aqui. Além disso, eu queria mostrar uma visão animada dessa informação. Então, ele está atribuindo inicialmente alguns clusters de forma aleatória, o algoritmo começa a recalcular os centroides e com isso atualizar os pontos de dados até que o modelo conclua, até que ele perceba que não há mais mudanças ou que as interações acabaram, e aí ele determina quais pontos fazem parte de cada um dos clusters, dos k clusters que foram definidos. Então, aqui é mais para ter uma visão mais visual e animada do que o algoritmo está fazendo. Aí tem um ponto aqui que é como é que a gente pode determinar a quantidade de clusters. Então, vou explicar isso aqui no próximo vídeo.

### Aula 5 - Como Definir a Quantidade de Clusters

#### Transcrição

Como é que a gente faz então para definir o parâmetro K, ou seja, a quantidade de clusters que a gente quer dividir e usar no nosso algoritmo de caminhos? Existem alguns métodos de mercado, eu vou trazer uma explicação deles para vocês. O primeiro deles é o método do cotovelo, ou Elbow Method, que envolve plotar o valor da função de custo, por exemplo, a soma dos quadrados das distâncias intra-cluster, ou seja, as distâncias que existem entre os pontos dentro de um mesmo cluster, em relação ao número de clusters, e aí observar quando ocorre uma mudança significativa na inclinação da curva. O ponto onde a curva começa a se nivelar é frequentemente escolhido como o número ideal de clusters. Um outro método que a gente pode utilizar é o método da silhueta, ou Silhouette Method. Esse método avalia a qualidade dos clusters formados por diferentes valores de K. Ele calcula a silhueta média, e essa silhueta eu vou explicar, é uma medida, uma métrica de validação de clusters, de todos os pontos de dados em cada cluster, para diferentes valores de K. E aí ele escolhe o valor de K que maximiza a média da silhueta. Um outro método que pode ser usado é o chamado método Gap Statistics. Esse método compara a dispersão intra-cluster, novamente, quando a gente fala de intra-cluster são os pontos dentro de um mesmo cluster, para diferentes valores de K, como a dispersão esperada assume um modelo de referência nulo, por exemplo, dados aleatórios. O número ideal de clusters é aquele que maximiza a lacuna estatística entre as duas expressões. Um outro método é a validação externa. Você pode ter, muitas vezes, acesso a informações externas sobre os dados que indicam o número correto de clusters. Então, olha, vou, por exemplo, classificar clientes dentro de um banco. Já há, às vezes, uma pré-definição de quais são esses clusters, então você não precisa, na verdade, fazer um teste para determinar isso, porque essa informação você já tem com os especialistas do negócio. Então, geralmente, você pode ter um método de validação externa, por exemplo, o índice de validação ou comparação com rótulos conhecidos, para que você possa usar para determinar o número ideal de clusters. Por último, você pode também usar o conhecimento de domínio. Às vezes, o conhecimento prévio de domínio ajuda a determinar o número de clusters de forma mais precisa. Por exemplo, em ciências sociais, pode-se ter um conhecimento prévio sobre o número de grupos ou de categorias existentes que vão ser aplicados naquele específico dataset.

## Bloco 3 - Demais Algoritmos para Algoritmos de Clusterização

### Aula 6 - Métricas de Algoritmos de Clusterização

#### Transcrição

Agora antes de falar do projeto, terminar essa parte teórica falando das métricas dos algoritmos de clusterização ou pelo menos as principais métricas. A primeira delas o que a gente chama de índice de silhueta que eu comentei anteriormente que pode ser uma estratégia para se definir a quantidade de clusters de um algoritmo. Ela avalia a coesão intra-cluster e a separação inter-cluster, o que quer dizer isso? Ela avalia o quanto os pontos dentro de um cluster estão juntos e o quanto que cada um dos clusters está separado um do outro dentro desse plano dimensional. Uma pontuação alta indica que os pontos estão bem agrupados dentro de seus clusters e mal conectados aos clusters vizinhos, ou seja, eles estão muito unidos dentro do cluster mas os clusters estão separados, muito bem separados dos outros clusters. Um outro índice é o chamado índice Davis-Bowding ou DB que ele mede a dispersão dentro de cada cluster em relação a separação entre clusters. Quanto menor o valor, melhor a separação entre os clusters. Um outro índice de Kalinske-Harabass ou CH calcula a relação entre a dispersão intra-cluster e a dispersão entre clusters. Ele pontua mais alto para clusters mais densos e mais bem separados. Um outro índice é o índice Dunn que é a razão entre a menor distância inter-cluster e a maior distância intra-cluster. Ele pontua mais alto para clusters mais compactos e mais distantes uns dos outros. Um outro tipo de métrica é o índice Range Ajustado que é uma comparação entre os rótulos atribuídos pelos algoritmos de classificação e os rótulos verdadeiros quando os mesmos estão disponíveis. Então se você, na verdade, está querendo confrontar com um algoritmo não supervisionado uma informação que você já tem, você usa esse índice de Range Ajustado. E o índice de Validade Interna que ele consiste em várias medidas internas como compacidade e separação dos clusters para avaliar a qualidade da clusterização sem rótulos verdadeiros.

## Bloco 4 - Projeto Prático

### Aula 7 - Apresentação do Projeto Prático

#### Transcrição

E agora falar um pouco do projeto que a gente vai fazer para explorar esse algoritmo do Camins e não só o algoritmo, mas explorar alguns conceitos realmente de algoritmos não supervisionados de classificação. Aqui a gente vai trazer o mesmo exemplo, o mesmo dataset que a gente trabalhou no módulo de árvore de decisão, porém com uma abordagem diferente. Então, recapitulando, a gente tem uma empresa lá de concessão de crédito que tem um catálogo, uma base de dados dos clientes com as informações da idade, o faturamento mensal, nível de inovação e entre outras informações. E a ideia é que essa empresa quer dar um atendimento mais apropriado e dessa forma quer agrupar esses clientes conforme essas características. Então, ele não tem a priori quais são os segmentos de mercado, os segmentos de clientes. Eles querem descobrir isso, descobrir esses padrões, esses dados e obter esses clusters através de um algoritmo não supervisionado. Então, de forma que seja possível classificar os novos clientes, a gente vai construir um algoritmo de clusterização para agrupar esses clientes de segmentos com base nas informações disponíveis sobre cada um desses clientes. O que é importante mencionar quando a gente fala de projetos de algoritmos supervisionados, é que muitas vezes os projetos focados nisso, em algoritmos não supervisionados, podem ser um ponto de partida para depois você trabalhar num projeto de algoritmo supervisionado. Então, imagina uma empresa como essa, ela não possui ainda a priori nenhum tipo de classificação que ela poderia gerar para esses clientes. Ela poderia, por exemplo, criar um algoritmo não supervisionado, definir os clusters, avaliar o quanto que esses clusters estão funcionando em termos de ações de marketing ou atendimento personalizado, e se aquilo estiver funcionando, usar esses rótulos que foram definidos num algoritmo não supervisionado e aí começar a aplicar um algoritmo supervisionado de classificação, usando como base esses clusters que foram definidos anteriormente. Então, esse é um uso, diria, bastante frequente quando você não tem ainda os rótulos definidos, você aplica um algoritmo que vai entender muito bem os padrões dos dados, a estrutura, como eu posso agrupar esses dados, e aí se as suas estratégias estiverem funcionando para aquilo que foi definido como cluster, você passa a empregar um algoritmo supervisionado. A estrutura do projeto, ela vai seguir uma lógica muito parecida com as anteriores, mas com a diferença de que, como isso aqui é um algoritmo supervisionado, a gente não precisa fazer uma separação dos dados entre treino e teste, porque a gente não tem o dado verdadeiro para confrontar esse dataset que a gente vai trabalhar aqui, ele não tem o rótulo do segmento, a gente vai dar o rótulo conforme cada cluster, cada ponto de dados, em cada cluster que esse ponto de dados for ficar. Então, a gente vai carregar os dados, vamos fazer um EDA diferente do que a gente já fez, trabalhar alguns outros conceitos EDA para não repetir o que já foi feito no módulo de árvore de decisão, vamos treinar esse modelo, validar esse modelo e aí fazer alguns ajustes de hiperparâmetros, com base nessas validações, vamos salvar o modelo e vamos entregar esse modelo via aplicação Batch. Então, chega de conceitos e vamos para o código.

## Bloco 5 - Análise Exploratória de Dados 

### Aula 8 - Setup e Carga de Dados

#### Transcrição

Fala Dev, vamos começar agora o nosso projeto prático dentro desse módulo de clusterização, falando do algoritmo K-Means. Eu já estou com o meu ambiente pronto aqui dentro do Visual Code, onde eu tenho aqui uma pasta com o meu data set, que é o mesmo data set que a gente usou de cadastro de clientes PJ, só que porém sem a coluna target que a gente tinha, que era o segmento, que é a coluna que a gente vai aqui, no caso, trabalhar ela no módulo, no algoritmo não supervisionado. Nosso objetivo, de novo, é que a gente possa, com base nas características desses clientes, a gente poder agrupar esses clientes em clusters ou grupos, para dar um atendimento personalizado para cada um desses grupos. Vou criar aqui, primeiramente, um arquivo para que a gente possa ter o nosso Jupyter Notebook, vou chamar ele de clusterização-clientes.ipynb. Opa, tem um Y a mais. Vou nomear aqui, ipynb. Pronto. E aí, para iniciar, vou colocar aqui quais são as bibliotecas que a gente vai precisar instalar. Elas já estão instaladas no meu ambiente, mas eu já vou deixar aqui pronto, aqui dentro do notebook. Então, não tem nada diferente do que a gente já tenha usado anteriormente dentro dos nossos projetos. A gente está usando Pandas, Plotly, Scikit-Learn, Optuna, para otimizar hiperparâmetros, Wi-Fi Kernel, para que a gente possa ter o ambiente de Jupyter funcionando, NBformat também, para alguns gráficos, o Matplotlib, o Pinguim, que a gente vai trabalhar com alguns testes estatísticos, e o Gradle, que vai ser a ferramenta que a gente vai usar, a aplicação que a gente vai criar, para poder subir um arquivo e ele devolver um arquivo para a gente, com os clusters definidos para cada um dos clientes. Ok? Então, feito isso, a gente já pode começar a fazer o import das bibliotecas. Então, em termos de bibliotecas, vamos pensar se de EDA e visualização de dados. A gente vai fazer uma visualização de dados depois do treinamento do modelo. Eu vou importar aqui o Pandas, vou importar o Plotly, Express SPX, e aqui eu vou ter uma dica, que muitas vezes a gente precisa visualizar informações numéricas, valores contínuos, com valores muito pequenos, e muitas vezes o Pandas acaba transformando, não o Pandas, mas a forma que a informação é mostrada, acaba sendo num formato científico. Então, aqui tem uma dica que eu vou compartilhar, que é como mostrar os valores no formato float mesmo, sem ser o formato científico. Então, a gente coloca pd.setoptions, estou setando uma opção do Pandas, essa opção chama-se display.floatformat, e aqui a gente coloca lambda, x, é uma função virtual, e aí eu coloco aqui qual que é o formato que eu quero. Então, para cada vez que eu tiver que mostrar uma informação, ele transforma essa informação em x, aplicando esse formato com duas casas decimais, é o que está sendo escolhido aqui. Para a parte de ML, a gente vai importar o sklearn.cluster, é um pacote, e a gente vai importar o caminho. A gente vai importar algumas métricas, e aí são métricas diferentes do que a gente está habituado a trabalhar, são métricas dos algoritmos que estão supervisionados, especificamente dos algoritmos de clusterização. Então, a gente vai usar aqui o silhouette.score, e vamos usar o pairwise.distance. Vamos usar também um standard scaler para poder normalizar os dados numéricos que a gente tem, que estão em escalas diferentes. Então, a gente vai usar um standard scaler, deixar todos na mesma escala, one hot encoder, por conta das variáveis categóricas, e um original encoder por conta da variável inovação, que é uma variável original. Quanto maior o nível de inovação que tiver aqui, maior a grandeza dessa empresa nesse quesito. E vamos usar aqui um column transformer, que a gente vai aplicar antes do k-means. Então, a gente vai usar aqui, compose, import, column, transform. E vamos usar aqui para a parte de otimização, de hiperparâmetros, a gente vai usar o Optuna. Mais para frente, a gente vai importar mais alguns outros para a COT, para a parte principalmente de estatística, o pinguim, mas eu vou importar ele, os módulos específicos, quando for o momento. Então, a gente pode já rodar isso aqui. Vamos escolher aqui o kernel, o kernel vai ser o k-means. Pronto, e aí eu já vou fazer a carga dos dados. Então, vou colocar aqui uma anotação do markdown. E vamos carregar os dados. Então, vou chamar aqui, carga de dados, vou chamar de clientes, dfclientes, igual a pd.read.csv e o arquivo que a gente vai importar, que é o ./.datasets clientes.pj.csv Carreguei esses dados, já tenho condições de visualizar, só para confirmar o que a gente já tinha feito no outro modo. Então, aqui visualizar, visualizar a estrutura. Então, a gente pode vir aqui e colocar dfclientes.info Então, nós temos aqui 500 clientes, onde a gente tem variáveis categóricas, como a atividade econômica e localização, temos aqui o faturamento mensal, como um float, temos o número de funcionários como inteiro, a idade da empresa, um inteiro, e a inovação, que é um inteiro, que é um índice de inovação. Vamos visualizar, dar um recap dos primeiros registros. Primeiros registros, então, dfclientes.read10 Então, só mostrando aqui que a gente tem a atividade econômica, o comércio, o negócio, serviços, indústria, faturamento mensal, número de funcionários, localização, são empresas no Sudeste, então só tem aqui cidades do Sudeste, a idade da empresa em anos, e o quesito de inovação, de 0 a 9. Então, a gente já tem aqui uma ideia de como os dados estão, carregamos esses dados, vimos a estrutura deles, vamos dar uma reforçada aqui no CSV. O CSV aqui tem a estrutura de atividade econômica, que é a empresa, faturamento mensal, o número de funcionários, a localização dela em termos de cidade, o tempo de existência da empresa em anos, e o nível de inovação de 0 a 9. Então, fechamos aqui essa primeira parte introdutória de carregar as principais bibliotecas, carregar os dados, e vejo vocês no próximo vídeo.

### Aula 9 - EDA

#### Transcrição

Dando sequência ao nosso notebook, aqui o nosso Jupyter, o nosso projeto, a gente vai começar a fazer agora o nosso EDA. A ideia aqui não é repetir o EDA que a gente já fez no módulo de análise, no módulo de classificação, o árvore de decisão, a ideia aqui é a gente explorar algumas outras coisas. Então, vou colocar aqui uma marcação aqui, um markdown, que a gente vai começar aqui o EDA, e até para explicar umas técnicas novas para vocês em termos de testes estatísticos. Primeira coisa que a gente vai fazer, isso a gente já tinha feito, é olhar um pouco da distribuição da variável inovação, variável inovação, ou seja, quantas empresas mais ou menos a gente tem para cada nível de inovação, para ver se há uma tendência de ter empresas mais um nível no outro, ou se de repente há uma distribuição mais parecida com uma distribuição uniforme. Então como é que a gente faz isso? A gente vai calcular o percentual de inovação, vou colocar isso em uma variável, que é dado pelo nosso DataFrame, a gente vai fazer uma contagem de valores da coluna inovação, e vamos dividir essa contagem de cada um deles pelo tamanho do dataset de clientes, e vamos multiplicar por 100, e aí vamos gerar um gráfico disso, vamos usar o Plotly, um gráfico de barras, pxbar, onde a gente vai colocar o percentual de inovação, e a cor a gente vai usar o percentual de inovação.index, como isso aqui virou um DataFrame, ele vai colocar cores diferentes para cada um dos níveis de inovação. Então a gente consegue ver aqui os níveis de 0 a 9, onde o 0 é o roxinho e chega até o amarelo que é o 9, e ele mostrando aqui a distribuição percentual das empresas em cada um dos níveis de inovação. O que a gente percebe aqui é que não há nenhum tipo de nível de inovação, nenhum nível de inovação que seja muito maior que os demais. Você tem aqui o nível 1, que é um pouquinho maior que o 2, então veja aqui 11.4%, 11.2%, aqui você tem 10.8%, 10.6%, aqui você tem um pouco menos, 9.6%, mas não tem nada que distoie tanto no que diz respeito aos valores inferiores, quanto aos valores superiores. E aí cabe aqui uma hipótese que a gente quer levantar, que é assim, será que a gente tem empresas com níveis de faturamento diferentes para cada inovação, para cada nível de inovação? Ou seja, eu tenho médias diferentes de faturamento mensal quando eu tenho níveis diferentes de inovação, será que essas empresas se comportam de forma diferente? Será que esses podem ser critérios pelos quais a gente poderia, ou o algoritmo poderia agrupar essas informações e olhar, olha, as pessoas que faturam X mais as empresas que estão em tal nível, talvez elas formem um cluster, formem um agrupamento. Então para a gente testar isso, a gente vai usar um teste estatístico, um teste de hipótese, que eu já comentei em algumas aulas, que é o teste de ANOVA, que é o teste de análise de variância. O objetivo desse teste é que ele verifica se há variações significativas numa média para diferentes grupos. A gente pode usar esse teste de várias formas, eu poderia, num exemplo, eu tenho uma medição de produtividade de cinco máquinas e eu faço essa medição dessas cinco máquinas em vários dias e aí eu posso comparar para cada dia se essas variações, se essas máquinas por medidas tiveram uma grande variação de um dia para o outro. Ou eu poderia até fazer o inverso, eu tenho a mesma máquina, as mesmas cinco máquinas medidas todo dia e eu quero validar se aquela máquina teve algum tipo de variação, e aí eu não estou olhando pelo dia, eu estou olhando pela visão da máquina, se ela teve uma variação de produtividade ao longo dos dias. Então tem várias formas que a gente consegue avaliar isso. Então a gente vai usar aqui para isso, para a gente definir, será que existem variações grandes de média de faturamento para cada grupo ou para cada nível de inovação? É isso que a gente vai validar nesse teste de hipóteses. Eu vou colocar aqui que o teste, a nova, chama análise de variância, ele vai verificar se há variações significativas na média de faturamento mensal para diferentes níveis de inovação. É isso que a gente quer saber. Nós temos aqui dez níveis de inovação, então a gente quer comparar, na verdade, dez grupos diferentes e ver se entre esses dez grupos eu tenho uma diferença significativa entre as médias desses grupos. Para a gente fazer esse teste da nova, existem alguns pressupostos, algumas suposições que a gente tem que fazer. Então eu vou colocar aqui algumas suposições ou pressupostos, e eu vou colocar aqui quais são. Primeiro deles, as observações precisam ser independentes umas das outras. Segunda, a variável dependente, no caso, a gente está fazendo análise da média do faturamento, precisa ser uma variável contínua. Terceira suposição pressuposto, que os dados dessa variável seguem uma distribuição normal. Um outro pressuposto é que haja homogeneidade das variâncias. E por último, que as amostras sejam de tamanhos iguais. Então a gente precisa validar esses pressupostos para garantir que a gente possa, de fato, aplicar esse teste estatístico. E é isso que a gente vai fazer daqui para frente. A gente vai pegar alguns, alguns a gente já sabe, alguns a gente vai fazer em outro teste estatístico para validar essa suposição ou esse pressuposto, para a gente avaliar se a gente pode fazer uma nova no modelo normal dela, vamos pensar assim, ou se a gente tem que fazer o que a gente chama de uma nova de Welsh, que é quando a gente não tem algum desses tópicos tendo atendidos. Por exemplo, se a gente não tem uma variância homogênea, ou se a amostra não é de tamanho igual, as amostras não são tamanhos iguais, eu não posso fazer a nova tradicional, eu tenho que fazer uma nova de Welsh, porque ela consegue trabalhar com essa variação e ela acaba sendo mais robusta. Então é isso que a gente vai ver daqui para frente, vejo vocês no próximo vídeo.

### Aula 10 - Introdução ANOVA e Testes Estatísticos

#### Transcrição

O primeiro ponto que a gente vai olhar em relação aos pressupostos que a gente precisa atender dentro do nosso teste estatístico de ANOVA, a gente vai avaliar se há variações, se as variâncias entre os grupos são homogêneos, a gente vai olhar isso aqui. A gente também vai olhar isso daqui, se segue uma distribuição normal, a gente já sabe que as observações são independentes e a gente já sabe que a variável independente é contínua, é uma variável float, uma variável que não possui valores inteiros. A gente também já sabe que as amostras não são de tamanhos iguais por causa disso aqui, a gente sabe que cada grupo de inovação tem uma quantidade diferente de empresas. Então só por aqui eu já poderia assumir que a ANOVA que eu tenho que fazer é uma ANOVA de Welch, mas eu quero explicar, principalmente para outras necessidades, como fazer um teste de homogeneidade de varianças e aqui a gente já fez, mas eu vou fazer novamente esse teste para validar se o nosso dado segue uma distribuição normal. Então o primeiro ponto aqui, checar se as varianças, que aqui é o faturamento, entre os grupos, que no caso é a nossa inovação, são homogêneas, hemogêneas, pronto. Para fazer isso existem alguns testes estatísticos, um deles que a gente vai aplicar aqui é o chamado teste de Bartlett, aplicar o teste de Bartlett. Sempre que a gente fala de teste estatístico, a gente fala de hipótese nula e hipótese alternativa. Então qual é a hipótese nula? Que as varianças são iguais entre os grupos. H1, varianças, não temos evidências para dizer que as varianças são iguais. As varianças não são iguais, ou não temos evidências para dizer isso. Como que a gente vai fazer esse teste? A gente vai importar um pacote, um módulo do pacote do SciPy. O SciPy já vem incluído dentro do Pandas e do Scikit-learn, então a gente não precisou fazer o pif dele, fazer uma instalação própria dele. Então a gente vai chamar um pacote de estatística dentro do SciPy chamado Bartlett. E o que a gente vai fazer? A gente vai separar, a gente vai criar como se fosse uma lista de listas para colocar dentro dessa função. Na verdade o Bartlett, o que ele recebe de dados? Ele recebe valores, ele recebe várias listas, sendo cada lista um agrupamento. Então a gente vai montar essa lista de listas e vai colocar essa informação dentro como argumento para a função Bartlett. Então como a gente vai fazer isso? A gente vai separar os dados de faturamento em grupos com base na coluna inovação. Então eu vou criar aqui uma variável chamada dados agrupados e vou fazer o seguinte, eu quero pegar aqui do DF clientes a coluna, DF clientes eu quero a coluna faturamento mensal, onde DF clientes inovação, DF clientes inovação, seja igual a um grupo. Mas que grupo é esse? Aqui eu vou fazer um loop, o que a gente chama de um list comprehension. Então a gente vai fazer for grupo em DF clientes inovação e eu vou colocar aqui unique. Então o que esse comando gigante está fazendo? Fechou aqui errado. Ele está fazendo um for em todos os possíveis valores de inovação, que a gente sabe que é de 0 a 9. Para cada um deles ele vai fazer um loop gravando dentro de uma lista, dizendo quais são as empresas, os faturamentos da empresa da inovação igual a 0, que é exatamente esse filtro que eu estou fazendo aqui. Aí ele vai listar todos esses valores. Aí ele vai colocar uma segunda lista, todos os valores igual a 1, vai preencher numa segunda lista todos os faturamentos de empresas igual a 1, e vai jogar tudo isso aqui dentro de uma lista. Mas como é que eu alimento essa lista dentro do algoritmo? Então eu vou colocar aqui, executar o teste de Bartlett. Então ele recebe dois parâmetros, na verdade ele retorna dois parâmetros. Ele retorna o teste estatístico, ou seja, o retorno do teste estatístico de Bartlett, e o que nós temos interesse em saber nesse caso é o p-value, p-valor. A gente quer checar se esse p-value é menor ou maior do que 0.5. E aí ao invés de a gente colocar dados agrupados, a gente vai expandir esses dados agrupados porque ele é uma lista de listas. A gente vai colocar um asterisco aqui. Quando a gente coloca o asterisco, ele vai fazer o que? Ele vai pegar essa lista, quebrar essa lista em várias listas, com cada uma das listas sendo um argumento para essa função, que é exatamente o que ela recebe. Ela recebe várias listas. Vou executar o teste, e aí eu vou exibir os resultados. Então eu vou fazer um print do resultado, que vai ser a estatística do teste de Bartlett. Eu posso fazer assim, colocar Bart... Vou até copiar daqui, posso fazer com o F. Eu pego o Bart aqui e coloco aqui. E a mesma coisa para o p-value. Então aqui o p-value, e aí eu vou trazer o p-value para cá. Vou executar. Ele me mostra o seguinte, que esse teste de Bartlett se tornou o p-value de 0.28. Como 0.28 é maior do que 0.5, eu não posso rejeitar H0. Portanto, o que ele está me dizendo aqui, é que as variâncias são iguais. Então, como eu não posso rejeitar H0, eu aceito H0 e digo que as variâncias entre esses grupos são iguais. Então, uma das condições aqui do ANOVA foi satisfeita. As variâncias são homogêneas, todos esses grupos de inovação, que é o que a gente está trabalhando aqui. O próximo teste que a gente vai fazer é o teste de normalidade. Então, a gente vai fazer o teste de normalidade. A gente já fez ele algumas vezes. Então, executar o teste de Shapiro-Wilk. A gente já fez esse teste algumas vezes. E aí, reforçando ele, é verificar se os dados seguem uma distribuição normal. Novamente, H0 segue uma distribuição normal. H1 não segue uma distribuição normal. Aí, nós vamos fazer o que aqui? A gente vai colocar a execução do teste. Da mesma forma, a gente executa esse teste e recebe como retorno dois parâmetros. Antes disso, a gente pode importar o módulo. Então, deixa eu importar o módulo aqui. Aqui é o módulo, deixa eu até colocar da forma como está. Aqui, a gente vai importar no status do SciPy, Shapiro. E aqui a estrutura, a gente até pode copiar daqui, porque é bem parecida. A gente tem que ficar digitando de novo. Então, a gente tem aqui o Shapiro-Test, Shapiro-Test-Statistic, Shapiro-Test-Statistic e Shapiro-P-Valley. Só que aqui a gente vai passar o que? Shapiro, e vamos passar o DF-Clientes, faturamento mensal. Está querendo ver se esse valor como um todo, no nosso dataset, ele segue uma distribuição normal. E aí, a gente pode vir aqui e colocar Shapiro-Wilk, Shapiro-Wilk. E aí, a gente vai ver os resultados que esse teste vai nos retornar. Pronto. E aí, ele trouxe uma estatística de teste de 0,23. O que denota o seguinte, como esse número também está maior do que 0,05, eu não posso rejeitar H0. Então, eu também posso dizer que esses valores de faturamento mensal, eles seguem uma distribuição normal. Então, com isso, a gente conseguiu validar dois tópicos. Segue uma distribuição normal e as varianças são homogêneas. O único ponto aqui é que a gente não tem amostras de tamanhos similares, tamanhos iguais. Então, isso nos leva a ter que fazer um teste de ANOVA de Welch, que é justamente para essa condição, quando ele não conseguiu satisfazer a todos os pressupostos da ANOVA tradicional. E é o que a gente vai fazer no próximo vídeo.

### Aula 11 - Teste de ANOVA

#### Transcrição

Para concluir agora essa parte do EDA, fazendo esse teste de ANOVA, para a gente avaliar essa relação entre as médias dos valores para cada nível de inovação, a gente vai aplicar o teste de ANOVA de Welch, pois as amostras são de tamanho diferente. Então como é que a gente vai aplicar esse teste? Então a gente vai colocar aqui, aplicar a ANOVA de Welch, pois as amostras são de tamanhos diferentes. H0, não há diferenças significativas entre as médias dos grupos. H1, há pelo menos uma diferença significativa entre as médias dos grupos. Para isso a gente vai usar um pacote estatístico que a gente já usou algumas vezes, que é o PINGUIM. E aí nós vamos importar um módulo chamado Welch ANOVA, ou seja, ANOVA de Welch. ANOVA retorna os dados dentro de um DataFrame, então a gente coloca aqui o nome desse DataFrame, vamos chamar de AOV, Análise of Variance, que é a ANOVA, e aí vamos chamar o pacote Welch ANOVA, a gente passa aqui qual que é o nosso valor que a gente quer avaliar, no caso o faturamento mensal, para esse argumento chamado DV, a gente vai passar no argumento BETWEEN qual é o agrupamento que a gente quer, qual é a informação que tem o agrupamento, no caso aqui a gente tem a inovação, e a gente passa onde estão os dados, no caso a gente passa isso no argumento DATA, e a gente vai passar como DFCLIENTES, e aí como é que eu vou exibir os resultados, então exibindo os resultados, vou copiar aqui só para ganhar um pouco de tempo nessa questão do print, e aqui eu vou ter a estatística do teste de ANOVA de Welch, e o PVE do teste de ANOVA de Welch, aqui como eu tenho um DataFrame, eu preciso retornar o primeiro registro desse DataFrame, então eu não posso simplesmente apontar AOV e colocar a coluna que eu quero, eu tenho que dizer exatamente a linha que eu quero, então o que a gente faz, a gente usa aquele comando LOCK do Pandas, onde eu quero retornar o primeiro registro, a primeira linha do DataFrame, e a coluna F, que é uma das colunas que ele retorna nesse DataFrame, que é o valor da estatística de teste para esse teste de ANOVA, e faço a mesma coisa aqui embaixo, AOV.LOCK, aí eu retorno aqui o zero, e aí eu retorno essa informação chamada PUNC, que é o nome da coluna que guarda o resultado do PVE desse teste estatístico, com isso, se a gente rodar isso aqui, ele mostra a estatística de teste, então ele diz o seguinte, o PVE do teste de ANOVA de Welch é de 0,64, então o que ele está dizendo aqui é que, como esse valor está acima de 0,05, a gente não pode rejeitar H0, portanto, não há, por esse teste, diferenças significativas entre as médias dos grupos, ou seja, no nosso caso, o que essa informação está dizendo é que, independente do nível de inovação que a empresa esteja posicionada, quando a gente agrupa as empresas nesses níveis, não há diferenças significativas entre as médias de faturamento entre esses grupos, entre esses níveis, então você pode ter empresas com um baixo nível de maturidade, ou um alto nível de maturidade em inovação, e as médias em relação aos faturamentos mensais não têm diferenças significativas, ou seja, tem empresas dos mais diversos tipos, em termos de inovação, com faturamentos que podem ser os mesmos ou muito próximos, o que acaba gerando essa média bastante próxima, ou que não há diferenças significativas entre essas médias. Então, isso aqui foi uma outra forma de explorar o EDA, agora fazendo um teste estatístico, para comparar médias entre mais de dois grupos, para você avaliar se há alguma diferença entre esses grupos. Daqui para frente, a gente vai agora explorar especificamente o algoritmo de K-Means e os seus reflexos, e vejo vocês no próximo vídeo.

## Bloco 6 - Treinamento do Modelo

### Aula 12 - Preparação dos Dados para o KMeans

#### Transcrição

Dando sequência ao nosso projeto prático, a gente vai começar a trabalhar agora especificamente com o algoritmo de K-Means, que é o foco do nosso módulo. Então, a primeira coisa que a gente vai fazer é treinar o algoritmo, e para isso a gente precisa preparar os dados para fazer esse treinamento. Então, vou separar aqui uma parte no Markdown para colocar treinar o algoritmo K-Means. Para isso, a gente precisa fazer algumas transformações nos dados, porque a gente tem dados categóricos, temos dados variáveis contínuas que estão em escalas diferentes, e temos uma variável original. Então, a gente vai fazer algo que a gente já fez anteriormente, que é uma parte de transformar as colunas para que elas estejam no formato correto para o nosso algoritmo. Então, primeiramente, eu vou colocar as variáveis dentro de uma variável chamada x, lembrando que aqui, no caso, a gente não vai usar o conceito de x e y, porque a gente não tem um target, e a gente também não vai separar em treino e teste. A gente vai treinar a base toda para que ele possa retornar para a gente o que ele encontrou de padrões dos dados para nos dar a classerização. Então, selecionar as colunas para a classerização. No caso, a gente vai usar todas as colunas, porque a gente não tem target e não temos nenhuma coluna de identificação de cliente que a gente precisasse remover. Então, a gente pode simplesmente fazer um copy do nosso data frame inteiro e jogar em uma variável chamada x, para a gente manter o padrão de que o x é o input de dados para o nosso algoritmo. A gente precisa também fazer a separação entre as variáveis numéricas, categóricas e as ordinais. Que, na verdade, são um tipo de categóricas na estatística. Então, a gente está falando das variáveis contínuas, as variáveis categóricas nominais e as variáveis categóricas ordinais. Então, falando de features numéricas, a gente tem uma lista para a gente poder aplicar as nossas transformações na lista de features que a gente tem. Então, a gente tem uma feature chamada faturamento mensal, que é uma feature que a gente precisa aplicar. A gente tem uma feature chamada número de funcionários. E a gente tem uma outra feature chamada idade. Essas features têm escalas diferentes. Então, a gente vai colocá-las todas dentro da mesma escala usando um standard scaler. A gente tem features categóricas. Então, categorical features. Aqui, as categóricas, eu tenho a localização, o nome de uma cidade, a gente vai fazer um one-hot-encode e vamos fazer também com atividade econômica. E temos uma feature ordinal. Ordinal features. Onde nós temos aqui a inovação. Então, eu vou passar todas dentro do transformer para ficar uma coisa mais organizada. Então, a gente vai aplicar transformações. Transformações por tipo. Então, eu vou ter aqui um numeric transformer que vai ser, na verdade, um standard scaler. Eu vou ter um categorical transformer que vai ser um one-hot-encoder. E eu vou ter um ordinal transformer que vai ser um ordinal encoder. Já deu um enter aqui, mas vou continuar aqui. E agora eu vou fazer o que? Eu vou criar um pré-processor que vai transformar os dados de X em um outro dataset, que a gente vai chamar de X-transformed, onde a gente vai ter a transformação desses dados passando por esse pipeline de transformação. A gente vai separar o pipeline de transformação do pipeline que faz o treinamento propriamente dito do algoritmo. Então, eu vou ter aqui um pré-processor que é igual a um column transformer. Nesse column transformer, eu tenho que indicar quais são os transformers que eu vou usar. Então, eu vou ter aqui uma lista. Então, eu tenho um transformer 1, que é chamado numérico, onde eu vou ter um numerical transformer que vai ser aplicado nas numeric features. Eu vou ter um segundo, que é um transformer que eu vou chamar de cat, que seria categórico, onde eu vou ter aqui o categorical transformer que vai ser aplicado nas categorical features. E por último, eu tenho um outro, que é o ordinal, onde eu vou aplicar o ordinal transformer nas ordinal features. Então, aqui eu já fechei, fechei. E aí, como é que eu transformo esses dados? Eu dou transformar os dados. Então, você vê que eu crio aqui um X-transformer. O que eu faço? Eu posso dar um fit transform nesse pré-processor. Então, o que ele faz? Ele treina, ele treina esses transformadores e ele já aplica no próprio dataset, jogando o resultado para esse daqui, a transformação feita. Então, eu vou colocar fit transform de X. Vou rodar. Então, se eu olhar aqui agora o X-transformer, ele tem uma estrutura diferente. Ele transformou as variáveis numéricas em variáveis dentro de uma escala, do standard scaler. Se a gente olhar as variáveis, as variáveis one-hot-encode, ele transformou em one-hot-encode, por estar esse 0, 1 e 0. E ele transformou também as features ordinais numa escala ordinal. Então, foi isso que ele fez aqui na hora de criar esse transformer. Ou seja, eu estou jogando no algoritmo apenas números, esses aqui todos numa mesma escala e esses números, no caso, categórico e ordinal numa escala de 0, 1 ou numa escala ordinal, no caso da inovação. Então, ele transformou o nosso X num X-transformer, que é ele que eu vou colocar dentro para fazer o treinamento do algoritmo. Porém, uma coisa importante. A gente comentou na parte de conceitos que a quantidade de clusters é uma informação que é definida pelo cientista de dados. E você tem vários critérios para definir isso. Você pode ter um critério que é que você conhece a natureza daquele dado. Então, você consegue inferir ou, na verdade, impor um número e falar, olha, eu quero gerar quatro clusters, eu quero gerar cinco clusters. Existe como fazer o teste do cotovelo. Existe como fazer o teste de silhueta. Tem várias formas. A gente vai optar aqui por fazer o teste do silhuete. Por quê? Porque a gente também já vai explorar essa métrica. A gente vai mostrar como calcular essa métrica dentro dos pacotes. E vamos fazer um processo de avaliar qual é a melhor configuração de clusters e qual medida de distância a gente poderia usar para ter o maior silhuete score possível. Então, por essa razão que a gente vai usar o silhuete, não vamos usar o teste do cotovelo, que geralmente, na literatura, é o mais usado. Então, a gente vai usar o de silhuete para também já mostrar o cálculo desse silhuete score como nossa métrica de validação para o nosso projeto de k-means. Então, fechamos aqui a preparação dos dados. Daqui para frente, a gente vai fazer já a preparação do algoritmo, já dentro de um conceito de otimização. Vamos ver os resultados que a gente vai obter. Vejo vocês no próximo vídeo.

### Aula 13 - Executando o KMeans com Optuna

#### Transcrição

Com os dados agora tabulados, ou na verdade transformados e preparados para o nosso algoritmo, a gente pode começar agora a preparar o algoritmo de fato para a gente treinar o nosso modelo. E a gente vai fazer isso com o conceito de otimizar uma métrica, a gente vai otimizar a métrica de silhuete. Então, lembrando dos outros modos, a gente vai usar o Optuna para otimização de hiperparâmetros. Para isso a gente precisa primeiro definir uma função, que é a função que faz o treinamento do modelo, desse treinamento do modelo ele extrai uma métrica e aí depois na hora que eu faço a execução aleatória, ou através de um espaço de busca de parâmetros, ele vai me informar qual foi o parâmetro que funcionou melhor, então a gente vai criar essa função aqui. A gente vai criar uma função chamada K-Means Objective, que recebe um trial, ou seja, recebe uma tentativa, recebe uma interação que ele fez dentro do nosso loop, da nossa função. Então eu vou definir os hiperparâmetros a serem ajustados, a gente vai ajustar dois hiperparâmetros, o K, que é a quantidade de clusters, e a gente vai também mudar a métrica de distância, a medida de distância, a gente vai usar Euclidiana, que é a mais tradicional, vamos pensar assim, e a gente vai usar Minkowski, que é uma combinação entre Euclidiana e a medida de Manhattan, a gente vai usar essas duas e vamos ver que resultado a gente vai obter. Então a gente vai criar uma variável aqui chamada nClusters, que vai receber quais os clusters, ele vai fazer interação nisso x vezes e substituindo na função, a gente vai colocar aqui um sugest int, onde a gente vai chamar essa função de nClusters, e nós vamos passar o parâmetro. A gente vai fazer clusters, a gente vai trabalhar com clusters de 3 a 10, ou seja, ele não vai ter K1 e nem K2, K igual a 2, ele vai ter K3, K4, K5, K6, K7, K8, K9 e K10. E nós vamos trabalhar com a métrica de distância, tem várias métricas de distância no algoritmo, a gente vai trabalhar só com duas, então a gente vai colocar aqui trial, sugest categorical, distance metric, o nome que a gente está dando, e ele vai receber o parâmetro chamado Euclidean e um outro chamado Minkowski. Definidos os hiperparâmetros, a gente pode instanciar o K-Means com esses hiperparâmetros, então a gente vem aqui criar o modelo, então o modelo vai chamar K-Means, ou podemos chamar modelo K-Means, que vai ser K-Means, que a gente já importou ele, K-Means, nclusters igual a nclusters, que é essa variável aqui, que ela vai começar a receber os parâmetros, e distance metric, que vai receber essa variável chamada distance metric. Eu vou colocar aqui um fator randômico, um random state de 51 para a gente ter a reprodutibilidade do experimento, a gente sempre fizer com isso, a gente vai ter resultados não necessariamente exatamente iguais, como o K-Means inicializa de forma aleatória, a gente pode ainda ter alguma questão, de inicializou o ponto num outro espaço dentro do nosso espaço multidimensional, a gente tem várias dimensões, que são as features, que vão compor a posição desse ponto nesse espaço, e daí ele vai calcular essas medidas. A gente está ajustando o modelo, treinando o modelo, a gente criou o modelo, treinaram o modelo, então como é que a gente treina o modelo? Modelo K-Means.fit, e aí a gente não tem fit x e y, a gente tem fit só do x-transformer, que já são os dados transformados para o algoritmo, feito isso a gente pode calcular o Silhouette Score, e esse Silhouette Score a gente calcula de duas formas, com dois passos, a gente primeiro calcula as distâncias entre os pontos, e para isso que a gente usa essa função chamada pairwise distance, então a gente coloca pairwise distance de x-transformer para a métrica que a gente está querendo medir, que é a métrica que está vindo aqui no parâmetro. Então eu calculo, para x-clusters, qual é a quantidade, qual é a distância que existe entre esses, qual a distância que existe, considerando essa distância, qual é a distância que existe para esses pares, por isso que é pairwise, de objetos. E aí com base nisso, eu calculo o Silhouette AVG, a média, que seria Silhouette Score, que é uma função que recebe as distâncias, e recebe o K-Means Labels, ou seja, ele recebe onde está, qual foi o cluster atribuído para cada ponto de dado, e a gente vai usar isso mais para frente. Então ele fala o seguinte, bom, legal, se essas são as distâncias entre os pontos, e ele definiu para cada ponto quais foram os clusters, com base nisso eu sei calcular, eu consigo calcular o Silhouette Score, então aqui é modelo K-Means Labels, e aí eu retorno para ele o Silhouette. Retornei aqui, então vou registrar essa função, a gente já pode agora criar um estudo do Optuna e rodar esse estudo, então o que a gente vai fazer aqui embaixo, então é criar um estudo do Optuna, então a gente vai ter aqui um espaço de busca, porque a gente vai fazer o que? A gente tem na verdade, 3, 4, 5, 6, 7, 8, 9, 10, a gente tem 8 opções diferentes de cluster para dois tipos de métricas, então a gente já tem um espaço de busca pré-determinado, e a gente tem na verdade 16 opções possíveis. Então vou colocar aqui, nClusters, vou colocar 3, 4, 5, 6, 7, 8, 9, 10, e vou colocar aqui o Distance Metric, e vou colocar lá, vou até copiar aqui o Euclidiana e o Minkowski, porque uma coisa é o que está dentro da função, que é só para substituir dentro da chamada do K-Means, aqui eu estou definindo um espaço de busca para que ele se concentre e rodar os experimentos do Optuna, considerando apenas esse espaço, então a gente vai fazer na verdade o que a gente chama de um Grid Search, ele não está fazendo um Random Search, ele vai passar por todas as opções possíveis, a gente viu aqui que são 16 opções, 8 clusters para dois tipos de métricas. Então para isso eu tenho que criar um Sampler dentro do Optuna, Optuna.Samplers.GridSampler, onde o espaço de busca é igual ao espaço de busca que eu defini, e agora eu crio o estudo, estudo K-Means, Optuna.CreateStudy, a direção eu quero maximizar o meu Silhouette Score, quanto melhor os meus clusters, usando esse Sampler, e aí eu vou rodar o estudo, para rodar o estudo a gente coloca estudo K-Means.Optimize, a gente vai falar qual a função que a gente vai rodar, e a quantidade de Trials, se eu colocar aqui Trials, eu vou até colocar aqui igual a 100, não vai adiantar, porque na verdade a gente já sabe que são 16 Trials, mas eu vou colocar mesmo assim, ele vai parar no 16. Rodamos, vamos ver o que faltou aqui, Init, peraí que tem alguma coisa aqui, DistanceMetric, teve algum parâmetro aqui que ele não... Ah, o DistanceMetric não é no K-Means, o DistanceMetric não é no K-Means, DistanceMetric na verdade é só na hora de calcular a distância, então, refazer aqui, e refazer aqui, pronto. Então ele chegou aqui, fez aqui as opções, ele chegou aqui nos Trials, são de 0 a 15, são 16 Trials como foi comentado, foi calculando cada um deles, e na verdade ele descobriu que o Best Trial foi o primeiro, onde ele definiu que, 3 Clusters com DistanceMetric, e o Clidiana foi a melhor opção que ele chegou. A gente vai fazer um detalhe um pouco melhor nessa configuração, e fazer o treinamento especificamente com esse modelo, pra gente fazer algumas análises em cima disso. Então vejo vocês no próximo vídeo.

## Bloco 7 - Análise do Resultados

### Aula 14 - Analisando Resultados do K-Means

#### Transcrição

Dando sequência no nosso projeto técnico, projeto prático, a gente vai agora analisar os resultados do Optuna e a gente vai treinar aqui o modelo olhando para o resultado melhor aqui que o Optuna trouxe para nós. Então como a gente vai fazer isso? Então vamos dar sequência aqui em uma outra célula. Então primeiro a gente vai avaliar, guardar em uma variável aqui, qual foi a melhor configuração pelo Optuna, vamos chamar isso aqui de bestparams, melhores parâmetros, que foi o estudo caminhos .bestparams. O que a gente vai fazer agora é criar o modelo de fato, que é o modelo que a gente vai salvar depois com os melhores parâmetros, estanciando o caminhos com melhores parâmetros. Então a gente vai colocar aqui o bestcamins, o melhor camins que a gente teve, que é camins, onde o nclusters é igual a bestparams de nclusters, random state igual a 51. E eu vou dar um fit nesse modelo, fit x transformate. E a gente vai calcular o silhuete da mesma forma, então a gente pode até copiar daqui todo esse processo, que na verdade é, a gente vai calcular o silhuete score, o pairwise a gente vai usar a mesma coisa, o método aqui vai ser o bestparams, de distance metric, e o nosso silhuete, nosso bestsilhuete, que vai ser o nosso bestcamins.labels, e nós vamos dar um print aqui, o K, o número de clusters, vou colocar um F aqui para ficar mais fácil, dois pontos, e eu vou pegar aqui o bestparams clusters, vou colocar aqui dentro, e eu vou fazer um print, de qual foi a medida de distancia, distancia metrica, de distancia metricada, e qual foi o nosso silhuete score, nosso silhuete score, que foi o nosso bestsilhuete. E ele mostrou aqui para nós, que nossa quantidade de clusters que ele determinou, pelo Optuna foi de 3, a métrica de distancia escolhida foi a euclidiana, aqui no pairwise, e nosso silhuete score de 0.44. O que a gente só pode fazer a partir de agora, já que ele já chegou nesse melhor parâmetro, a gente só precisa atribuir para o nosso dataset, qual cluster que ele escolheu, e ele faz isso através do labels, então o que a gente vai fazer aqui, a gente vai criar uma coluna no nosso DataFrame, com o cluster escolhido, e ele não vai escolher o cluster com nome, ele vai escolher o número de cluster, no caso 0, 1 e 2, que são os clusters, então a gente vai fazer isso, a gente vai criar coluna com cluster escolhido, então a gente vai colocar aqui, df clientes, a gente vai chamar a coluna de cluster, e ela vai receber o que? O bestcamins.labels, como o bestcamins tem os labels de todos os registros que foram para o modelo, lembrando que a gente não vai colocar entre treino e teste, então as nossas 500 instâncias estão aí, e ele já está com tudo isso alinhado conforme o treinamento do modelo que a gente fez, então a gente coloca aqui esse labels, e a gente consegue agora visualizar os primeiros registros, já com essa coluna de cluster atualizada, então a gente pode vir aqui e dar um df clientes.head de 10, e a gente consegue ver aqui a atribuição dos clusters, então ele atribuiu para esse primeiro registro o cluster 0, o segundo também, para o cluster 2 ele atribuiu 1, para o registro 2 ele atribuiu 1, para o registro 5 ele atribuiu 2 e assim por diante, então com isso a gente conseguiu fazer a atribuição dos clusters, conforme o bestcamins, e avaliamos aqui qual foi o melhor resultado que o camins teve, rodando uma rotina de otimização de hiperparâmetros, a gente vai no próximo vídeo mostrar uma visualização de dados, olhando esses clusters para tentar ter uma ideia, que informações ele pode ter usado para separar bem esses clusters, vejo vocês no próximo vídeo.

### Aula 15 - Visualização de Decisões do KMeans

#### Transcrição

Para a gente fazer uma análise agora um pouco mais detalhada, para concluir essa parte do nosso modelo, tendo em vista que agora a gente tem os clusters definidos para cada um dos registros da nossa base, a gente vai fazer um cruzamento de dados aqui para tentar entender qual foi o critério que ele usou, eventualmente, para fazer a organização desses dados em clusters. Então, o que eu vou fazer aqui? Eu vou criar algumas visualizações para a gente tentar obter essas informações ou ter uma pista do que ele fez. Então, aqui eu vou colocar visualizar resultados. Na verdade, a ideia aqui é só criar alguns plots. O primeiro deles é avaliar o seguinte, será que se a gente cruzar idade com faturamento mensal, a gente só pode cruzar duas e duas variáveis e colocar a cor do cluster que ele escolheu, a gente tem aqui uma pista se ele usou essas informações, se essas informações foram relevantes ou não. Como é que a gente valida isso? A gente pode fazer cruzar idade e faturamento, apresentando os clusters. Vamos ver se a gente consegue identificar um padrão ou entender o padrão que ele chegou. Então, a gente vai fazer um scatterplot de 10 clientes, que agora tem um dado de cluster, onde o nosso X é a idade, o nosso Y é faturamento mensal e a cor de cada pontinho é dada pelo cluster. Aqui é o Y, Y é igual a faturamento mensal. Quando a gente olha esse plot, a gente percebe o seguinte, para diversos níveis de idade de empresa, que está em anos de experiência, anos de casa, a gente encontra vários clusters e para vários níveis de faturamento. Ou seja, não dá para perceber um padrão de que empresas que têm um faturamento maior mensal, caem e irem para o cluster zero. Mas você encontra esse cluster zero aqui também, em faturamentos mais baixos, em diversos níveis de idade, diversos valores de idade da empresa. Então, não dá para observar um padrão que ele tenha identificado clusters por esse critério. O que a gente pode fazer agora é avaliar uma outra questão. A gente tinha feito aquela análise por inovação. Então, a gente poderia, por exemplo, fazer um cruzamento de inovação com faturamento, apresentando os clusters. Então, a gente vai colocar aqui X inovação e faturamento. E aqui a gente vai ver um padrão, e um padrão bastante interessante, é um padrão que mostra aquilo que eu chamo a beleza dos algoritmos não supervisionados, que nos dão insights, nos revelam padrões muitas vezes que a gente não consegue perceber em um EDA. Nesse caso aqui, o que ele está trazendo, na hora que ele foi fazer a organização de clusters, é a seguinte, ele praticamente definiu como cluster zero, empresas que estejam no nível mais desacelerado de inovação, ou seja, empresas que estão no nível zero, nível um e nível dois, independente do valor de faturamento mensal. Ou seja, o faturamento não é relevante para definir o cluster de uma empresa, mas a inovação, com certeza, por esse plot, definiu aqui o cluster dessas empresas. Empresas que estão no nível, vamos pensar, moderado de inovação, 3, 4 e 5, entraram no que ele chamou de cluster 2. E aqui a questão do número do cluster pouco importa, não é uma ordem de grandeza, é só uma atribuição. Empresas que já estão no estágio intermediário para avançado de inovação, empresas que estão no nível de inovação 6, 7, 8 e 9, entraram no que ele chamou de cluster 1. De novo, independente do valor de faturamento. Então, isso é interessante mostrar, porque isso nos revela um padrão. Ou seja, é possível que a gente possa usar clientes, o nível de inovação de clientes, para que a gente possa personalizar o atendimento a esses clientes. Se a gente tivesse, talvez, outras variáveis desses clientes, além dessas que a gente tem aqui, a gente poderia ter outros padrões sendo revelados, ou até outras variáveis contribuindo para isso. Não é ideia de explorar todas com todas as variáveis, é algo que vocês podem dar continuidade aqui no experimento, no notebook, explorar as variáveis categóricas, explorar a variável de número de funcionários, por exemplo. A gente poderia até fazer aqui, número de funcionários. Número de funcionários que é uma variável numérica também, para não ter que fazer nenhuma conversão. Número de funcionários. A gente pode colocar aqui, número de funcionários e fazer um cruzamento. E a gente percebe aqui a mesma coisa, você tem níveis diversos de número de funcionários da empresa, para faturamentos distintos, e aí você tem uma mistura aqui. Você não tem, não dá para enxergar aqui dentro um padrão que ele clusterizou baseado nessa informação. Mas aqui é nítido que ele usou essa informação de inovação para que ele pudesse clusterizar esses clientes. Então, isso é o que chamamos de um outcome interessante. Além da gente ter uma visão dos clusters que o algoritmo nos trouxe, a gente consegue com alguns plots identificar quais são os fatores que influenciaram mais nessa definição de cluster. Realizamos por aqui essa parte do treinamento do modelo. Na próxima parte a gente vai fazer a parte, antes de fazer isso, a gente vai fazer o seguinte, para a gente poder concluir na verdade, a gente vai fazer o seguinte, a gente vai salvar o modelo. Então, vamos aqui, vou colocar aqui um markdown, salvar o modelo e o pipeline. O modelo e o pipeline estão separados. A gente tem que salvar os dois para poder aplicar o pipeline de transformação primeiro nos dados, para depois a gente poder fazer o treinamento do modelo. A gente vai fazer isso separado. Então, a gente vai fazer aqui import joblib. Vamos aqui salvar o modelo. Só transformar essa célula aqui em Python. Salvar o modelo. Que é o nosso melhor modelo. Vamos salvar isso aqui como modelo. E vamos salvar o pipeline. Dump. E aqui eu vou pegar aquela variável chamada preprocessor, que é um pipeline, na verdade, um column transformer. E vou chamá-la de pipeline clusterização clientes. Então, ela serve só para fazer o pré-processamento. E aí depois eu uso esse modelo para fazer o treinamento propriamente dito. Salvo aqui. Já estão salvos os dois arquivos. E aí a gente consegue usar agora esses modelos já salvos para a parte que a gente vai criar a aplicação batch, que a gente vai subir um arquivo e ele vai retornar um arquivo com o cluster que ele definiu. Então, vejo vocês no próximo vídeo.

## Bloco 8 - Entrega do Modelo com App em Batch

### Aula 16 - Aplicação Batch com Gradio

#### Transcrição

Para a gente agora concluir o módulo, a gente vai desenvolver, dentro do Gradle, uma aplicação simples. A gente até já fez uma aplicação parecida, mas a gente vai ter umas nuances aqui de que a gente vai transformar os dados em um pipeline separado. A gente não vai ter predict, na verdade, porque o nosso modelo não faz predições. Ele está, na verdade, trazendo uma estrutura com um algoritmo não supervisionado. E a gente vai popular essa coluna dentro de um arquivo e devolver esse arquivo para o usuário nessa aplicação. A gente vai fazer tudo isso no Gradle, portanto é só a gente criar uma célula separada para isso, com todas as informações. Vou colocar aqui a aplicação Batch no Gradle. Vou colocar aqui em Python. Vou colocar aqui Import Gradle as GR. Vou importar o modelo. Apesar do modelo já estar lá, mas eu vou importar ele, imaginando que isso aqui fosse uma aplicação separada. E vou carregar esse modelo. .modelo-clusterização-de-clientes.pkl E vou instanciar o preprocessor. Aqui ele vai ser o joblib.load.barra-pipeline-clusterização-de-clientes.pkl A primeira coisa que eu preciso fazer é eu definir uma função, que vai ser a função que vai ser chamada pelo front-end. Essa função eu vou chamar de clustering, onde ela recebe um arquivo. Esse arquivo vai ser lido, então eu vou colocar aqui df-empresas igual a pd.read.csv, um arquivo CSV que eu vou receber. Eu vou ler o arquivo .name, nome do arquivo que vai vir aqui do que o usuário estiver fazendo o upload. Preciso transformar os dados, então aqui eu vou fazer, só colocar aqui a observação. Carregar o CSV em um DataFrame. Transformar os dados do DataFrame para o formato que o Caminhos precisa, que é o pré-processamento. Então eu vou colocar aqui um xtransformed é igual a preprocessor.fit.transform do df-empresas direto. Não preciso fazer um copy de x, já estou fazendo direto do DataFrame. Vou treinar o modelo, então vou colocar aqui modelo.fit do xtransformed. Retornei o modelo, eu preciso colocar no arquivo, no DataFrame, a coluna de cluster. Então aqui eu vou criar a coluna cluster no DataFrame. Então vou colocar aqui df-empresas, vou usar o mesmo DataFrame, coluna cluster, que é igual a modelo.labels, underline. E vou salvar esse DataFrame, vou dar um to CSV, como um CSV. Esse CSV vai se chamar .barraclusters.csv e não vou salvar ele com índice, então índice igual a falso. E o que eu vou retornar na minha aplicação é justamente um arquivo. Então vou dar um return e vou colocar como retorno o nome desse arquivo que eu estou gerando, .barraclusters.csv. Então até aqui eu fiz a função que faz a chamada do Caminhos. E agora eu preciso só criar a interface para chamar. Então eu vou salvar isso aqui, vou rodar isso aqui. Ele já criou a função, subiu o modelo. Agora eu posso criar a interface. Aqui é criar a interface. Vou chamá-la aqui de aplicação igual a gr.interface, é uma interface do Gradle. O que vai ter nessa interface? Vai ter a função que eu chamo, é a função chamada clustering. E ela vai ter um parâmetro que é um tipo de dado, um tipo de input chamado file, que é para você fazer upload de arquivo. Onde eu só vou aceitar arquivos do tipo CSV. E ele vai me retornar um tipo de dado que também é um arquivo. Vou colocar aqui como file. Feito isso, vou rodar a aplicação. Para rodar a aplicação a gente dá um app.launch Está rodando o local, abriu aqui. Então a gente tem agora uma caixinha para subir um arquivo e ele vai jogar o arquivo de saída com o cluster. E a gente pode baixar esse arquivo. Então se eu clicar aqui, eu posso vir aqui em projetos, vou pegar o arquivo que está aqui. Eu vou pegar o próprio dataset completo, vou pegar o mesmo dataset. Então está aqui dataset client.pj. Vou dar um submit e ele gerou um outro arquivo chamado clusters.csv. Se eu vir aqui nesse clusters.csv e clicar e falar para ele baixar. Vou pedir para ele baixar isso daqui. Vou jogar aqui no meu downloads mesmo, tudo bem. Já tem um aqui, pronto. Se eu vir aqui em clusters e abrir isso aqui no meu sublime. Só fazer isso aqui, pronto. Está aqui. As colunas que já tinham o arquivo mais a coluna cluster e ele colocou aqui o cluster nessa coluna. Então a gente conseguiu finalizar essa parte. A gente criou uma aplicação batch que sobe um arquivo, ele devolve um arquivo, eu faço download e ele está ali com a classificação. Então com isso a gente fecha mais um módulo, nossa trilha de IA. Dessa vez dedicada a algoritmos supervisionados de clusterização. Onde a gente começou com o Kamins. Espero que vocês tenham gostado dos módulos e vejo vocês no próximo módulo.

> [voltar](../../../README.md) para a página anterior.