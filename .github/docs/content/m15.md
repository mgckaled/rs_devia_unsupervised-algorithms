# Módulo 14 - Clusterização Hierárquica

> [voltar](../../../README.md) para a página anterior.

## Sumário

- [Módulo 14 - Clusterização Hierárquica](#módulo-14---clusterização-hierárquica)
  - [Sumário](#sumário)
  - [Conceitos e Explicações:](#conceitos-e-explicações)
    - [O que é?](#o-que-é)
    - [Etapas da Clusterização Hierárquica](#etapas-da-clusterização-hierárquica)
    - [O que é Clusterização Hierárquica Aglomerativa?](#o-que-é-clusterização-hierárquica-aglomerativa)
    - [O que é Clusterização Hierárquica Divisiva?](#o-que-é-clusterização-hierárquica-divisiva)
    - [O que é Dendograma?](#o-que-é-dendograma)
  - [Transcriçao das Aulas](#transcriçao-das-aulas)
    - [Bloco 1 - Introdução](#bloco-1---introdução)
      - [Aula 1 - O Que é Clusterização Hierárquica?](#aula-1---o-que-é-clusterização-hierárquica)
      - [Aula 2 - Etapas da Clusterização Hierárquica](#aula-2---etapas-da-clusterização-hierárquica)
    - [Bloco 2 - Algorítimos de Clusterização](#bloco-2---algorítimos-de-clusterização)
      - [Aula 3 - Clusterização Hierárquica Aglomerativa](#aula-3---clusterização-hierárquica-aglomerativa)
      - [Aula 4 - Clusterização Hierárquica Divisiva](#aula-4---clusterização-hierárquica-divisiva)
      - [Aula 5 - Dendograma](#aula-5---dendograma)
    - [Bloco 3 - Projeto Prático](#bloco-3---projeto-prático)
      - [Aula 6 - Apresentacao do Projeto Prático](#aula-6---apresentacao-do-projeto-prático)
    - [Bloco 4 - AED](#bloco-4---aed)
      - [Aula 7 - Carga dos Dados](#aula-7---carga-dos-dados)
      - [Aula 8 - Limpeza dos Dados](#aula-8---limpeza-dos-dados)
      - [Aula 9 - Análise Univariada](#aula-9---análise-univariada)
      - [Aula 10 - Análise Bnivariada](#aula-10---análise-bnivariada)
    - [Bloco 5 - Treinamento do Modelo](#bloco-5---treinamento-do-modelo)
      - [Aula 11 - Transformar Dados para o Modelo](#aula-11---transformar-dados-para-o-modelo)
      - [Aula 12 - Criar Função Otimização Algoritmo Aglomerativo](#aula-12---criar-função-otimização-algoritmo-aglomerativo)
      - [Aula 13 - Execução Otimização Algoritmo Aglomerativo](#aula-13---execução-otimização-algoritmo-aglomerativo)
      - [Aula 14 - Criar Função e Execução Otimização Algoritmo Divisivo](#aula-14---criar-função-e-execução-otimização-algoritmo-divisivo)
      - [Aula 15 - Criar Melhor Modelo e Atribuir Clusters ao Dataset](#aula-15---criar-melhor-modelo-e-atribuir-clusters-ao-dataset)
    - [Bloco 6 - Análise dos Resultados](#bloco-6---análise-dos-resultados)
      - [Aula 16 - Geração e Uso do Dendrograma](#aula-16---geração-e-uso-do-dendrograma)
      - [Aula 17 - Visualizar Resultados da Clusterização](#aula-17---visualizar-resultados-da-clusterização)
    - [Bloco 7 - Entrega App de Consulta](#bloco-7---entrega-app-de-consulta)
      - [Aula 18 - Salvar Modelo e Dataset](#aula-18---salvar-modelo-e-dataset)
      - [Aula 19 - Aplicação de Consulta no Streamlit](#aula-19---aplicação-de-consulta-no-streamlit)

## Conceitos e Explicações:

### O que é?

A clusterização hierárquica é um método de análise de dados utilizado na mineração de dados e na análise de padrões. É uma técnica de agrupamento que busca construir uma hierarquia de clusters. Isso significa que os dados são agrupados em clusters de maneira hierárquica, onde cada cluster pode conter subclusters menores. Aqui estão alguns pontos importantes sobre a clusterização hierárquica:

1. **Divisão Hierárquica**: A principal característica da clusterização hierárquica é a criação de uma hierarquia de clusters, onde os clusters são organizados em uma estrutura de árvore ou dendrograma.

2. **Agrupamento Aglomerativo e Divisivo**: Existem duas abordagens principais para a clusterização hierárquica: aglomerativa e divisiva.
    - **Aglomerativa**: Começa com cada observação como um cluster separado e, em seguida, mescla iterativamente os clusters mais semelhantes até que todos os dados estejam em um único cluster.
    - **Divisiva**: Começa com todos os dados em um único cluster e, em seguida, divide iterativamente o cluster em clusters menores até que cada observação esteja em seu próprio cluster.

3. **Métodos de Ligação (Linkage)**: Na abordagem aglomerativa, um dos aspectos importantes é decidir como mesclar os clusters. Existem diferentes métodos de ligação que determinam a distância entre clusters:
    - **Ligação Completa (Complete Linkage)**: Calcula a distância máxima entre todos os pares de pontos nos dois clusters.
    - **Ligação Média (Average Linkage)**: Calcula a média das distâncias entre todos os pares de pontos nos dois clusters.
    - **Ligação de Ward**: Minimiza a variação total dentro de todos os clusters, escolhendo os clusters que minimizam o aumento na soma dos quadrados de qualquer fusão.

4. **Dendrograma**: É uma representação gráfica da hierarquia de clusters, onde os clusters e a forma como são agrupados são mostrados em uma estrutura de árvore.

5. **Seleção do Número de Clusters**: Um dos desafios na clusterização hierárquica é decidir em quantos clusters dividir os dados. Isso pode ser feito interpretando o dendrograma ou usando métodos de corte para decidir onde dividir a árvore em clusters discretos.

6. **Aplicações**: A clusterização hierárquica é amplamente utilizada em diversas áreas, incluindo reconhecimento de padrões, segmentação de mercado, bioinformática, análise de imagens, entre outros.

7. **Escalabilidade**: Uma desvantagem da clusterização hierárquica é sua escalabilidade em grandes conjuntos de dados. O custo computacional pode ser alto, especialmente para métodos aglomerativos, onde a matriz de distâncias completa precisa ser calculada.

Em resumo, a clusterização hierárquica é uma técnica poderosa para agrupar dados de forma hierárquica, permitindo uma análise mais detalhada das relações entre os elementos.

### Etapas da Clusterização Hierárquica

Para realizar um algoritmo de classificação hierárquica, é necessário definir a distância entre os objetos, construir uma matriz de distância, criar um dendrograma e escolher o número de clusters cortando o dendrograma em uma certa altura. A métrica de similaridade é baseada na distância escolhida, como a euclidiana ou manhattan. O dendrograma pode ser construído de forma aglomerativa, unindo clusters próximos, ou divisiva, dividindo um único cluster em dois até que todos os objetos tenham seu próprio cluster.

### O que é Clusterização Hierárquica Aglomerativa?

O algoritmo de clusterização hierárquica aglomerativo é explicado, destacando a inicialização dos clusters individuais, cálculo da matriz de distância, combinação dos clusters mais próximos, atualização da matriz de distância e repetição do processo até formar um único cluster. A construção do dendrograma é mencionada como forma de visualizar a formação dos clusters e determinar o número final de clusters.

### O que é Clusterização Hierárquica Divisiva?

O algoritmo de classificação hierárquica divisivo trabalha de cima para baixo, dividindo um cluster em sub-clusters com base em um ponto de corte na matriz de distância. Esse processo é repetido até que cada ponto seja seu próprio cluster ou um número desejado de clusters seja atingido. Pode-se usar algoritmos como K-Means ou DBSCAN para essa divisão. Um dendrograma pode ser construído para ilustrar a progressão da divisão dos clusters.

### O que é Dendograma?

O dendrograma é uma representação visual de agrupamentos hierárquicos de dados. Ele é composto por eixos x e y, nós que dividem clusters e ramos que subdividem os nós. Cortes no dendrograma determinam o número de clusters. O dendrograma é útil para análise exploratória, identificação de outliers e determinação do número de clusters em um processo de agrupamento. A estrutura do dendrograma ajuda a entender a estrutura dos dados e a visualizar a formação dos grupos.

> [Acesso aos slides](../pdf/ppts_m15.pdf) do conteúdo do módulo.

## Transcriçao das Aulas

### Bloco 1 - Introdução

#### Aula 1 - O Que é Clusterização Hierárquica?

E o que é clasterização hierárquica? Bom, ela é uma técnica distinta dentro dos métodos de agrupamento no aprendizado de máquina não supervisionado. A principal diferença dela é a forma como os clusters são estruturados e também a flexibilidade na questão do número de clusters. Além disso, ela também é particularmente útil para você fazer uma análise exploratória em algumas situações onde a relação entre os pontos de dados é mais importante do que a formação de grupos distintos com fronteiras claras, como a gente vê em alguns outros métodos de aprendizado de máquina não supervisionado, especificamente de agrupamento ou clusterização. Esse método de clusterização hierárquica também é apropriado para conjuntos de dados onde as relações entre os dados podem ser representadas em múltiplas escalas ou níveis de agregação. Aqui algumas características da clusterização hierárquica. Bom, ele possui uma estrutura de árvore, ou hierárquica, que é bem diferente do que a gente vê em métodos como o CAMINS, onde eu parto de uma definição inicial de quantos clusters são formados e, no caso hierárquica, ele cria uma árvore de clusters que a gente chama de dendograma. Essa árvore pode ser interpretada de diversos níveis de granularidade, permitindo uma visão mais detalhada ou mais agregada, conforme necessário. Como a gente pode ver nesse gráfico aqui, eu vou explicar o dendograma mais pra frente com mais detalhes. Uma outra característica é que ele não requer, a priori, o número de clusters, porque você define os clusters que você quer, apesar do algoritmo do SeqTilane permitir que você coloque isso, você não precisa definir isso, você pode definir a altura ou a distância ao qual você quer cortar o seu dendograma, eu vou explicar isso também mais pra frente. Mas, a ideia aqui é que, em um agrupamento hierárquico, o número de clusters é determinado após a construção do dendograma, através da sua análise e do ponto do corte escolhido. Uma outra característica em relação à clusterização hierárquica é a sensibilidade a mudanças nos dados. No agrupamento hierárquico, ele constrói os clusters baseados em etapas sucessivas de aglomeração ou divisão, o que torna sensível a ordem dos dados e as outliers. Isso pode resultar em clusters diferentes se a ordem dos dados for alterada na hora de você aplicar o algoritmo. Por exemplo, de um método como o dbScan, que é um método baseado em densidade, que são mais robustas as outliers e a ordem dos pontos. Fazendo uma comparação com o CAMINS que a gente viu no módulo anterior, o CAMINS é bastante eficiente em grandes conjuntos de dados e para clusters que a gente chama de forma esférica, mas ele fala em capturar clusters de formas complexas ou tamanhos variados. Por outro lado, o agrupamento hierárquico pode identificar essas estruturas mais complexas, embora seja menos eficiente em termos de tempo computacional para grandes conjuntos de dados. O CAMINS pode resultar em diferentes soluções dependendo dos centroides iniciais. Quando a gente inicializa o CAMINS, ele inicializa os centroides de forma aleatória. Então, dependendo da forma como esses centroides são inicializados, eu posso ter soluções diferentes. Já na clusterização hierárquica, ela tem uma saída mais estável em uma hierarquia construída, mas é mais sustentível a ser influenciada por outliers.

#### Aula 2 - Etapas da Clusterização Hierárquica

E quais são as etapas para que a gente realize um algoritmo, a gente execute um algoritmo de classificação hierárquica. Primeiro passo é a gente poder definir, a gente precisa definir qual é a distância, a medida, a métrica de similaridade que é conectada à distância que a gente quer usar entre os objetos da nossa análise. Então a gente pode usar da mesma forma que no camisa, a distância euclidiana, manhattan ou uma outra distância, e com base nessa distância que a gente vai definir a nossa métrica de similaridade, elas podem ser específicas do domínio que você está trabalhando para definir o quão similares os instintos são aos objetos. Depois de definida essa similaridade, essa métrica de similaridade, a gente precisa construir o que a gente chama de uma matriz de distância, onde ele calcula a distância entre cada par de objetos no conjunto de dados, resultando nessa matriz. Depois a gente constrói o que a gente chama do dendrograma, essa representação, pode ser construído de duas formas, no modelo aglomerativo, ele começa uma abordagem bottom up, ou seja, cada objeto é tratado como um cluster individual e ele vai se fundindo com os clusters mais próximos, de acordo com as suas distâncias, para formar novos clusters. No caso divisivo, ao contrário, uma abordagem top down, eu inicio com um único cluster que inclui todos os objetos e o dendrograma vai dividindo esse cluster em dois, vai dividir em dois, até que ele consiga dividir todos os objetos e que eles façam parte, eles tenham cada um deles no seu próprio cluster. Depois que fez essa construção do dendrograma, a gente escolhe o número de clusters, cortando o dendrograma em alguma certa altura, se a gente, por exemplo, cortasse ele aqui, a gente conta a quantidade de nós, de ramos, na verdade, e essa quantidade de ramos, um ponto de corte, é o que define a quantidade, o número final de clusters que eu vou trabal

### Bloco 2 - Algorítimos de Clusterização

#### Aula 3 - Clusterização Hierárquica Aglomerativa

Vou falar agora sobre o algoritmo de clusterização hierárquica, o aglomerativo, quais são as etapas para a gente rodar esse algoritmo, lembrando que ele é um algoritmo bottom-up. Então primeiro, inicialize o algoritmo onde cada ponto de dado é tratado como um cluster individual. Faça o cálculo da matriz de distância, que contém as distâncias entre todos os pares de ponto, e aí eu encontro os clusters que estão mais próximos uns dos outros com base na matriz de distância e com base na métrica de similaridade que a gente definiu e combino esses dois clusters, esses dois pontos num novo cluster. Feito isso, eu preciso atualizar a matriz de distância para refletir a distância desse novo cluster que foi formado e os demais clusters que estão presentes no seu conjunto de dados. Esse processo de encontrar os clusters mais próximos, juntá-los e atualizar a matriz de distância vai sendo repetido até que todos os pontos de dados estejam no mesmo cluster, ou seja, eu parto daqui até o momento que eu tenho um cluster só aqui no alto, que é como se fosse a minha raiz da minha árvore, da minha hierarquia. A construção do dedograma pode ser realizada ao mesmo processo para visualizar a formação dos clusters e decidir qual o melhor ponto de corte que determina o número final de clusters.

#### Aula 4 - Clusterização Hierárquica Divisiva

Falando agora do algoritmo de classificação hierárquica divisivo, ele trabalha na abordagem repetindo top-down, ou seja, eu parto de um único cluster que tem todos os pontos lidados. Ele é o cluster de nível mais alto. Da mesma forma eu faço um cálculo da matriz de distância, com base na métrica de similaridade que foi definida, e identifico o ponto de corte. E determino como dividir o cluster escolhido em dois sub-clusters. Então aqui eu defino qual ponto de corte eu vou cortar esse cluster. Vamos imaginar que aqui eu tenho uma raiz do cluster, em dois outros clusters. Depois que eu defini esse critério, eu aplico esse critério de divisão para separar um cluster, que está aqui no alto, em dois, e vou fazendo isso até que eu tenha cada ponto de dados dentro do próprio cluster. Nesse caso, para executar essa divisão, eu posso usar tanto o algoritmo CAMINS ou como o DB-SCAN para fazer isso. Repito esse processo de selecionar e dividir clusters até que cada ponto tenha seja o seu próprio cluster, ou até que se atinja um número desejado de clusters. Durante o processo eu também posso construir um dedograma que lustre como os clusters estão sendo divididos progressivamente.

#### Aula 5 - Dendograma

Agora eu vou mostrar para vocês uma estrutura do dendrograma, quais são os componentes que fazem parte de um dendrograma. Então o primeiro deles são os eixos, eu tenho um eixo x que é composto pelas observações ou quando esse dendrograma está em um nível mais alto, ele agrupa aqui embaixo a quantidade de objetos que fazem parte daquele cluster específico, mas pensando num dendrograma que traga todos os níveis, o que a gente vai encontrar aqui embaixo são as observações ou os objetos de dados do nosso dataset. E aqui no eixo y a gente encontra a distância, a nossa métrica de similaridade que a gente está analisando para fazer a nossa divisão, a nossa construção da nossa árvore, nossa hierarquia. Um outro componente importante são os ramos e os nós, os nós são os pontos que na verdade dividem um cluster em mais de um cluster, então esse ponto aqui e esse ponto aqui a gente chama de nó porque é o ponto onde um cluster está se dividindo em dois. Além disso a gente tem os ramos que na verdade são as subdivisões de cada um dos nós, então aqui eu tenho um ramo, aqui eu tenho um outro ramo. Uma outra coisa importante que eu falei algumas vezes são os cortes, então se eu tenho um ramo e eu preciso definir a quantidade de clusters, o que eu faço é definir em que ponto do meu gráfico, no meu dendrograma, eu quero cortar e baseado na quantidade de ramos que esse corte fez é que eu tenho a quantidade de clusters. Então nesse caso aqui, se eu fizer um corte aqui nessa altura, mais ou menos talvez 35 de distância, eu vou chegar num K, numa quantidade de clusters igual a 3, eu tenho um ramo, dois ramos e três ramos passando por esse corte. Se eu descer um pouco essa distância para próximo de 30, 31, eu já vou encontrar um K igual a 6, então vou ter aqui um ramo, dois ramos, três, quatro, cinco, seis ramos. Então isso significa o seguinte, tudo que está abaixo desse ramo, ou seja, toda essa parte vermelha que está aqui nesse desenho, vai fazer parte de um cluster só. Tudo que está abaixo desse ramo, que seria essa parte verde, vai fazer parte de um cluster. Aqui tudo que está aqui abaixo vai fazer parte do cluster, que é o azul, aqui o roxo que está abaixo desse, aqui o amarelo que está abaixo desse e aqui o rosa que está abaixo desse. Se eu descer um pouquinho mais, chegando próximo do 29, eu já encontro aí um K, uma quantidade de clusters igual a 7, porque aqui esse cluster aqui, nesse ponto, ele é um só, mas nesse ponto aqui já são dois clusters, eu já subdividi ele, então eu teria um, dois, três, quatro, cinco, seis, sete clusters. Então aqui, esse pedaço aqui desse cluster, ainda que está em vermelho, seria um cluster, mas esse pedacinho aqui um pouco menor, já seria um novo cluster, já atribuía um novo cluster. Então, essa é a estrutura, pensando visual, de como a gente interpreta um dendograma. E aí, quais são as aplicações que a gente pode ter para um dendograma? A gente pode usar o dendograma para fazer uma análise exploratória, ele ajuda os analistas, os cientistas de dados, a entender melhor a estrutura dos dados e visualizar como que os grupos são formados e quão semelhantes ou diferentes eles são entre si. Uma outra coisa importante quando você tem um dendograma, é que você consegue determinar o número de clusters, porque a partir do momento que você fizer o corte, dependendo da granularidade que você quer ter daquele dendograma, você consegue determinar melhor o número de clusters que você vai ter no seu processo de agrupamento. E um outro aspecto importante também é a identificação de outliers. Os outliers, ou os pontos anômalos, muitas vezes aparecem como ramos isolados do dendograma, facilitando a sua identificação e a sua análise.

### Bloco 3 - Projeto Prático

#### Aula 6 - Apresentacao do Projeto Prático

Falando agora no nosso projeto para aplicar os conceitos de Clustering Hierárquico, o projeto que eu trouxe aqui é usar um Clustering Hierárquico de uma forma que eu posso dizer que é um pouco ortodoxa, porque a ideia aqui é usar um Clustering Hierárquico para fazer o que se costuma chamar de um sistema de recomendação ou um Rexis. Não é uma aplicação comum, apesar de que em Ciência de Dados a gente está sempre explorando formas diferentes e muitas vezes você usa uma ferramenta que comumente serve para uma coisa e aplica ela em outra coisa, e foi exatamente isso que eu quis trazer aqui. Existem várias outras técnicas de você fazer sistemas de recomendação, mas o objetivo aqui foi o seguinte, eu trouxe um Dataset de um Marketplace, e esse Marketplace tem uma série de Laptops, Notebooks de diversas marcas, diversas configurações, diversos tipos de processadores, esse Marketplace tem um estoque limitado para a pronta entrega desses produtos. O que acontece, quando tem algum item que não está em estoque, porque ele tem um estoque limitado, o cliente faz o pedido, o que ele tem que fazer, ele tem que fazer um pedido junto ao fabricante, ou ao distribuidor, e isso acaba gerando um atraso, e o cliente se frustra, porque ao invés de ele receber um Laptop que está em estoque, ele vai ter que esperar esse Laptop ser solicitado, faturado, para poder ser entregue a ele. Então qual que é a ideia, que o Marketplace quer implementar um sistema de recomendação inicial que dê sugestões de alternativas com configurações similares aos produtos que estiverem disponíveis. Então, se ele seleciona um Notebook que está com um estoque muito baixo, ou pode ter acabado o estoque, ele pode ver Notebooks ou Laptops que se pareçam com aquele, e o cliente ao invés de ele falar, não vou esperar 10 dias por esse Laptop, eu posso comprar esse outro que se parece com o que eu quero, e receber esse Laptop em pronta entrega. Então pra isso a gente vai fazer o que, a gente vai desenvolver um algoritmo de classificação hierárquica, onde a gente vai agrupar os equipamentos desses Laptops, semelhantes com base nas suas especificações, e a gente vai ver isso melhor no código, quais são essas especificações e características. Além disso, a gente vai criar uma interface de consulta, baseado nessa clusterização ou agrupamento que vai ser gerado, de forma que o cliente possa encontrar facilmente um produto alternativo, dentro do mesmo grupo do item selecionado, ou seja, se eu tenho um Notebook que tem uma característica de ter 8 GB de memória, 512 GB de SSD, que tem uma placa dedicada de vídeo de 4 GB, que tem uma tela de 14 polegadas, a ideia que a gente vai ver no algoritmo, é se ele consegue capturar desses modelos todos, modelos que sejam similares, para que ele possa ter alternativas, caso aquele modelo exato que ele queira não esteja disponível. E aí, qual vai ser a estrutura do projeto? Bom, a gente vai fazer a carga dos dados, vamos fazer uma análise exploratória de dados, algumas variáveis categóricas e variáveis numéricas, vamos treinar e validar esse modelo com métricas, usando o conceito de hyperparamétricos, para que a gente possa obter a melhor métrica possível e ter o nosso melhor modelo possível. A gente vai salvar esse modelo, vamos salvar um dataset novo, já com os clusters atribuídos a cada um dos modelos, e aí com base nesse dataset, a gente vai criar, vai entregar um app, uma aplicação, para fazer a consulta desses laptops e poder ver que laptops são similares àqueles que a gente escolheu. Então sem mais delongas, agora vamos para o código.

### Bloco 4 - AED

#### Aula 7 - Carga dos Dados

Fala Devs! Vamos começar agora a parte prática do nosso módulo de classerização hierárquica, com aquele projeto que eu expliquei do Marketplace de Laptops. Eu já estou com o ambiente configurado aqui no Visual Code onde eu tenho aqui uma pasta com os meus datasets, com o dataset e uma explicação aqui do qual é o conteúdo desse desse dataset. Então vou começar criando um arquivo, vou chamar ele de classterização-laptops.ipynb Como eu comentei já em módulos anteriores, eu já estou com o meu ambiente totalmente configurado em termos de bibliotecas. E eu já vou copiar aqui uma colinha de quais são as bibliotecas que eu estou usando. Então eu estou usando o Pandas, Seaborn, Scikit-learn, Matplotlib, SciPy, WiPyGuide, Gates, WiPyKernel, Optuna e Streamlit. Na verdade faltou uma aqui que é o PyPlot. Também estou usando o PyPlot. Então vamos começar importando as bibliotecas. Então primeiro bibliotecas de DA e visualização de dados. Então vou importar aqui o Pandas, import pandas.spd, import seaborn.sns, import plotly.express.spx, from matplotlib, import pyplot.splt. Bibliotecas de Machine Learning. Eu vou importar a biblioteca, as duas bibliotecas que eu vou usar, eu vou fazer um experimento tanto com clustering aglomerativo quanto com clustering divisivo, para a gente comparar os resultados. Então sklearn.cluster, import, eu vou usar o aglomerative clustering e vou usar o bisecting k-means, que é o divisivo. Em termos de métricas do sklearn, eu vou usar o silhouettescore como métrica para validar os meus clusters. Vou usar os módulos de pipeline para processing para fazer a transformação das variáveis. Então vou dar aqui o import standardscaler, hotencoder, e vou importar o sklearn.compose, vou importar o column transformer. Vou importar o numpy.snp e vou importar o módulo do scipy para a questão do dendograma. Cluster hierarquia, hierarquia, import, dendogram, linkage e cuttree, mostrar como é que a gente faz o corte da nossa hierarquia da nossa hierarquia. E para a parte de otimização a gente vai usar o obtuner. Vou escolher aqui um ambiente, deixa eu ver o que faltou aqui, scipy cluster hierarquia, importadas as bibliotecas. A primeira coisa que a gente vai fazer é carregar os dados. Então já vou colocar aqui um markdown, quero carregar os dados, e nós vamos carregar, vou trocar aqui para python, então carregar dataframe. Vou chamar de dflaptops, vou fazer um pd read7v e vou importar aqui .datasets .dflaptops/.laptopsnew. Importei e agora já posso começar a analisar a estrutura dele, então analisar a estrutura. Então posso vir aqui dflaptops.info para ver as colunas. E aí antes de eu visualizar alguns dados eu vou explicar essa estrutura desse dataset, tem bastante coluna. Então tem aqui um arquivo chamado metadata.laptops que vai seguir para vocês lá no plataforma. Então esse dataset é composto dessas colunas. Tem uma coluna chamada index, que é como uma identificação única do registro. Brand, que é a marca do laptop, então no caso Dell, HP, Acer, enfim. Qual modelo, tem uma descrição completa do modelo do desktop, então por exemplo é um Acer, modelo tal, que tenha tanto de memória, que tem tanto de disco, tem uma descrição, vou mostrar alguns exemplos. O preço do laptop, que aqui no caso está em dólar. O rating, é um rating de 0 a 100, dado ao laptop, então quanto maior, melhor o rating, melhor a avaliação desse laptop. Qual a marca do processador, então vamos imaginar, você tem lá Intel, AMD, você tem a da Apple, Silicon, então você tem as marcas dos processadores, as marcas dos processadores. O tipo do processador, então você tem lá, Core i3, Ryzen 5 e por aí vai. Quantidade de cores, que a máquina tem. A quantidade de threads, que a máquina tem. Quanto de memória tem em gigabytes. Qual o tipo de armazenamento principal que essa máquina tem, então pode ser SSD, HDD, HD mecânico, HD SSD. Qual a capacidade desse armazenamento em gigabytes. Qual que é a brand da GPU, se é uma GPU da NVIDIA, da AMD, da própria Apple. Qual o tipo de GPU, se é uma GPU integrada ou dedicada. Tem um booleano aqui, uma variável binária, dizendo se o computador tem touchscreen ou não, se a tela é touchscreen. Qual o tamanho do display em polegadas, então 15 polegadas, 14 polegadas, 11 polegadas. Qual a resolução na largura, a resolução na altura em pixels. Qual o sistema operacional e quanto tempo de garantia tem essa gráfica, também em anos. Um ano, dois anos, enfim. Então essa aqui é a estrutura desse dataset. E aí eu quero mostrar para vocês alguns dados desse dataset. Então, visualizar os primeiros registros. Então vem aqui, dflaptops.head de 10. Então ele mostra aqui o índex, que é esse índice aqui, a brand, tecno, hp, acer. O modelo, tá vendo que é uma descrição bem completa. Então acer, extensa, modelo, tem o nome do modelo, qual é o tipo de core que tem. Então 12ª geração do core 5. Aqui é um Lenovo, aqui é um Apple Macbook Air de 2020. Esse aqui é o nome do modelo. Então tem uma descrição bem completa. Aqui o preço em dólar, o rating, é um valor inteiro. Aí aqui vem o que eu falei, a brand, AMD, Intel. Aqui ele considera como Apple, apesar de que na verdade esse computador Macbook Air, na verdade o processador é Intel, mas ele definiu como Apple. Qual o tipo de processador, na verdade aqui coloca o 2020. Na verdade esse aqui é um Air M1, então por isso que tá Apple aqui. E aí tem todas as informações aqui, com as informações de cada um desses notebooks. E vamos visualizar aqui embaixo, os registros finais, os últimos registros, dflaptops.tail e 10. Então mesma coisa também, nada de mudança de estrutura, se tem todo o screenshot ou não, verdadeiro ou falso. Tempo de garantia, um ano de garantia, dois anos de garantia, sistema operacional Windows, e por aí vai. E aí que a gente pode derivar daqui, começar a derivar aqui para frente na área do exploratório, entender um pouco das variáveis numéricas, não todas, porque algumas acho que talvez a gente não precisava tirar uma média da resolução da máquina, mas é interessante a gente analisar a price, analisar o rating da máquina, e tentar tirar algumas conclusões antes de começar a partir para o modelo propriamente dito. Então concluímos essa parte inicial aqui de configuração das bibliotecas, carregamento do data set, e vejo vocês no próximo vídeo.

#### Aula 8 - Limpeza dos Dados

Vamos a sequência agora, o nosso projeto prático, iniciando a parte da EDA, da análise exploratória de dados. Então, vou criar aqui uma célula, uma célula Markdown, EDA. Primeira coisa que eu tenho interesse aqui em fazer, é fazer uma análise estatística de algumas variáveis. Você poderia selecionar quais são as variáveis, como comentei anteriormente, avaliar a média da capacidade, a média do tamanho do display, talvez agora não seja tão importante, mas principalmente a questão das variáveis de preço e de rating, que é a avaliação. Vou colocar aqui um Deftesktops, Laptops, Describe. Describe faz o que? Ele gera uma estatística de contagem, média, pedesvio padrão, mínimo, primeiro quartil, segundo quartil que é a mediana, terceiro quartil e o valor máximo. Então, o que a gente percebe aqui principalmente é a questão do preço e do rating. Como eu comentei, os ratings vão de 0 a 100, mas a gente encontra aqui nessa base, ratings que vão de 24 a 89. Então, tivemos máquinas com rating bem baixo e máquinas com rating bastante alto. E outra coisa que chama atenção aqui é a questão do preço. A gente tem máquinas muito baratas, máquinas de 100 dólares até máquinas de 5 mil dólares. Podem ser máquinas gamer, podem ser algumas máquinas da Apple, apesar que hoje em dia não sei se tem alguma máquina que chegue, laptop que chegue nesse valor, talvez com uma configuração ultra especial. Mas você tem aqui um range bem grande, tem inclusive um pedesvio padrão bastante alto aqui entre esses valores de preço. Em termos de cores, como eu comentei, você tem lá máquinas com 2 cores, máquinas com 6, 8, 10 até 24 cores. Memória também RAM não varia muito, você tem máquinas, laptops que são mini laptops, ou o que a gente chamava antigamente, não eram notebooks, mas era um mini notebook que você tinha uma baixa capacidade para conseguir carregar esse notebook em qualquer lugar, com 2 GB de memória e até máquinas com 36 GB de memória. Em termos de capacidade de HD, também eram máquinas com 32 GB de capacidade, com máquinas de até 2 TB de capacidade. Em termos de display também, máquinas com tela de 10 até tela de 18, também nada fora do padrão. Se a gente tivesse encontrado uma máquina aqui, por exemplo, com 100 de display size, opa, isso aqui seria um outliers, pode ser um erro. Mas nada aqui está muito fora do comum, talvez o preço de uma máquina de 5.450 seja um problema, mas tem máquinas, laptops que estão nessa faixa de preço. O que eu gostaria de avaliar aqui, em relação a estrutura das variáveis, voltando aqui no info, é que é analisar um pouco do tipo das variáveis. Então a gente vê aqui que marca e modelo é texto, o preço é um float, o rating é um inteiro, a brand e a tier são textos também, ou tal como object. O tipo primário do HD também é um object, é inteiro. GPU brand e GPU type é object. O width, height e screen ele está considerado como booleano, está ok. O display size é um float, porque ele é expresso em polegadas, então geralmente o computador pode ter 13.3 polegadas, 14.4, o resolução é inteiro. O sistema operacional é um texto, Windows, Mac e year of warrant, ou seja, tempo de garantia. E aí essa variável chama atenção, porque ela é uma variável que está como object. E como ela é tempo de garantia, em anos, então deve ter algum valor aí que deve estar baleando essa variável, vamos pensar assim, e transformou essa variável em uma variável object ao invés de uma variável inteira. Então é uma coisa que a gente vai precisar tratar antes da gente poder rodar o algoritmo. E como é que a gente vai fazer isso? A gente vai inspecionar quais são os valores únicos dessa variável. Então, vou olhar aqui, checar, variável, year of warrant. E aí a gente vai fazer, def laptops, vamos colocar aqui a variável, year of warrant, ponto, unique. E aí ele traz para a gente a seguinte informação, que tem computadores com 1 ano de garantia, 2 anos de garantia, 3 anos de garantia, e tem computadores que não tem informação de garantia. E aí ao invés de ele trazer um número, ele trouxe uma informação, um no information, e aí acabou transformando a variável em uma variável texto. Como naturalmente a gente já faria um trabalho de transformar as variáveis categóricas em variáveis numéricas, através de um hot encode, a gente já vai fazer o tratamento dessa variável, para que ela pelo menos fique dentro do padrão que a gente gostaria. Então, como é que a gente vai fazer isso? No caso aqui, eu vou optar pelo seguinte, de transformar uma variável, Toda vez que eu encontrar o no information, o que eu vou fazer é colocar zero. Se eu não tenho informação, eu vou considerar que o tempo de garantia não tem garantia. É bem difícil, quase todas as máquinas tem, mas isso é uma questão que você pode questionar. Outra forma seria, por exemplo, ver a maioria, pegar qual é a maioria, o que a gente encontra mais de garantia. A gente encontra um, dois ou três, e aí aplicar isso dentro dessa máquina, das máquinas que estão com essa informação. Mas eu vou assim, considerando o conhecimento que eu tenho do problema, imaginar que eu conversei com alguém, olha, quando não tem informação, é preferível colocar como não tem garantia. Só tem a garantia, vamos dizer, da compra lá dos sete dias. Mas isso pode ser alterado, se você quiser alterar para um ou para dois, não tem problema. Se você alterar para um ou para dois, o que vai acontecer é que ela vai se encaixar em outros clusters. Então, toda máquina, a gente pode até considerar isso, toda máquina tem pelo menos um ano de garantia. Então, vamos considerar um ano. E aí, na hora que eu for fazer o clustering e vou considerar essa informação de garantia como uma parte da informação, da característica da máquina, essa informação sendo um, ela vai acabar se agregando a outras máquinas que são um. Obviamente, considerando outras características. Então, a gente pode até fazer isso, em vez de colocar o zero, a gente colocar um, que a gente entenda assim, pelo menos um ano de garantia a máquina vai ter. Então, o que eu estou fazendo aqui? Eu estou usando o comando lock, que na verdade ele serve para você localizar registros que atendam a um certo critério. Ou você pode informar de forma categórica uma chave, vamos supor que você queira uma linha, então você pode falar os valores daquela linha, ou você pode colocar uma condição dentro dessa função. Então, aqui eu estou colocando uma condição. Então, eu vou dizer o seguinte, localize todos os valores de DFLaptop da coluna YearOfWarrant, que forem iguais a nenhuma informação. E aí, o que eu quero fazer? Eu quero pegar e atribuir a variável YearOfWarrant um valor. E aí, eu escolho qual valor. Vamos supor que eu queira colocar aqui o valor 1. E aí, tem outra coisa. Como essa variável está do tipo Objetivo, eu também tenho que transformar essa variável em inteira. Então, eu venho aqui e coloco Laptops.YearOfWarrant. Eu preciso transformar essa variável em uma versão dessa variável inteira. Então, o que eu faço aqui? Eu atribuo a ela mesma, só que eu uso o comando asType. Ou seja, pegue a conta dessa variável como se fosse tipo inteiro e armazene nessa própria variável. É isso que eu estou fazendo. Então, primeiro ajustando aqui o ano de garantia, quando for NoInformation para 1, considerando que pelo menos um ano de garantia cada máquina tem no mercado. E depois transformando essa variável em uma variável inteira. Então, se eu fizer isso aqui. Inclusive, eu fizer esse Describe, agora a variável YearOfWarrant já passa a figurar aqui. Então, eu tenho aqui ela também sendo computada. Ela não estava sendo computada antes porque ela estava como Objetivo. E se eu trocar aqui o Info, ele vai mostrar o YearOfWarrant igual a inteiro. Então, eu fiz esse primeiro ajuste. Outro ajuste que eu costumo fazer também, a gente até fez em uma outra ocasião, é transformar a variável do IsTouchedScreen, que ela está como booleana, True ou False, para uma variável inteira. Então, transformar variável booleana, uma variável binária, em Int. No caso aqui, a IsTouchedScreen. Então, o mesmo conceito. Def Laptops. Is. Nesse caso, eu não preciso fazer uma substituição. Eu só estou fazendo uma substituição de tipo, não de valor. Então, aqui o que eu estou dizendo é, IsTouchedScreen é igual a ele. Cujo tipo aqui é um S-Type também. Ou seja, está convertendo o tipo booleano para um tipo inteiro. E ele converteu. Então, se eu vier aqui e colocar também de novo aqui o Info, a variável IsTouchedScreen passa a ser uma variável inteira e não mais uma variável booleana. Então, fizemos essa primeira parte de analisar as estatísticas das variáveis. Fazer um ajuste em duas variáveis. Essa Warrant, que estava com uma sujeira aqui em relação a quantidade de anos que ele tem de garantia. E também já transformamos uma variável booleana inteira para facilitar também na hora de fazer o uso do algoritmo. Então, com isso a gente termina essa parte inicial do EDA. Vejo vocês no próximo vídeo. Tchau.

#### Aula 9 - Análise Univariada

Olá, Davi! Dando sequência ao nosso EDA, a gente vai começar agora a explorar algumas variáveis, tanto variáveis categóricas quanto variáveis numéricas. A primeira delas que eu gostaria de explorar, que pode ser uma variável que de certa forma seja bastante usada ou influencie bastante no trabalho do clustering, nesse clustering, é a variável brand, a marca. Muitas vezes quando você está escolhendo um computador, geralmente você tem ali algumas marcas favoritas, então esse pode ser um fator de decisão, então eu queria avaliar se existe alguma concentração de marcas dentro desse dataset. Uma coisa que eu esqueci de mencionar anteriormente, é que a gente vai trabalhar com um conjunto gigante, mas a gente está trabalhando aqui com praticamente 991 máquinas dentro do estoque disponível para esse marketplace, então é uma quantidade razoável para a gente poder explorar se o algoritmo vai conseguir performar bem. Então a primeira coisa que eu quero avaliar aqui é a distribuição da variável brand, que é a marca. Eu quero saber como é que está mais ou menos se tem alguma concentração de máquinas em uma certa marca. Então eu vou criar uma variável chamada percentual brand, onde eu vou fazer um cálculo da quantidade de máquinas, então eu vou fazer um value count pela coluna brand, marca, e vou dividir essa contagem pela quantidade de registros que eu tenho no meu dataset, e multiplicar por 100 para ter o percentual. Então eu vou gravar isso em uma variável, e depois eu vou gerar um plot usando o plotly, onde eu vou colocar o percentual brand, e vou colocar a cor como sendo o percentual brand ponto índice, para que cada marca fique em uma cor diferente. Vou rodar aqui, pronto. Então ele mostra aqui pelo gráfico, fica fácil de ver, que ele tem aqui uma concentração bem grande de pelo menos 3 modelos. Modelos da ASUS, que representam 21% da base, modelos da HP que representam mais 20% da base, quase 21%, e aqui modelos da Lenovo que representam 20%. Então nós estamos falando aqui basicamente de que 3 marcas cobrem praticamente 63%, 62% das máquinas que estão nesse dataset. Aí você tem aqui a Dell com 10%, você tem a MSI com mais 9%, e você tem a Acer com mais 9%. Ou seja, com praticamente 6 máquinas, 100 brands, você cobriu muita coisa. Aqui 60% mais 70%, 80%, 90%, quase que 90% das máquinas estão aqui concentradas nessas 6 marcas. Aí você tem uma Apple que tem 1%, uma Infinix, uma Samsung, uma LG, e aí você tem outras marcas que a gente consegue ver aqui pela legenda. Então aqui consegue identificar que há uma concentração em poucas brands. Continuando nosso estudo, a gente vai fazer a mesma coisa agora para a questão do processador. Não existe tantas versões de processador assim, de processor brand, mas eu queria avaliar se existe também alguma concentração. Então nós vamos fazer a mesma coisa, distribuição da variável processor brand. Para a gente não ter que digitar tudo de novo, a gente pode fazer exatamente a mesma coisa aqui, só trocando as variáveis. Então processor brand, eu quero ver também a quantidade de registros por processor brand. E eu vou criar aqui um plot para o processor brand. E ele gerou. Então aqui ele tem grande concentração da Intel, então mais de 70% das máquinas que estão aqui são de processador Intel. Temos aqui 26% de processador AMD, temos 1,5% de processador considerado como processador da Apple, e outras marcas, menos de 1%, menos de 0,5% de outras marcas. Então aqui também tivemos um caso mais presente ainda nas duas principais marcas do mercado, vamos pensar Intel e AMD. A concentração está feita nessas duas variáveis. Saindo um pouco agora de variáveis categóricas, gostaria de explorar agora variáveis numéricas. Avaliar a distribuição de algumas variáveis para entender o comportamento do nosso data set. Então eu vou avaliar primeiro a distribuição da variável. Price, que é o preço. Para isso eu vou usar o Seaborn, porque o Seaborn já tem um gráfico que me mostra tanto o histograma, mas ele também já mostra o KDE, o Kernel Density Estimation, que mostra uma curva que acompanha o histograma. Fica fácil de entender como que a curva, o nosso histograma está plotado com essa função. Que é uma coisa que no Plotly tem, mas você tem que construir eles, tem que fazer 2, 3 plots para chegar na mesma informação. Então, como aqui estou fazendo uma análise exploratória mais rápida de dados, então eu vou usar a biblioteca que for mais conveniente para aquilo que eu preciso. Então a gente vai usar aqui o HistPlot, o plot de histograma. Estamos colocando a variável Price, usando a função KDE que mostra a curva, que acompanha as barras do histograma. Vou usar uma cor aqui chamada Light Blue, um azul claro. Um DFLaptops, vamos ver aqui o que aconteceu. SnHistPlot, Laptops, DFLaptops. Aqui é, coloquei parênteses. Pronto, aí ele mostra aqui o gráfico. Então, vejam o gráfico, que ele mostra o seguinte, a maioria dos Laptops, esse é um histograma que mostra o que a gente chama de uma assimetria positiva, ou seja, a cauda da minha curva está para o lado direito. Então, isso que a gente chama de uma distribuição assimétrica, lembrando lá do módulo de estatística para Davis, lá no início da nossa trilha, essa aqui, pegando uma medida de assimetria, uma assimetria chamada assimetria positiva, cuja cauda, que a gente chama, está ao lado direito dessa curva. Com preços que a gente pode considerar como preços moderados, aqui na faixa dos R$800, R$700. Mas com alguns outliers, como a gente também viu lá no Describe, que mostra as estatísticas, com alguns preços bem altos. Então, a gente tem aqui alguns notebooks, não são poucos, aparecendo aqui na faixa dos R$4.000, um pouco menos, e até chegando aos R$5.000, um pouco mais do que isso. Então, essa é a característica do que a gente tem nessa curva, preços em geral moderados, mas com alguns outliers aqui com preços altos. Vamos olhar agora para a variável Rating, que é uma outra variável que pode influenciar. Estou pegando as variáveis que podem mais influenciar o processo do clustering, ou seja, a marca, porque eu estou falando de um sistema de recomendação, pelo menos uma improvisação de um sistema de recomendação usando um clustering, onde marcas, se você vai pegar um computador de uma marca, e você tem outro similar da mesma marca, provavelmente você vai escolher, se você já tem essa preferência. Se você tem uma máquina que você sabe que a avaliação dela é alta, você talvez não escolheria, mesmo que o preço seja mais baixo, uma máquina com uma avaliação muito baixa. Então, são essas relações que eu gostaria de avaliar, para depois a gente ver se o clustering consegue capturar essa essência e nos trazer recomendações que sejam fazendo sentido. Esse é o objetivo dessa análise exploratória que eu estou fazendo nessas variáveis. Então, vou fazer aqui a distribuição da variável Rating. Mesma coisa aqui, só copiar. Então, aqui a gente copia a variável Rating. Só vou trocar um pouco a cor aqui, vou colocar um red, vermelho. E ele mostra aqui uma curva que parece ser uma normal, está muito mais próxima de ser uma normal do que essa. Próximo dos valores entre 60 e 70, está mais aqui, o topo dessa curva. E ela tem uma simetria também, só que é uma simetria mais leve, e é o que a gente chama de uma simetria negativa, ou seja, a cauda dela está para o lado esquerdo, está para cá. Então, essa é uma outra característica. Em termos de Rating, a gente não tem... Tem alguns Outlines? Tem, tem algumas máquinas com Rating bem baixo, mas não há uma diferença tão grande, tão gritante quanto temos aqui. Aqui a cauda é bem mais longa, para valores mais extremos. Aqui você tem poucos valores extremos aqui do lado esquerdo, essa simetria leve e negativa. Então, pode ser uma coisa que não influencie tanto. E aí, para a gente fechar essa parte do EDA, falar um pouco de cruzar algumas informações. Preço com Brand, o preço muda muito com Brand, o preço muda muito, o Rating muda, e fazer uma correlação entre essas coisas. Então, vejo vocês no próximo vídeo

#### Aula 10 - Análise Bnivariada

Dando sequência ao EDA e para finalizar essa parte do EDA, antes da gente partir, propriamente dito, para a clusterização, então eu vou gerar mais três plotes aqui para analisar a correlação entre alguns fatores, uma variável categórica e uma variável numérica. Primeira delas que eu vou fazer é entre a brand e o preço, ou seja, existem grandes diferenças de preços quando a gente está tratando de máquinas diferentes, e a gente consegue avaliar isso através de um boxplot, então a gente consegue ver aqui um plot de distribuição, que na verdade é um boxplot, por brand e preço, seguindo o nome da variável. Então para isso a gente pode usar o plotly, usar o px, box, que é o boxplot, a gente coloca qual que é o dataset, o dataframe, qual a variável que vai no eixo x, eu vou escolher que é o preço, e vocês vão entender por que eu estou colocando o preço na parte de baixo e o brand na parte de cima, o ideal seria colocar o contrário, como nós temos muitas brands, o ideal é que esse boxplot não fique na vertical, ele fique na horizontal, vai ficar mais legível, então eu estou colocando aqui que a cor é dada pela brand, e aí eu tenho um parâmetro aqui desse gráfico que é o orientation, eu vou usar um orientation igual a h, horizontal, quando eu faço isso, ao invés de ele mostrar a brand e as marcas aqui e ele mostraria muitas marcas e ficaria difícil de enxergar, eu coloquei ao contrário. Então a gente percebe o seguinte com esse gráfico, que sim, há grandes variações de preços, de medianas inclusive, dependendo da marca, então você olha assim, HP, Lenovo, HP e Lenovo tem a Tecno que está aqui em cima, você tem a HP, você tem a Acer e a Lenovo, não está aparecendo a Acer aqui no eixo, elas estão bem próximas, mas quando você vai para uma outra marca, como a Apple, ela já vem para um outro patamar de valor, que está aqui, aí você pega aqui a outra marca que é a Asus, está mais próxima aqui, você tem a Dell, que está um pouquinho também próxima, aí você tem uma outra marca aqui lá embaixo que é a iBall, você tem a LG, então você tem sim uma variação, vamos dizer, bastante grande de valores, de preços, dependendo das marcas, e até inclusive outliers, uma quantidade razoável de outliers em cada uma dessas marcas, então isso também pode ser um fator de que na hora que você vai fazer uma recomendação, se você puxar muito para a marca só, você pode encontrar uma máquina que ela seja de uma categoria de preço ou totalmente diferente, só ver essa máquina aqui, as máquinas da Apple, que estão com essa cor aqui laranja, veja que a amplitude que tem de valor de máquina, você tem máquinas de 4.700 dólares até máquinas de 800 dólares, é um range muito grande de máquinas, quando você vai para a máquina da Lenovo, você já encontra máquinas de 260 a 4.000, que ele já considerou como outlier, então isso também é uma coisa para a gente analisar na hora que a gente for fazer o sistema de recomendação, ver como é que ele se comportou com essas discrepâncias, com esses outliers. E eu vou fazer a mesma coisa agora olhando o rating, apesar que o rating a gente percebeu pela distribuição dele, da variável de forma isolada, que o rating está meio próximo de uma normal, eu quero ver se existe alguma predileção, olha essa marca, ou há uma diferença muito grande de rating entre as marcas, ou se é uma coisa mais equilibrada, então vou colocar aqui rating, e ele vai fazer da mesma forma, então a gente percebe aqui uma outra configuração aqui, ao passo que a gente tinha um monte de outlier positivo e negativo, quer dizer positivo, a gente tem uma concentração dentro desse ponto aqui com uma série de marcas, você tem uma marca aqui, essa Wings aqui, acho que é a Wings ou a Vita, que está um pouco fora, mas veja que quase todas as marcas estão meio concentradas em uma região aqui entre 60 e 70, que foi exatamente aquilo que a gente viu no gráfico de distribuição dessa variável de forma isolada, então não parece que tenha essa variação. E aí um último gráfico para a gente fechar esse EDA, é fazer justamente um cruzamento entre rating e price, e aí colocando algumas cores diferentes aí para cada uma das brands, para ver se existe algum padrão que a gente reconhece, olha quanto maior o preço, maior o rating, será que acontece isso, se maior o preço, provavelmente a máquina tem uma qualidade ou uma configuração muito avançada, mas isso é sinal de um rating muito bom, a gente pode avaliar isso em um gráfico como esse, então vou fazer um gráfico aqui, aqui a gente vai usar um scatterplot de pricing e rating, também podemos usar o plotly, usar o scatter, df-laptops, aqui no caso o eixo x vai ser price, o eixo y vai ser o rating, e a gente consegue colocar a cor como sendo a brand, você consegue ver o efeito também de uma brand ao longo dessa relação entre preço e rating, preço e avaliação. A gente olha o gráfico, e a gente percebe o seguinte, não há para dizer aqui que há uma forte correlação, a gente vê uma curva de que parece, olhando assim, olha quanto mais o preço, maior o rating, mas a gente encontra máquinas, por exemplo, a máquina da Apple aqui, que tem um rating de 65, um rating mediano, como você tem máquinas na faixa dos 3 mil dólares, com um rating de 89, então talvez isolando esses pontos, pensando que esses pontos talvez sejam outliers dentro desse contexto, então você tem máquinas que são caras, que estão em um rating mediano, as da Apple principalmente, 63, 64, 58, tudo máquina da Apple aqui, máquinas de 2 mil dólares, máquinas que não são baratas, e aí você encontra as máquinas mais em conta, máquinas de 1.000, 1.500 dólares, com ratings altíssimos, uma máquina da Lenovo de 1.585, com rating altíssimo, dentro dessa base. Então parece ter uma correlação, mas não dá para dizer, a gente poderia até fazer um teste, aplicar aqui uma correlação de Pearson para avaliar essas duas variáveis, mas como o nosso objetivo não é a gente inferir isso, porque a gente vai fazer um modelo regressivo, de adivinhar preço, o nosso objetivo é mais organizar os laptops em grupos, para que a gente possa fazer uma recomendação, então aqui a gente consegue ter essa visão com esses gráficos. Então a gente finaliza aqui a nossa parte do DDA, vamos iniciar a parte agora de treinamento do modelo, a gente avaliar os resultados, vejo vocês no próximo vídeo.

### Bloco 5 - Treinamento do Modelo

#### Aula 11 - Transformar Dados para o Modelo

Vamos começar agora o treinamento do nosso algoritmo de clusterização hierárquica. Então, vou abrir uma célula aqui e colocar Markdown. Treinar modelos hierárquicos. De clustering hierárquico. A primeira coisa que a gente precisa fazer, de forma bem análoga que a gente fez no módulo anterior de Key Means, é a gente preparar as variáveis do nosso dataset para que elas possam passar pelo modelo. Então, a gente vai ter que transformar variáveis categóricas em variáveis numéricas através de um hot encoding. E também a gente vai fazer um standard scaler para colocar todas as variáveis numa mesma escala, já que eu tenho preço que está numa escala, rate que está numa escala, número de cores que está numa outra. Então, para a gente standardizar isso, e o modelo não capturar uma variável mais forte do que a outra, na ordem de grandeza, a gente vai fazer isso. Então, primeira coisa. Vamos selecionar as colunas para clusterização. Então, eu vou fazer primeiro uma cópia, vou falar que x aqui é uma cópia do meu DFLaptops. Então, vou fazer uma cópia. E aí, um ponto importante. Dentro das variáveis que a gente avaliou, as variáveis que a gente viu na nossa estrutura do nosso DataFrame. Então, voltando aqui, é mais fácil aqui pelos metadados. Então, dentro dessas variáveis aqui, tem muitas variáveis que tem a característica de serem variáveis que representam, de fato, categorias. Então, tipo de CPU, integrado ou dedicado. Então, por acionar Windows, Mac, tipo de processador, Core i3. Então, você tem um conjunto aqui, que é um conjunto, vamos dizer, mais restrito de possibilidades ou de valores possíveis dentro dessas variáveis. Mas quando você olha uma variável, por exemplo, modelo, Model Description of the Laptop, ou seja, Código de Descrição do Modelo. A gente viu que essa variável tem lá a descrição completa, que tem lá, às vezes, o código do modelo. Então, essa é uma variável que, na verdade, ela é uma variável nominal, uma categórica nominal. E se a gente for usar ela para classear, na verdade, olhando essa variável, se a gente fosse encodar essa variável, a gente encontraria, na verdade, um problema. Por que? Provavelmente, eu não vou ter modelos repetidos. Então, se eu for transformar essa variável no One Hot Encoding, o que vai acontecer é que eu vou ter 991 variáveis, porque eu não vou ter modelos repetidos, com 0 e 1. Então, eu vou trabalhar com um modelo bastante grande. E vai acontecer o que a gente falou, lá também no módulo de Fundamento de Machine Learning, o que a gente chama de Maldição da Dimensionalidade. Por que? Se a gente tem 991 modelos diferentes de computador. Por que? Porque mesmo que o modelo seja o mesmo, na descrição do modelo, eu tenho características do modelo que os diferem. Então, eu posso ter um modelo que é um MacBook Air 2020, mas se ele tiver 8 GB, na descrição vai estar escrito que tem 8 GB, o outro que tem 16 GB, vai na descrição, vai estar 16 GB. Essa informação eu já tenho em outro lugar. Eu já tenho ela em uma coluna específica. Então, se eu fosse pegar, se a gente for fazer uma conta rápida aqui, eu tenho 991 modelos diferentes. Eu tenho aqui, talvez 10 modelos diferentes de processos artificial, talvez seja até mais do que isso. Primary Storage Type, talvez sejam 2 tipos, 3 no máximo. Tipo de GPU, integrado ou dedicado, 2 tipos. Sistemas operacionais, 2 tipos. Então, essas variáveis, elas vão acabar gerando, cada uma delas vai gerar colunas diferentes. Então, eu tenho aqui 2 colunas, vamos por 4 colunas, 6 colunas, 10 colunas, são 16. 16 colunas, mais as 991 que ele vai gerar como hot encode daqui. Mais as brands, que a gente viu que deve ter umas 20 brands diferentes. O que vai acontecer aqui no final é, a gente vai ter mais coluna do que linha. Porque se a gente já tem uma coluna para cada modelo, eu já tenho 990 colunas para representar esse dado. Mais as colunas que tem, e mais os outros one hot encode que a gente vai ter, a gente vai ter que chegar na situação, e é uma situação que a gente tem que evitar ao máximo. Quando a gente tem, em um dataset, uma quantidade maior de colunas do que de linhas. Porque vai ficar difícil do nosso modelo convergir, do nosso modelo entender algum padrão e poder clusterizar. Até porque a variável model description, ela funciona quase como um índice, como um valor único. Eu não tenho uma repetição. Assim como o índice. O índice é apenas um identificador da linha, que também não tem nenhuma informação ali, alguma coisa que faça parte, ou que eu consiga clusterizar. Então, resumindo, tanto a variável index como a variável model são variáveis que a gente não vai usar dentro do nosso modelo. A gente vai usá-las como parte das características da clusterização, porque elas tem uma alta cardinalidade. Essa aqui tem 991 tipos diferentes, essa aqui também 991 tipos. Então não faz sentido a gente ter essas variáveis dentro do nosso modelo de clustering. Voltando no código, então o que a gente vai fazer? A gente vai criar colunas desnecessárias. Para isso, para esse propósito. Desnecessárias. Então a gente vai fazer um x.drop. Já vou usar o x direto para manter o meu dataframe íntegro, e vocês vão entender porque isso é importante. A gente criar um dataframe novo, trabalhar com ele para o modelo e deixar o nosso dataframe de EDA bonitinho. Então estou removendo as duas colunas. Axis 1, o eixo da coluna, in place e goto. Ele já vai fazer a remoção e automaticamente jogar dentro do x. Posso rodar isso aqui. Então já tenho aqui o x. Se eu ver aqui, só para mostrar aqui o x. Ele mostra aqui 991 linhas, 18 colunas. Então já removeu a coluna de index, já removeu a coluna de branch. Aí o que a gente pode fazer agora? A gente pode fazer o trabalho do pré-processor, da gente conseguir transformar as colunas numéricas com standard scaler, aplicar uma transformação baseada na escala normal, na distribuição normal e fazer uma hot encode nas variáveis, que são as variáveis categóricas. Então vamos lá. Primeiro vou criar algumas variáveis para facilitar esse trabalho do pré-processor. Então separando variáveis numéricas e categóricas. Então vou criar aqui uma variável chamada numeric features e vou relacionar todas as features numéricas que eu tenho. Então eu tenho aqui price, eu tenho rating, eu tenho numcores, eu tenho número de threads, eu tenho memória RAM, eu tenho primary storage capacity, aí eu tenho o display size, eu tenho resolution width e tenho resolution height, altura. Vou fazer a mesma coisa para as variáveis categóricas, categorical features, que são bem menos. Eu tenho aqui a branch, eu tenho o processor branch, eu tenho o GPU branch, eu tenho o GPU pipe, e tenho o sistema operacional. Definindo essas variáveis, vou separar aqui as variáveis, eu posso aplicar as transformações por tipo. Então aqui eu vou aplicar as transformações. Então eu faço numeric, vou criar um numeric transformer, que é um standard scaler, e vou ter um categorical transformer, que vai ser um hot encoder. E agora eu crio um pré-processador para aplicar propriamente ditas transformações. Então aqui é, eu vou colocar aqui definir transformações, e aqui aplicar, vou até separar, criar pré-processador de transformações. Então vou usar um column transformer, onde eu vou definir quais são esses transformers. Então eu tenho transformers, transformers, transformers, opa. Transformers, igual a, eu vou definir um primeiro transformer, que vai ser chamado de num, de numérico, onde eu vou usar o numeric transformer, e aplicar a uma lista de features, que está em numeric features. E vou aplicar um segundo transformer, que eu vou chamar de cat, de categorical, onde eu vou aplicar, onde eu vou usar o categorical feature transformer, categorical transformer, e vou aplicar ele no categorical features. Fecho aqui, e fecho aqui, deixa eu ver se tem alguma coisa aqui faltando, transformers, colo um transformer, ele está reclamando de alguma coisa, isso aqui, transformers, vamos ver aqui, vamos ver o que ele vai reclamar, faltou algum parênteses, ah, faltou abrir esse parênteses, pronto. Pronto, criei o pré-processador, agora o que eu vou fazer, eu vou transformar os dados, então eu vou criar um novo X, com base no X, nesse pré-processo, então eu vou chamar esse X, X transformers, e eu vou aplicar um pré-processor, que na verdade ele faz o que, ele faz um fit, ele treina o modelo, e já transforma, com base no X, então ele, deixa eu trocar essa variável para Python, célula, então ele faz o que, ele aprende os valores das variáveis, para cada uma desses transformers, já transforma esse X, e grava esse X, numa variável chamada X transformers, então, vou rodar isso, e aí se a gente quiser ver o X transformers, X transformers, aí tem uma coisa interessante, ou seja, como, mesmo com, tirando o brand, ou o modelo, ele ficou, o que a gente chama de uma matriz esparsa, porque ele tem aqui, 54 variáveis, dentro dele, então, quando você tem isso, deixa eu ver se eu coloco um shape aqui, shape de 0, deixa eu ver se é um shape, não é um shape, é uma tupla, shape de 0, 0 é a quantidade de linhas, então ele continua com as 991 linhas, só que agora, ele tem 54 colunas, e tem muita coluna vazia, com 0, por causa da brand, ele tem muitas brands, então, ao invés de ele criar um objeto, pura e simplesmente, um arreino NumPy, que é o que ele faria normalmente, ele criou o que a gente chama de uma matriz esparsa, do tipo NumPy float 64, transformou tudo num float, então, isso tem algumas questões, na hora que a gente for fazer o treinamento do modelo, que a gente tem que observar, e fazer uma transformação de volta para um array, para que o modelo consiga ler essas informações, e poder fazer o processo, é um detalhe que eu quis trazer aqui, para, nossa, eu tentei pegar e não deu, eu fui pegar justamente o shape transformer de como estava, tentei colocar ele no modelo e não consegui, por que? porque ele não está agora como um array, puro do NumPy, ele está como uma matriz esparsa, ele precisa ser transformado para isso, então, fechamos aqui a primeira parte, de transformação das variáveis, para que a gente possa começar a trabalhar nos nossos modelos, vejo vocês no próximo vídeo.

#### Aula 12 - Criar Função Otimização Algoritmo Aglomerativo

Agora a gente vai começar a trabalhar nos modelos. E para a gente trabalhar nesses modelos a gente vai fazer o uso, como a gente também fez no módulo anterior, do Optuna para que a gente possa escolher os melhores hiperparâmetros. Bom, eu comentei que a priori a visão clássica que a gente tem de clustering hierárquico aglomerativo, ou clustering hierárquico como todo, é que você não precisa definir a priori a quantidade de clusters. Só que para facilitar a vida do cientista de dados, ele não tem que gerar um dendograma e ele ficar fazendo cortes no dendograma e depois coletando a métrica baseada na quantidade de clusters e na pureza desses clusters, que é de certa forma o que o Silhouette faz. Ele olha os pontos e dados de cada cluster estão juntos e o quanto cada cluster está separado um do outro. Isso é o que o Silhouette analisa. As ferramentas dão possibilidade, mesmo que você não precise inferir isso, você não precisa importar essa informação, de você colocar a quantidade de clusters que você quer dividir. Uma outra opção no caso do cluster aglomerativo é você usar a medida de distância e falar que quer fazer o corte por essas medidas de distância e aí ele vai derivar a quantidade de clusters baseado nesses muitos cortes que você está definindo dentro do seu modelo. Tendo isso posto, a gente vai usar o número de clusters, mas com o objetivo de a gente poder ter uma resposta mais rápida e não ter que ficar gerando uma série de dendogramas. Mas de qualquer forma, eu vou fazer um exercício com vocês depois, ainda dentro do projeto prático e se eu fizesse um corte do dendograma, o que aconteceria? Quantos clusters iria gerar? A gente vai simular aqui dentro do projeto prático algumas coisas para dar esse exemplo e falar que toda aquela história de cortar o dendograma tem como fazer isso. A gente vai usar essa abordagem um pouco mais para frente aqui no módulo. Então, dando só essa explicação de como a gente vai trabalhar. Tendo isso posto, vamos começar a pôr a mão na massa e desenvolver o nosso modelo. E como que a gente vai fazer esse modelo? A gente vai usar, como eu falei, o Optuna. A gente vai criar uma função e nós vamos fazer experimentos com essa função até que ele chegue no melhor objetivo possível. No nosso caso, a gente vai usar o mesmo conceito do módulo de K-Means, que é maximizar o Silhouette Score. Eu quero que o Silhouette Score, cujo valor máximo é 1, fique o mais próximo de 1. Então, vamos criar uma função chamada Hierarchical Clustering Agglomerative. A gente vai criar o agglomerative e depois a gente vai fazer um teste com o divisivo e vamos comparar os dois para ver qual a gente vai seguir no nosso modelo de recomendação. Então, a gente cria sempre o padrão de uma função que a gente vai criar para o Optuna e a gente passar um parâmetro para ela, que é o Trial, que é a tentativa que ele vai fazer. Aí a gente vai definir os hiperparâmetros a serem ajustados. E nós vamos ajustar aqui dois hiperparâmetros. O primeiro hiperparâmetro é a quantidade de clusters. Então, a gente vai usar aqui N clusters. Vamos colocar aqui um Trial para ele sugerir valores inteiros. E aí tem um outro aspecto importante, sobre a escolha da quantidade de clusters. Nós estamos falando de quase mil máquinas, quase mil modelos diferentes. Vamos imaginar que a gente pense que existem, vamos falar assim, olha, eu tenho um conhecimento de que não existem mais do que 10 linhas ou 10 tipos de máquinas diferentes. Então, eu tenho a máquina que a gente chama de Budget, que é uma máquina super barata. Eu tenho uma máquina que seria para uso geral, uma máquina que você usa só para fazer acesso à internet, que ela não é a mais barata. Aí você tem uma máquina para um uso um pouco mais avançado, você tem a máquina do desenvolvedor, você tem a máquina da pessoa que joga, você tem aquela máquina da pessoa que precisa de alta performance para algum outro tipo de tarefa, por exemplo, um AutoCAD, um CAD e alguma outra ferramenta. Vamos imaginar que a gente tivesse 10 tipos de clusters, 10 clusters diferentes. Se a gente pegasse essas, arredondando mil máquinas e vamos supor que ele fizesse uma distribuição uniforme, ele pegasse os mil máquinas e colocasse dentro desses 10 clusters. O que aconteceria? A gente teria 100 máquinas por cluster. E aí pensando no nosso objetivo final, que é de montar um sistema de recomendação, imagine o usuário pegando uma máquina que ele escolheu e ele falando, tá bom, deixa eu ver quais são as recomendações. E ele olhando que tem mais, sei lá, 99 máquinas que ele poderia olhar. Então, se de um lado você quis tirar, deixar o usuário menos chateado porque a máquina que ele queria não tem, por outro lado você vai deixar ele tão chateado quanto quando você falar para ele que ele tem mais 99 opções para olhar. Pensando nisso, a gente vai colocar valores de quantidades de clusters altas. Porque nós estamos falando de novo, quase mil máquinas. Então, qual o ideal, pensando se eu fosse um usuário? Porque eu gostaria de ver, sei lá, 15 máquinas, 10 máquinas. Isso para mim é aceitável? Aceitável. Talvez menos, talvez seria melhor umas 4 máquinas. Me diga aqui quais são as máquinas melhores que estão no mesmo cluster. Aí eu poderia trabalhar com clusters, sei lá, de 200 clusters. Se ele fizesse, não é garantido que ele vai fazer isso, ele dividiria, sei lá, em 5 máquinas por cluster. Então, o que eu vou fazer aqui? Tudo isso eu estou contando para falar que eu estou trabalhando com quantidades de clusters altas. O que significa isso? Vai dar para identificar, como a gente fez em alguns outros exemplos, que tem um cluster que é o cluster gold, silver, bronze e starter. Ou o cluster, como eu comentei aqui, o budget, o gamer, performance, developer e office. Não, porque se a quantidade de clusters que o modelo gerar for muito alta, talvez você não consiga ter uma identificação muito clara. Qual é o cluster 120? Qual é o cluster 130? Mas o nosso objetivo aqui é a gente ter uma quantidade razoável de máquinas para dar o poder de escolha para o usuário. Sempre bom lembrar disso. Às vezes a gente faz o algoritmo pensando, qual é a métrica que eu vou bater? Ah, eu bati a métrica, o meu silhouette score está super alto. Mas a quantidade de clusters, ele gerou 15 clusters e tem cluster que está com, sei lá, 200 máquinas. O usuário não vai pegar uma lista de 200 máquinas e falar, ah, deixa eu ver dessas 200, qual que é a que está maior, qual que está melhor. Porque vai ficar difícil para ele. E de novo, aqui a gente está improvisando, fazendo um uso diferente de uma clusterização para um sistema de recomendação. Então a gente vai trabalhar com números aqui, pelo menos de 10 clusters até 150 clusters. Então vamos pensar que pode ser que tenha 9 máquinas, 10 máquinas por, de novo, uniforme para cada uma dos clusters. Não vai ser isso, vai ter clusters com mais, vai ter clusters com menos, vai ter clusters que vão ter uma máquina só. Mas é só para manter, pensando na média de máquinas que eu gostaria de ter para avaliar a gente trabalhar com esse número de 150 clusters. E o que a gente vai fazer agora? A gente vai definir uma outra variável, que é a forma, qual critério que ele usa para fazer a aglomeração. E o algoritmo propriamente dito, não é nem o Scikit-learn, ele fornece algumas formas para você, para ele analisar a métrica e saber por qual métrica ele vai fazer. A gente também vai usar essas diversas formas como um parâmetro que a gente quer avaliar qual deles sai melhor. Qual é esse parâmetro? Esse parâmetro se chama linkage. Ou seja, de que forma, qual é o critério que eu vou usar para avaliar as observações, os conjuntos de observações para que eu possa fazer a aglomeração. Então eu tenho um primeiro tipo, que a gente chama de Ward, que é o default do algoritmo, que na verdade ele minimiza a variância dos clusters que estão sendo numergiados. A variância, quando a gente fala, é a variância da distância que é o que ele está fazendo análise lá na matriz. Ele tem uma outra que chama-se average. O average, ele usa a média das distâncias de cada observação dos dois conjuntos de dados que ele está tentando unificar. Você tem um terceiro que se chama complete, que nas versões anteriores se chamava máximo. Na verdade ele usa a distância máxima entre as observações dos dois conjuntos de dados. E você tem um último que se chama single, que ele usa a distância mínima das distâncias. Então, para deixar aqui documentado, Ward usa a variância, o average usa a média, o complete usa a máxima, e o single usa a mínima. Quando eu falo desse máximo, a mínima é a distância. Então, até eu colocar aqui, a mínima é que é o linkage. Linkage é critério de distância entre dois conjuntos, que são os dois conjuntos para formar o cluster. Já que eu estou saindo de clusters que são únicos, estou saindo de baixo para cima, estou fazendo uma abordagem que a gente chama de bottom-top. É o bottom-up, melhor dizendo. Ou seja, cada cluster tem o seu próprio cluster, cada item de dados, cada ponto de dados tem o seu cluster, ele começa assim, cada objeto de dados, e eles vão se fundindo, se aglomerando, até formar o cluster principal, a raiz. Então, esses aqui são os critérios que a gente pode usar. Então, fizemos aqui um linkage. Vamos instanciar aqui o modelo. Então, instanciar o modelo. Então, o modelo se chama, e eu vou chamar ele de hierarchical model, que está dentro de uma função, depois eu posso usar o mesmo nome. Ele se chama aglomerative clustering. Eu passo para ele o critério de linkage, e eu posso passar a quantidade de clusters. A quantidade de clusters. Aí, o que eu vou fazer? Eu vou fazer um fit, eu já vou fazer um fit predict, ou seja, eu vou treinar o modelo, e eu já vou gerar a predição desse modelo numa outra variável. Então, eu vou treinar o modelo, e já executar como se fosse a predição. Não é bem uma predição, é uma clusterização, Então, eu vou jogar isso aqui numa variável Y. Então, eu vou usar o hierarchical model, ponto fit predict, ou seja, faço o treinamento e já jogue o resultado da predição nessa variável Y, e aí que tem aquilo que eu comentei, X-transformer. Se eu usar o X-transformer sozinho, vai gerar uma variável Y. Então, eu vou executar, e aí que tem aquilo que eu comentei, X-transformer. Se eu usar o X-transformer sozinho, vai dar erro na hora do Optuna tentar rodar, ele vai falar, opa, isso aqui é uma matriz esparsa, não consigo lidar com ela. Então, o que a gente tem que fazer aqui? Colocar um comando de transformar essa matriz esparsa em um array. Então, a gente usa QArray. Aí, a gente faz o que? Calcular o silhouette score. Silhouette score. Que é dado pelo X-transformer e o Y. Então, ele usa essas duas informações para calcular com base nas distâncias. Silhouette score. Silhouette avg, eu vou chamar aqui de, na média dos silhouettes, é igual a silhouette score, e eu vou passar o X-transformer, o Y, que foi o meu input, e o Y, que foi o meu output. E vou retornar dessa função o meu silhouette, que é o que eu quero otimizar. No caso aqui, maximizar. Criei a função, e aí a gente vai, na sequência, fazer a criação do estudo para rodar. Então, vejo vocês no próximo vídeo.

#### Aula 13 - Execução Otimização Algoritmo Aglomerativo

Então, criar um estudo noOptuna. Primeira coisa, a gente vai fazer aqui, da mesma forma que a gente também já fez alguns módulos, a gente não vai fazer uma busca randômica por parâmetros, a gente vai testar todos. Então, a gente vai testar, fazer toda essa cobertura de quantidade de clusters versus essa cobertura de formas, de critérios de linkage, de distância que a gente chama de linkage. Ele vai gerar uma quantidade razoável de parâmetros, de tentativas, de trials, para a pessoa obter aquele que for o melhor. Então, o que a gente vai fazer aqui? Primeiro, qual o espaço de busca que a gente vai ter para cada uma das variáveis que eu quero? Então, eu vou ter uma variável chamada nClusters, e ela vai ter um campo de busca de 10 a 151. Por quê? Porque eu tenho que sempre considerar um a mais, porque na hora do range ele tira esse 151, para que eu siga aqui de 10 a 50. Então, por isso que aqui fica um 151. E aí, no linkage, que é uma outra variável que eu quero trabalhar, eu vou colocar os valores possíveis dessa variável, que são categóricos. Então, eu posso copiar isso aqui daqui e jogar aqui. Criar um dicionário. O que eu vou fazer agora? Criar um sampler dentro do Optuna. Vou colocar sampler ag, de aglomerativo. Então, eu vou chamar Optuna Samplers, e eu vou chamar o Grid Sampler, que é o que vai fazer a busca em todas as opções possíveis dentro desse campo de busca. Então, vou colocar aqui um search space, igual a search space. E até vou colocar aqui um ag, para separar também o que é aglomerativo daquilo que é divisível. Agora, eu vou criar o estudo propriamente dito. Então, estudo. Eu também vou chamar de estudo ag, de aglomerativo, e vou chamar optuna.createStudy. Vou passar qual a direção que eu quero, que no caso aqui é o maximize, eu quero maximizar o valor do Silhouette Score, e qual o sampler que eu vou usar. O sampler vai ser sampler ag. Feito isso, criou o estudo em memória, a gente pode agora executar o estudo. Então, executar estudo do Optuna para o cluster aglomerativo. Então, eu venho aqui e coloco o estudo ag, .optimize, e aí eu coloco qual é o objetivo, qual a função, no caso aqui, essa aqui é a minha função que eu vou chamar, e a quantidade de trials. Quantidade de trials eu vou pôr mil, mas na verdade vai ser as 564 trials. Então, se eu colocar aqui 600, ele já vai cobrir tudo. E vou rodar. Opa, nTrials. Pronto. E começou a rodar. Então, ele está rodando aqui mostrando quantidades de clusters diferentes, com tipos de linkage, e vai rodar aqui. Vai rodar relativamente rápido, acho que são 564 trials, e a gente vai ver o resultado que ele vai obter. A gente vai fazer depois a mesma coisa, a gente vai avaliar qual foi o melhor, quantos clusters ele achou, qual foi o linkage que ele usou, qual o Silhouette Score que ele chegou, e a gente vai fazer a mesma coisa para o divisive. Terminou? A gente já pode até olhar aqui. Então, o que ele falou aqui foi o seguinte, o Best World Trial 380, com o valor 0,32,745. Veja que até quase no final, o Best Trial estava em 28, ou seja, o Silhouette Score não está uma maravilha, o Silhouette Score é baixo. Mas, acho que o que é importante, de novo, a gente não quer que o Silhouette Score seja baixo, mas como que está a percepção do usuário pelas recomendações que a gente está fazendo. Isso que a gente vai ver na hora que a gente for colocar a aplicação para funcionar e fazer um piloto, um teste, escolher essa máquina, que máquina que ele me recomendou. Está fazendo sentido as máquinas que ele está recomendando, sim ou não? Então, a gente pode escolher uma outra métrica, vamos escolher um índice de Davis Moulding, podemos escolher outros outros parâmetros, mas é importante ver também a visão do usuário, se você está construindo a aplicação para o usuário, é ver a percepção dele. Ele chegou aqui em 32 no Trial 380, nem está mostrando todos aqui, na verdade mostrou o Trial 380, é que ele pulou aqui os outros parâmetros, para não poluir a tela. E a gente vai fazer agora o que? A gente vai mostrar qual foi esse Trial. Então, vou colocar aqui embaixo, uma célula, Code, então vamos lá. Mostrar o melhor, configuração doptuna, e aqui para o aglomerative. E aí como a gente vai fazer? Vou gravar isso em uma variável, então vou chamar aqui de best, os melhores parâmetros do ag, do aglomerative, que é igual ao estudo ag.bestparams. Se eu vier aqui e fizer isso, e já colocar essa variável, já consegue ver, mas eu vou deixar ela bonitinha para a gente ver isso legal. Ele mostrou o seguinte, que a quantidade de clusters foi 149 clusters, ou seja, eu coloquei 150 e disse que a melhor configuração foi 149, e que o linkage melhor que trabalhou foi o linkage do tipo Word. Então a gente trouxe isso aqui, e aí que é importante saber qual foi o melhor Silhouette Score, 32745. Então, se a gente quisesse calcular aqui, a gente poderia calcular esse Silhouette Score, rodando o modelo especificamente com os parâmetros que ele pediu, que ele encontrou, para a gente obter esse mesmo valor, para a gente mostrar qual foi esse mesmo valor. Então aqui, a gente pode instanciar esse modelo, mas a gente mostrar, pelo menos de uma forma mais amigável, essa informação. Então a gente pode colocar aqui o print, que aqui é a quantidade de clusters, ou igual ao best, parâmetros a G, e eu posso pegar aqui nClusters. E eu posso fazer a mesma coisa, só para deixar mais bonitinho, que é a quantidade de, qual foi o linkage e eu venho aqui e ponho o linkage. Então, 149 e o linkage que ele usou foi o Word. Mas a gente vai ter que ter um modelo que seja melhor que aquele modelo que a gente vai salvar. Então a gente salvar os dois modelos agora, a gente vai ver qual que se saiu melhor, se ele se saiu melhor foi o modelo aglomerativo ou divisivo, e aí aquele que for o melhor, a gente salva ele, gera um modelo para ele, faz um treinamento com o modelo, lembrando que aqui eu não tenho um modelo, ele fez uma série de interações, mas eu não tenho um modelo resultante disso que eu possa salvar e usar para alguma outra coisa. Então eu preciso criar esse modelo. Mas para não criar aqui, porque eu não sei se o aglomerativo vai ser melhor que o divisivo, a gente espera avalia os resultados e depois a gente define qual deles a gente vai instanciar para salvar e usar posteriormente. Então concluímos aqui mais uma parte, agora fizemos o treinamento e otimizamos o modelo do tipo aglomerativo, vamos para o tipo divisivo, vejo vocês no próximo vídeo.

#### Aula 14 - Criar Função e Execução Otimização Algoritmo Divisivo

E dando sequência a parte de treinamento dos nossos modelos, agora a gente vai fazer a mesma coisa que a gente fez no aglomerativo para o divisivo. Então também para a gente aproveitar o código, a gente consegue aqui pegar esse código e adaptar ele no modelo que a gente vai criar. Então vou copiar essa célula para cá e a gente vai chamar de Hierarchical Divisivo, Divisivo, Objective, Objective, na verdade faltou um C aqui tá? Lá também faltou, então já vou arrumar aqui, Objective, e aqui a gente tem que mudar também para Objective, para deixar certinho. Então aqui no caso do divisivo, a gente vai trabalhar somente com o parâmetro, que é o parâmetro de quantidade de clusters. A gente não tem o parâmetro de linkage dentro desse tipo de modelo. Então a gente também vai trabalhar com a mesma quantidade de clusters, de 10 a 150. Então a gente não tem linkage. E o modelo que a gente vai usar aqui é um modelo chamado Bisecting K-Means. Eu não tinha trabalhado com esse modelo ainda, os trabalhos que eu fiz com clusters divisivos eu não fiz nem com Python, foi com R. O R tem algumas coisas um pouco mais rebuscadas em termos de clustering, mas aí eu descobri na documentação que se você quiser fazer um cluster, trabalhar como se fosse um cluster hierárquico divisivo, existe um método que está dentro do pacote, na verdade do K-Means, no pacote Clustering do Scikit-Learn, mas que está lá como K-Means que na verdade faz exatamente um cluster hierárquico divisivo. Então só explicando aqui porque tem esse nome, não é divisive clustering, ou hierárquico clustering, porque não existe de fato um método para isso. Então a gente chama esse outro método, que eu consigo passar aqui a quantidade de clusters, eu tenho outros parâmetros, mas a gente vai fazer usar mais esse, você pode usar qual modelo você quer usar, se é o modelo K-Means ou se é o modelo K-Means++, que ele tem algumas vantagens. Vamos treinar o modelo, fazer um FitPredict, também usando o conceito do ToArray, de converter, e vamos obter o Silhouette Score. Então até aqui nada muito diferente do que a gente fez anteriormente. E aí a gente também vai criar o estudo do Optuna para ele. Então também vamos copiar aqui o estudo do Optuna. Só que aqui eu não preciso ter o linkage, só tenho o range, eu vou chamar aqui de search space DI, divisivo, aqui o sampler DI, aqui o sampler DI, e aqui o estudo DI. Também tudo próximo daquilo que eu já fiz no aglomerativo. Criar um estudo em memória. Mesmo sampler, só que apontando para esse espaço de busca e cria um novo estudo divisivo usando esse sampler. E agora a gente também pode otimizar, rodar a otimização de hiperparâmetros. Vou copiar aqui também. Então aqui não são 600 só, são menos, porque na verdade eu só tenho uma variável para iterar, então aqui são 140, 10 até 150, 141. Então eu coloco aqui o estudo, e a minha função aqui é HierarchicalDivisiveObjective. E o meu estudo é o estudo DI. E vou mandar rodar. Então tá rodando aqui, não tá mudando muito ainda, o melhor aqui é 21, tá na faixa de 21 e já terminou. Então voltando aqui, vamos fazer a mesma coisa que a gente fez no anterior também. Vamos olhar os melhores parâmetros do aglomerativo e ver o que que ele chegou. Então aqui, divisive. Então se a gente pegar o BatchParams DI, e aqui estudo DI, eu só tenho aqui a quantidade de clusters, não tem o linkage. E também chegou em 149 clusters. Ah não, peraí, vou colocar aqui DI. E chegou em 124 clusters, mas o importante, o número aqui. Então a gente teve lá, voltando aqui, 22, o meu SilhouetteScore. O meu SilhouetteScore máximo no divisível foi de 21. Então temos um vencedor aqui. O modelo, a abordagem que funcionou melhor para o nosso caso de uso, foi a abordagem aglomerativa. E aí o que a gente precisa fazer agora? A gente vai criar um modelo com aglomerativo, que foi a melhor solução que foi dada pela Optuna. Vamos treinar esse modelo com os dados, treinar os modelos com algoritmo, porque depois a gente pode salvar esse modelo, a gente pode salvar, ter o SilhouetteScore. E aí uma coisa importante, a gente vai pegar o resultado desses clusters e nós vamos jogar esses dados para dentro do DataFrame. Para a gente poder ter o DataFrame agora, com o cluster que foi atribuído pelo nosso modelo. Até para a gente poder visualizar o que ele chegou de conclusão do nosso modelo. Então vejo vocês no próximo vídeo.

#### Aula 15 - Criar Melhor Modelo e Atribuir Clusters ao Dataset

Para concluir a parte agora do treinamento do modelo, antes de a gente visualizar os resultados, a gente vai agora treinar o modelo, de forma que a gente possa pegar esse modelo, salvar esse modelo e principalmente atribuir os valores obtidos por esse melhor modelo dentro do nosso DataFrame para que a gente tenha esse DataFrame e possa usar isso na segunda parte do nosso projeto prático que é justamente criar uma interface para o usuário poder visualizar que laptops, notebooks se parecem com aquele que ele está selecionando de uma lista. Então, como é que a gente vai fazer isso? A gente vai, vou colocar aqui, criar modelo com melhor configuração, que no caso aqui foi a aglomerativa. Então, a gente vai fazer o seguinte, a gente já tem quais são os melhores parâmetros, a gente salvou lá o bestparams.ag A gente vai fazer o seguinte, a gente vai criar um modelo, então vamos chamar esse modelo de best.ag ou até bestmodel, que é o melhor de todos, aglomerativeclustering e aí nós vamos passar os parâmetros. Então, parâmetro número 1, nclusters é igual ao que bestparams.ag nclusters. Parâmetro 2, linkage é igual a bestparams.ag e linkage. É só isso que ele precisa para gravar, para criar o modelo. Cria o modelo, pronto, agora posso fazer um fit desse modelo. Aí eu falo, como é que eu vou fazer esse fit, eu posso pegar esse fit e depois salvar? Eu posso fazer um fit transform e chegar no Y, mas o Y não traz só informação do label, ele traz outras informações de distância que são importantes. Então, o que a gente vai fazer aqui, a gente vai treinar o modelo e a gente vai obter os labels do modelo que são, na verdade, valores inteiros que representam quais são os clusters, de 0 até X. No caso aqui, a gente tem 149 clusters, que foi o que foi definido pelo melhor modelo. Então, a gente vai ter um número de 0 a 148. Então, cria o modelo, eu venho aqui agora, coloco treinar modelo, podemos treinar esse modelo. E aí, eu faço o que? bestmodel.fit x-transformed, mesma coisa, to array. Treinei o modelo, 149 clusters, com o parâmetro linkage aqui também citado. Aí, se eu quiser ver o silhouette score, como é que eu faço? Verificar silhouette score. E aí, eu vou falar o bestscore é igual o que? Silhouette score, vou passar o x-transformed. Ah, mas e o Y? Eu não fiz o fit transform. Aí, você consegue obter essa informação como bestmodel.labels. Underline, ele salva para você o label. Então, salvo isso aqui e posso até mostrar. Vou colocar aqui bestscore, já para visualizar. 0, 32, 745, foi aquele número que a gente viu lá em cima. Então, treinamos o modelo, criamos um modelo aglomerativo, treinamos ele com os parâmetros que foram aqueles que melhor se saíram na optuna. E checamos o silhouette score. O que está faltando agora é a gente poder atribuir. A gente vai atribuir esses labels que ele obteve, de 0 a 149, a 148, para cada um dos registros da nossa base. E daí, a importância de a gente ter o nosso dataset íntegro. Por quê? A gente vai criar uma coluna com o cluster escolhido no dataframe original. Por quê? Porque, na verdade, o meu X é o meu dataframe original, só que eu fiz uma cópia. Se eu tivesse feito um apontamento, o que eu mudaria em um, mudaria no outro. Aqui eu fiz uma cópia, então o meu DFLaptops está íntegro. E o meu X foi o conjunto de dados que eu usei para treinar. Eu removi colunas dele, basicamente removi duas colunas dele. Mas a ordem do dataset não mudou, eu não fiz nenhuma mudança. Então, na hora que eu jogo os clusters, ele vai encaixar exatamente com a mesma ordem que está o meu dataframe. Então, eu venho aqui e coloco o DFLaptops, que eu vou chamar de cluster, a variável, e vou colocar o quê nela? O bestmodel.labels, underline. Então, ele vai pegar esses clusters e colocar em cada uma das variáveis, cada um dos registros dentro do meu dataframe. Se a gente quiser checar lá em cima a estrutura do meu dataframe, está aqui embaixo o cluster de uma variável inteira. Se eu vier aqui e der um HEAD, está aqui o cluster. Cluster 73, 44, 28, 53, 14, 113, 4, 19, 95. Até agora não repetiu nenhum. Não repetiu nenhum. Até o momento aqui, nessa primeira página, não tem nenhum cluster, uma máquina sendo indicada parecida com outra máquina que está logo abaixo. Se eu visualizar os últimos registros, é a mesma coisa. 21, 86, 116, 37, 11, 85, 7, 64, 115, 36. Então, ele jogou o código de cluster lá e é por esse código de cluster que a gente vai fazer a próxima parte, na segunda parte do nosso projeto prático, que é criar uma tela de consulta para apresentar. Então, a gente vai finalizar essa parte. Vamos partir agora para visualizar os resultados que a gente obteve. Eu vou mostrar o dendograma. E o dendograma, como a gente simula, por exemplo, o corte de um dendograma. Eu vou mostrar isso. Vejo vocês no próximo vídeo.

### Bloco 6 - Análise dos Resultados

#### Aula 16 - Geração e Uso do Dendrograma

Dando sequência, a gente vai agora visualizar os resultados que a gente obteve depois de fazer a atribuição dos clusters dentro do nosso DataFrame original. Então vou voltar aqui embaixo, vou criar mais uma célula Markdown aqui e visualizar os resultados. Primeira coisa que eu gostaria de trazer aqui, é só ter uma visão do que o Optuna fez, ou seja, como é que ele fez os trials, ele fez muitos trials. Então vou mostrar aqui um chart, vou mostrar o chart dos trials do Optuna. Para gerar um chart do Optuna, a gente geralmente gerava o chart com mais de um objetivo, eu tenho que minimizar uma variável ou maximizar outra variável. Aqui a gente está falando de uma variável só, a gente tinha uma variável para maximizar que era o Silhouette Score. Então para a gente obter esse gráfico que mostra a evolução dos trials, a gente usa o pacote Visualization e a gente tem um plot chamado PlotOptimizationHistory. E o que a gente passa para ele, qual o estudo que a gente quer que ele analise. Então é o estudo AG que é o nosso interesse, que foi aquele que obteve o melhor resultado. A gente atribui isso a uma figura do MatchPlotLib e dá um show nessa figura. E a gente consegue ver aqui, ele partiu aqui na primeira por um valor baixo, foi subindo e a cada interação ele ficou um bom tempo com esse valor máximo de 32.487. E aí quando foi aqui no 380 ele bateu 32.745 e foi até o final. E aí estão aqui todos os valores que ele foi obtendo para cada um dos experimentos, dos trials que ele fez dentro do nosso modelo. Então essa é uma visão aqui. E aí vamos começar agora, depois de olhar isso, ao invés de olhar os resultados que ele obteve com base, olhar o dendograma. Como é que a gente gera esse dendograma? E eu vou falar para vocês, pasmem, mas não existe de forma nativa dentro do Scikit-Learn, uma função que gere um dendograma. Por mais que seja um artefato importante dentro do clustering, na verdade existe dentro do Scikit-Learn, você vai pesquisar dendograma, ele te dá uma função, ele te dá um código em Python que você copia e cola, que ele gera um dendograma com base no modelo que você treinou dentro dele. Eu até pensei, bom, vou pegar esse código e a gente traz esse código, mas eu pensei em algo mais, assim, um customzê straightforward, mais direto ao ponto. O Scipy, um pacote científico do Python, que tem, de certa forma, emprestou algumas coisas para o Scikit-Learn e para outras bibliotecas Python, ele também tem uma função de clustering hierárquico dentro do Scipy. E ele tem sim uma função nativa para você poder imprimir um dendograma. Então eu falei, se eu tenho os melhores parâmetros, só que eu não preciso, porque eu não quero cortar o dendograma por cluster, eu quero ver o dendograma e aí poder fazer umas brincadeiras de fazer alguns cortes e ver quantos clusters ele está gerando de acordo com os cortes que eu estou fazendo. Esse é o objetivo para simular, mostrar na prática aquilo que foi mostrado na teoria lá quando eu expliquei o dendograma. Então, como a gente vai fazer isso? O foco vai ser mostrar o dendograma. Eu vou criar um modelo aqui chamado modelo de dendograma, só para simplificar. Eu vou chamar uma função que eu importei lá em cima, chamada linkage, que ela faz justamente perform hierárquico aglomerativo e clustering. Ela faz um clustering aglomerativo no Scipy. O que eu passo para ele? Eu passo como parâmetro o meu x, também fazendo o toArray, preciso converter para um array. Digo qual é o método que eu quero. Eu vou usar o bestparams, o bestparam ag. Qual foi o método aqui no caso de linkage. E aqui eu tenho um parâmetro para facilitar a ordenação, o optimalOrdering. Ou seja, ele cria uma otimização ótima de ordenação na hora que vai facilitar para mim na hora de mostrar o dendograma. Então eu crio isso. Na verdade eu vou até mudar aqui, treinar modelo. Treinar modelo com Scipy. Treinei esse modelo, bem rápido. E agora sim eu vou mostrar o dendograma. Mostrar dendograma. Nosso trava-línguas. Então eu vou criar uma figura aqui, pltFigure. Vou criar um tamanho aqui de figura de 10 por 7. Se a gente precisar a gente pode aumentar isso aqui, se ficar muito pequeno. E aí eu uso uma função chamada dendograma do Scipy. O que eu mando para ele? Qual modelo? Então no caso aqui o modelo 10, o modelo do dendograma que eu criei. E aí eu posso dizer o seguinte, ele não vai mostrar o dendograma completo. Eu defino o que eu quero mostrar do dendograma, quantos níveis eu quero mostrar do dendograma. Porque imagina, eu tenho 991 itens. Imagina você conseguir 991 itens no eixo X e ele explodindo tudo. Então você escolhe, você tem um valor que você arbitra quantos níveis você quer ver. E você quer ver os primeiros níveis, os 10 primeiros níveis, os 10 últimos níveis. E aqui eu vou escolher ver os 6 últimos itens do meu dendograma. Onde ele mostra onde ele criou o cluster e onde o cluster ainda está como um cluster único. E a gente vai ver isso de forma diferente na hora do eixo X. Então o truncate mode mostra o que? Eu quero ver last p, os últimos X, os últimos p níveis do meu dendograma. Eu vou escolher um número aqui, inicial. Níveis do meu dendograma. Eu vou escolher um número aqui, inicial de 15. Vocês podem brincar com esse número. O leaf rotation é só na hora de visualizar eu ver esse leaf, essa folha em 90 graus. Vai ficar uma visualização bem mais agradável, bem parecida na verdade com a que eu mostrei nos exemplos. E o leaf font size igual a 10, também vai dar legibilidade no gráfico. E aí como é um gráfico do Matplotlib, eu posso pôr um título nele. Então eu vou colocar aqui, dendograma, clustering, hierárquico, aglomerativo. Eu coloquei tudo em português. Eu posto aqui um plt e coloco um label que está aqui, tamanho do cluster ou objeto de dado. Se o cluster for o único vai estar lá um índice daquele valor. E o plt.ylabel que é a distância. E eu dou um plt.show. Veja que ele mostra bem bonitinho aqui. E a gente já consegue ver algumas coisas. Com essa quantidade de p que eu coloquei, o p15. Então 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15. Então 15 níveis aqui embaixo. Veja que eu tenho um cluster aqui que tem 140 máquinas. De novo, com esse nível de 15. Se eu for aumentando esse número ele vai mostrando uma divisão maior, uma aglomeração maior. Então ele mostra o seguinte. Nesse cluster eu tenho 32, nesse cluster eu tenho 38, nesse cluster eu tenho 173. Lembrando que ele criou 149 clusters. Se eu colocar aqui um nível 50, já quebrou mais. Então já vê que tem cluster aqui de 48, ainda tem clusters grandes. Cluster com 4, 11, 24, 12, 48. São clusters razoáveis. Visualizar 11 máquinas está ok, agora visualizar 50 máquinas já não é tão ok. Mas de novo, eu arbitrei um p de 50 porque eu quero ver se eu estivesse quebrando os 50 clusters como ele faria isso. Se eu colocar aqui, vamos ver se ele vai conseguir colocar isso visualmente agradável. 149 clusters. Já não consegue ver. Vou até mexer um pouco na imagem. Se eu colocar aqui um 20 e colocar um 12. Mostrou. Talvez não esteja super, não está super hiper nítido. Mas a gente já consegue ver aqui. Pegando assim, por que tem valores entre parênteses e valores que não estão em parênteses? Vou até voltar aqui para ficar mais fácil de explicar. Quando está com 50, não sei se tem algum valor que ainda está sem parênteses. Não tem, todos estão com parênteses. Quando o valor está entre parênteses aqui, significa que é a quantidade de valores, de objetos que estão nesse cluster. Então eu tenho 14 objetos nesse cluster aqui. Se eu tivesse um valor aqui que não estivesse com parênteses, significa que é um objeto de dados específico de índice, do índice que está lá no eixo X, que está naquele cluster. Ou seja, é um cluster que só tem um valor. Então se eu colocar aqui o 149 que eu comentei, que é o que chegou, tem alguns valores aqui que não estão com parênteses. Com parênteses, por exemplo aqui, 635. Então 635 tem um cluster próprio. O 365 tem um cluster próprio. O 619 tem um cluster próprio. Ah, mas isso significa que ele é um multilayer? Não necessariamente. Deixa eu ver se aqui consegue ver melhor. Não deu para ver muito bem ainda. Vamos subir aqui. Deixa eu ver se dá para aumentar um pouco. Aumentei o máximo que eu pude. Aqui dá para ver um pouquinho melhor. Então 513, 608, 404, 601 são clusters únicos. São clusters que só tem um valor. Então se eu escolher essa máquina aqui, 608, ele não vai me indicar uma outra. Ou seja, ele não encontrou uma outra máquina parecida com essa. Mas vocês percebam que a maioria dos clusters está com máquinas. Então, 13 máquinas, 2 máquinas, 11, 16, 10, 11, que cai de novo naquilo que eu falei. O importante é a gente ter uma quantidade razoável de máquinas que o usuário não se sinta desconfortável em ter que escolher. Lembrando uma coisa, isso aqui eu só fiz uma representação, porque na hora que eu rodei esse algoritmo no SciPy, ele pode ter gerado, como eu não coloquei a quantidade de clusters de forma explícita, ele pode ter gerado uma divisão diferente. Mas eu quis mostrar aqui qual a importância do dendograma. Aí, como que é a questão do corte? Então, vamos pensar aqui, vou trabalhar com 50, vou ter 50 clusters aqui embaixo. Aí vou imaginar que eu queira trabalhar, cortar o dendograma. Então, se eu quiser cortar o dendograma, por exemplo, nessa posição, vamos supor que eu cortasse aqui, vou fazer 32. O que ele faria? Ele geraria 1, 2, 3, 4, 5 clusters. E é isso que a gente vai fazer, fazer um experimento aqui de cortar o dendograma. Então, cortar o dendograma. Como a gente faz isso? A gente vai criar um objeto que vai receber o corte desse dendograma, e a gente vai fazer o que? Vai apontar quantos clusters ele chegou nisso. Então, vou colocar aqui, clusters, DE, scipy, do dendograma. E aí que eu vou usar o CutTree, que é uma função que eu importei lá no começo. O CutTree recebe dois parâmetros, basicamente. Ele recebe um parâmetro que é o modelo, que é o nosso modelo DE, e qual a altura que eu quero cortar. Então, eu coloco o height, que é um parâmetro. Se a gente colocar um height aqui de 32, e aí eu vou fazer o que? Eu quero saber quantos clusters ele vai gerar. Então, quando eu faço isso, ele faz o corte e gera para cada ponto de dados os clusters. Então, o que eu vou fazer? Eu quero saber quais são os clusters únicos. Então, eu vou pegar NP, por isso que eu estou usando NumPy aqui, porque como isso aqui é um array que ele me gera, eu preciso ter um UNIC de um array. Eu não vou usar o UNIC de um pandas. Eu não estou com um DataFrame pandas aqui. Estou com um array do Python, um array do NumPy. Então, eu ponho um NP UNIC, passo para ele, só que quando ele der um UNIC, ele vai me mostrar, na verdade, quais foram os UNICs. Eu não quero saber quais foram, eu quero saber quantos são. Então, se eu clicar, se eu fizer só assim, ele me mostra 0, 1, 2, 3 e 4. Eu quero saber quais são. Então, eu poderia colocar aqui, LEN, desse número, tamanho, 5. Então, aqui é a demonstração prática do corte do dendrograma. Então, eu peguei aqui, como eu falei, 32, que é mais ou menos aqui, nesse parâmetro aqui do eixo Y, 32, que é aqui, eu pego quantos nós, quantos ramos, 1, 2, 3, 4, 5 ramos. Ou seja, ele cortou e dividiu isso em 5 clusters. Então, aqui é uma demonstração de como a gente usa o dendrograma para fazer uma análise e poder fazer a inferência, ou fazer a gente poder decidir qual corte que a gente quer fazer dentro do dendrograma. Por essa parte do dendrograma, a gente finaliza. Agora, a gente vai dar sequência na parte de visualizar os resultados que a gente obteve do modelo que a gente criou dentro do Scikit Learn. Então, vejo vocês no próximo vídeo.

#### Aula 17 - Visualizar Resultados da Clusterização

Sem transcrição

### Bloco 7 - Entrega App de Consulta

#### Aula 18 - Salvar Modelo e Dataset

Para concluir agora essa parte do nosso notebook, na verdade do nosso notebook, a gente vai salvar o modelo. Então, colocar aqui. A gente vai salvar tudo. A gente vai salvar o modelo, o pipeline, preprocessor, que a gente usou, e CSV atualizado. A gente vai criar um CSV com o dado do cluster, para que a gente possa carregar esse CSV, na hora de apresentar ele na nossa aplicação Streamlit, que nós vamos criar. Então, para salvar o modelo, a gente vai fazer o que? Aqui é o markdown. Então, vamos lá. Código. Então, a gente tem aqui. Salvar modelo e preprocessor. A gente dá um import no Joblib. Para salvar o modelo. O Joblib é aquela ferramenta que salva modelos, que a gente pode usar como arquivo pico. Salvar o modelo. A gente vai salvar o Joblib, usar o dump. Qual o nosso modelo melhor? O nosso bestmodel. E vamos salvar ele como? Vamos salvar ele com o nome de modelo-clusterização-laptops.pkl E vamos salvar o preprocessor. O que a gente tem, ele salva uma variável, que chama-se preprocessor. Vamos chamar ele de preprocessor-clusterização-laptops. Feito isso, salvamos os dois arquivos. Inclusive, se a gente abrir aqui, modelo e preprocessor. E agora a gente vai salvar um CSV, já com a informação do cluster, para que a gente possa colocar na nossa aplicação. Então, salvar CSV atualizado com dados de cluster. Para salvar um CSV, a gente vai usar o cluster-laptops.to.csv. Vamos escolher a pasta. Eu vou salvar na mesma pasta que está a minha entrada, que é o datasets.barra-datasets. E vou salvar ele como clusterização-laptops.csv. Uma coisa importante é que eu não quero salvar o índice como uma coluna, até porque essa coluna já tem um índice, porque eu mantive ela na versão original do meu dataset. Então, vou colocar um índice igual a falso. E aí, eu gerei um novo dataset. Aqui é o clusterização-laptops. Está com a mesma estrutura, só que ele está com uma coluna final aqui, chamada cluster, que é um número inteiro, que vai de 0 até 148, conforme o melhor modelo que a gente definiu. Finalizamos aqui o nosso experimento, vamos dizer assim, da parte do modelo. E vamos partir agora para construir a aplicação Streamlit, para ler esses dados, e poder lá, o usuário poder testar e ver quando ele selecionar um notebook, que outros notebooks aparecem recomendados para ele. Vejo vocês no próximo vídeo.

#### Aula 19 - Aplicação de Consulta no Streamlit

Agora que a gente concluiu o nosso modelo, já criamos um dataset novo em CSV, a gente vai poder usar esse CSV para alimentar uma aplicação que o usuário vai consultar como se fosse o nosso sistema de recomendação. Então o que a gente vai fazer? A gente vai criar um arquivo Python que vai ser chamado pelo Streamlit. Então vou criar aqui um novo arquivo chamado app.py e a gente vai construir o código. O código para fazer isso é um código relativamente simples, a gente não vai criar uma interface super complexa, rebuscada, não é essa a ideia, mas uma interface que o usuário consiga escolher um determinado modelo e com base naquele modelo a gente consiga fazer uma seleção de modelos e ver quais outros modelos que são parecidos com aquele. Então a primeira coisa que a gente faz é importar o Streamlit, e a gente vai importar o Streamlit, que é um pandas, que vai trazer os dados do CSV que a gente salvou. Agora o que a gente vai fazer é o seguinte, o Streamlit tem um conceito que você pode fazer cache dos dados, então se você não tem alteração desses dados, que é o nosso caso, ao invés de ele ficar toda hora pegando dado e trazendo, você pode cachear esse dado. Então a gente vai carregar o CSV e a gente não vai carregar ele direto, a gente vai pôr isso em uma função e a gente vai fazer uma uma assinatura, a gente vai colocar um decorador nessa função para dizer que a gente quer cachear o resultado desse dado. A gente vai carregar dados e colocar no cache do Streamlit, para evitar que a hora ele fique lendo o dado ali, se o dado de novo não tem alteração, ele pode usar o cache. A gente cria uma função chamada de carregar dados, que não tem parâmetro, é fixo o arquivo e ela retorna um DataFrame, que é o width.csv, e eu vou pegar aqui, datasets.clusterizao.laptops.csv e é isso que ele retorna. E aí eu vou criar um decorador para essa função, o decorador sempre com arroba, arroba st do Streamlit, ponto cacheData. Então, criei um decorador aqui para essa função. Criado esse decorador, eu posso agora criar um DataFrame com base nesses dados. Então eu vou criar um DataFrame chamado df, simplesmente df, e vou carregar esse dado. Até aqui só fiz a parte de carga de dados, agora vou começar a fazer a parte realmente do front-end. Então, como que a gente vai fazer esse front-end? Primeira coisa, eu vou criar uma barragem lateral, onde eu vou pôr o filtro de modelo que eu quero escolher, para depois ver os modelos que eu tenho similares para o filtro de recomendação. Então, eu vou ter uma sidebar aqui para o filtro. Então, eu coloco aqui st.sidebar, que é um método, .header, e vou chamar o header desse sidebar de filtros. E aí eu vou ter o filtro propriamente dito, que é de selecionar os modelos. Como é que eu seleciono os modelos? Bom, eu tenho um DataFrame que carregou todos os modelos. Eu preciso fazer o que? Trazer os modelos únicos do meu DataFrame, que está na variável model. Então, eu crio uma variável, que é a variável que eu vou apresentar na tela, para filtrar, que ela vai ser. Dentro do sidebar, eu vou usar um select box, que na verdade é uma lista. Eu coloco o label que eu fiz. Então, eu vou colocar aqui, selecionar modelo, e que dados eu quero colocar dentro desse select box. Então, eu vou colocar aqui, df model, e eu quero os valores únicos dessa variável. Então, ou seja, ele vai criar isso, e na hora que eu selecionar um modelo, ele vai jogar o modelo selecionado para dentro dessa variável model. Por que eu preciso disso? Para eu saber que cluster é esse modelo. Ah, é o cluster 40. Então, agora me traga todos os notebooks, laptops, que façam parte do cluster 40. É por isso que eu preciso do modelo selecionado, que eu vou usar esse modelo para trazer a informação do cluster. Então, selecionei o modelo. Agora, eu vou filtrar, eu vou criar um DataFrame modelo, que tenha os laptops, que traga o laptop daquele modelo que ele escolheu. Então, laptop filtros, modelo. Supondo que a gente tivesse modelos repetidos, eu não chequei isso, mas imaginando que eu tenha 991 modelos diferentes, eu não precisaria fazer o unique. Mas, eu preciso ter o registro, aquele registro específico daquele modelo, para eu poder obter o cluster dele. Dado que quando eu trago esse valor aqui, model unique, ele só retorna para mim o modelo. Então, agora eu preciso ir lá no DataFrame, filtrar aquele modelo e capturar a variável, a coluna do cluster. Então, eu venho aqui e coloco laptop modelo. Eu vou fazer o que? Eu vou filtrar df, df model, que seja igual ao modelo que eu selecionei no select box. Então, agora eu tenho um DataFrame com o modelo escolhido. E agora, eu vou filtrar, criar um DataFrame, que é o que vai aparecer na lista, com quais são os modelos do cluster selecionado. Ou seja, escolhi o modelo, o modelo tem um cluster. Mostre-me todos os outros modelos que também estão naquele cluster. Então, eu vou criar um outro DataFrame chamado df-laptops-final, que é o que eu vou apresentar. Eu vou fazer o que? df, df-cluster, igual ao df-laptops-modelo, ponto iloc. Por que eu uso o iloc? Porque aqui eu quero localizar um item específico, pelo índice e não pelo valor. Então, eu quero o índice 0, porque, na verdade, quando eu retornar esse modelo, ele vai trazer só um registro. Então, eu quero o primeiro, que é o 0, e eu quero que ele me traga a coluna cluster. Ou seja, traga todos os laptops onde o meu DataFrame original, a variável cluster do meu DataFrame original, seja igual ao cluster do modelo escolhido, que está aqui nesse outro DataFrame. Então, por isso que eu usei o iloc. O iloc me traga o primeiro registro desse DataFrame, a variável cluster, e eu vou filtrar o meu DataFrame total por essa variável. O que falta agora é a gente mostrar esses valores. Então, a gente faz o que? Visualizar modelos. E a gente faz o que? stwrite, a gente vai colocar uma mensagem ali, lá na recomendações de modelos, e a gente vai gravar uma tabela, sttable, e eu vou colocar o df-laptops, final. E aí, salvamos esse código. E o que a gente precisa fazer com esse código agora? Somente executá-lo. Então, se eu vir aqui, talvez eu tentei aqui fazer, é executar esse código fora do... Deixa eu ver se ele abre aqui. Vamos ver se ele está abrindo. ipenv shell, vamos ver se ele vai abrir. Clusterização hierárquica. Então, ele está aqui no correto, no virtual environment correto. Então, se ele está no virtual environment correto, o que a gente pode fazer? A gente pode abrir o streamlit, e executar o streamlit. Então, eu posso pôr aqui, streamlit run pp.py. O que acontece quando eu fizer isso? Ele vai abrir uma seção no meu browser, apontando para essa aplicação. Então, eu clico aqui. Não sei se ele vai abrir na mesma janela. Ele abriu uma outra. Deixa eu ver aqui. Ele deu um erro. Só para mostrar para vocês. Ele falou o seguinte, loadData linha 9. Ele falou que loadData is not defined. Por quê? Porque eu coloquei um loadData aqui, que na verdade não era loadData, era carregar dados. Então, aqui, carregar dados. Pronto. Por isso que deu erro. Salvei aqui. Posso voltar. Deixa eu ver se eu consigo rodar daqui mesmo. Pronto. Então, ele deu erro. Ele deu ctrl-c. Vou fechar o terminal aqui, para liberar. Terminal. New Terminal. Pronto. Loop in Vishel. E streamlit run pppy. Ele abriu outra janela de novo, mas vou trazer para cá. Pronto. Então, aqui ele mostrou o seguinte. Esse primeiro notebook. Aqui ele trouxe, ele trouxe em filtro, ele trouxe todo mundo, tá? Então, deixa eu ver aqui e falar. Bom, traga notebooks que sejam iguais a um Acer Extensa, que é um Core i5 de 8 giga. O que ele fez? Ele filtrou todos que, na verdade, fazem parte do qual cluster? Cluster 53. Então, vamos dar uma inspecionada nisso. Rating. Eu tenho Rating 62, 60 e 56. Todos são processadores Intel. Tem máquina da Acer, tem máquina da MSI, tem máquina da Infinix. Em termos de preço, esses dois até estão próximos. Esse aqui está mais caro, pelo menos 50% mais caro desse. E já é um Core i7. Os dois aqui tem 16, esse aqui tem 8 de memória. Todos eles tem 512 de capacidade. Todos eles tem uma GPU integrada. Todos eles com monitor de 14 e todos eles com a mesma resolução e mesma garantia. Ou seja, aqui eu estou pegando só os três primeiros, tá? Se a gente olhar os de baixo, MSI também está bem próximo desse outro MSI, que também está com uma pontuação alta, 65, mas também é um Core i7. Então, ele pegou alguns Core i5, pegou o Core i7, pegou máquinas de quase 1000 dólares e misturou aqui nesse cluster. Vou pegar um outro cluster aqui qualquer, Macbook. O Macbook, se ele indicar alguma coisa diferente, eu vou achar estranho, mas vou fazer assim, vou deixar a tela cheia. Então, o Macbook, ele achou ele mesmo, na verdade o cluster aqui é o cluster 113. Ele indicou outros Macbooks, ele achou um outro Macbook M2, um outro M2, também o preço bem mais alto, preço de quase metade do preço a mais, 50% a mais. Um outro M2 Apple e um outro M2 Apple. Vamos pegar outra marca aqui, notebook gamer aqui, Game Laptop. Então, aqui nesse Game Laptop, ele pegou um game, trouxe outro game, trouxe um outro game, um outro Game Laptop, aqui já não trouxe um game, trouxe um computador que não é uma faixa de gamer. O outro game, esse aqui, ele trouxe uma, ele não trouxe um computador que está escrito como Game Laptop, aqui ele não está descrito como game, lembrando que a gente não colocou a informação do modelo lá, mas é um computador que tem uma placa gráfica dedicada de 4GB. Então, é um computador que serviria para uma finalidade de game. Aqui também, não está escrito que é um game, mas ele também tem uma placa gráfica. Então, aqui dando uma ideia geral, aqui tem 900 modelos, então dá para navegar aqui à vontade. Finalizando essa parte prática aqui com a nossa aplicação, mostrando que, considerando que a gente usou um conjunto até razoável de características, a gente usou na verdade 18 variáveis, a gente tirou o índice e o modelo e fizemos as tratativas, ele até que conseguiu generalizar bem para um modelo que não foi feito naturalmente para essa finalidade. Eu queria entender padrões, mas eu consegui entender esses padrões e consegui que ele me desse aqui uma forma de eu poder recomendar produtos similares a um produto que eu estou escolhendo. Então, espero que vocês tenham gostado dos vídeos, gostado da parte prática, da parte teórica e que eu consiga aplicar isso em outros contextos, de realmente entender os dados, os padrões, mas também vimos aqui que a gente pode trabalhar com um conceito, com um contexto diferente aqui no caso de recomendação para esse tipo de algoritmo de Clustering Hierárquico. Então, vejo vocês no próximo módulo.

> [voltar](../../../README.md) para a página anterior.
