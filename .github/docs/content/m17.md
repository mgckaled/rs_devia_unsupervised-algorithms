# Módulo 17 - Distributed Stochastic Neighbor Embedding (t-SNE)

> [voltar](../../../README.md) para a página anterior.

## Sumário
- [Módulo 17 - Distributed Stochastic Neighbor Embedding (t-SNE)](#módulo-17---distributed-stochastic-neighbor-embedding-t-sne)
  - [Sumário](#sumário)
  - [Objetivo](#objetivo)
  - [Projeto Prático](#projeto-prático)
  - [Conceitos e Explicações](#conceitos-e-explicações)
    - [O que é t-SNE?](#o-que-é-t-sne)
      - [Principais Características e Funcionamento do t-SNE](#principais-características-e-funcionamento-do-t-sne)
      - [Como o t-SNE Funciona em Detalhes](#como-o-t-sne-funciona-em-detalhes)
      - [Aplicações do t-SNE](#aplicações-do-t-sne)
      - [Considerações Finais](#considerações-finais)
    - [Etapas do t-SNE em Análise de Dados](#etapas-do-t-sne-em-análise-de-dados)
      - [Exemplos de Aplicação](#exemplos-de-aplicação)
    - [Mecânica do t-SNE](#mecânica-do-t-sne)
      - [1. **Cálculo das Probabilidades de Vizinhança no Espaço de Alta Dimensão**](#1-cálculo-das-probabilidades-de-vizinhança-no-espaço-de-alta-dimensão)
      - [2. **Mapeamento das Probabilidades para o Espaço de Baixa Dimensão**](#2-mapeamento-das-probabilidades-para-o-espaço-de-baixa-dimensão)
      - [3. **Minimização da Divergência de Kullback-Leibler (KL)**](#3-minimização-da-divergência-de-kullback-leibler-kl)
      - [4. **Iteração até a Convergência**](#4-iteração-até-a-convergência)
    - [Exemplos de Mecanismos em Ação](#exemplos-de-mecanismos-em-ação)
    - [Limitações do t-SNE](#limitações-do-t-sne)
      - [1. **Escalabilidade e Complexidade Computacional**](#1-escalabilidade-e-complexidade-computacional)
      - [2. **Dependência de Hiperparâmetros**](#2-dependência-de-hiperparâmetros)
      - [3. **Dificuldade em Interpretar Escalas Globais**](#3-dificuldade-em-interpretar-escalas-globais)
      - [4. **Instabilidade dos Resultados**](#4-instabilidade-dos-resultados)
      - [5. **Dificuldade em Reproduzir Resultados**](#5-dificuldade-em-reproduzir-resultados)
      - [6. **Integração com Dados Novos**](#6-integração-com-dados-novos)
      - [7. **Comparação entre Diferentes Conjuntos de Dados**](#7-comparação-entre-diferentes-conjuntos-de-dados)
      - [8. **Visualização em Espaços Superiores a 3D**](#8-visualização-em-espaços-superiores-a-3d)
      - [9. **Interpretação dos Resultados**](#9-interpretação-dos-resultados)
      - [10. **Limitações no Tratamento de Ruído**](#10-limitações-no-tratamento-de-ruído)
  - [Transcrição das Aulas](#transcrição-das-aulas)
    - [Bloco 1 - Introdução](#bloco-1---introdução)
      - [O que é t-SNE?](#o-que-é-t-sne-1)
      - [Etapas do t-SNE](#etapas-do-t-sne)
      - [Mecânica do t-SNE](#mecânica-do-t-sne-1)
      - [Desafios e Limitações do t-SNE](#desafios-e-limitações-do-t-sne)
      - [Aplicações do t-SNE](#aplicações-do-t-sne-1)
    - [Bloco 2 - Conceitos Complementares](#bloco-2---conceitos-complementares)
      - [t-SNE vs. PCA](#t-sne-vs-pca)
      - [O Hiperparâmetro Perplexity](#o-hiperparâmetro-perplexity)
    - [Bloco 3 - Projeto Prático](#bloco-3---projeto-prático)
      - [Apresentando o Projeto Prático](#apresentando-o-projeto-prático)
    - [Bloco 4 - AED](#bloco-4---aed)
      - [Carga de Dados](#carga-de-dados)
      - [Análise Univariada - parte 1](#análise-univariada---parte-1)
      - [Análise Univariada - parte 2](#análise-univariada---parte-2)
      - [Análise Bivariada](#análise-bivariada)
    - [Bloco 5 - Treinamento do Modelo](#bloco-5---treinamento-do-modelo)
      - [Preparação dos Dados para Modelo](#preparação-dos-dados-para-modelo)
      - [Treinamento do Modelo](#treinamento-do-modelo)
    - [Bloco 6 - Análise de Resultados](#bloco-6---análise-de-resultados)
      - [Visualização dos Resultados em 2D](#visualização-dos-resultados-em-2d)
      - [Visualização dos Resultados em 3D](#visualização-dos-resultados-em-3d)

## Objetivo

O objetivo é apresentar o t-SNE, discutir suas etapas, mecânica, desafios, aplicações e compará-lo com o PCA. Abordaremos o hiperparâmetro perplexity e realizaremos um projeto para uma empresa de cosméticos, desde EDA até a visualização dos resultados. Serão discutidos os tópicos: o que é t-SNE, suas etapas, mecânica, desafios, aplicações e comparação com o PCA, além do projeto prático.

## Projeto Prático

Desenvolveu-se uma ferramenta visual para uma empresa de cosméticos criar um algoritmo de recomendação de produtos. Utilizou-se o algoritmo t-SNE para visualizar os produtos em um gráfico 2D e ajustar o hiperparâmetro perplexity. A estrutura do projeto envolveu carregar dados, explorar análises, treinar o modelo e armazenar simulações com diferentes valores de perplexity para visualizar os resultados em um gráfico animado.

## Conceitos e Explicações

> [Acesso aos slides](../pdf/ppts_m17.pdf) do conteúdo do módulo.

### O que é t-SNE?

O t-SNE, sigla para **t-distributed Stochastic Neighbor Embedding** (ou Embedding Estocástico de Vizinhança Distribuído em t), é uma técnica de redução de dimensionalidade não linear usada principalmente para visualização de dados de alta dimensão em espaços de duas ou três dimensões. Criado por Laurens van der Maaten e Geoffrey Hinton, o t-SNE é amplamente utilizado em diversas áreas, como aprendizado de máquina, bioinformática e análise de dados, para ajudar a compreender a estrutura dos dados complexos.

#### Principais Características e Funcionamento do t-SNE

1. **Redução de Dimensionalidade Não Linear**:
   - O t-SNE reduz a dimensionalidade de dados de alta dimensão, preservando as relações de proximidade (vizinhança) entre pontos de dados. Isso significa que pontos que são próximos no espaço de alta dimensão tendem a permanecer próximos no espaço de baixa dimensão gerado pelo t-SNE.

2. **Estocasticidade**:
   - A estocasticidade (aleatoriedade) no t-SNE é introduzida na forma de uma distribuição de probabilidade que modela as relações de vizinhança entre pontos de dados. No espaço de alta dimensão, as probabilidades são calculadas de maneira que pontos próximos tenham alta probabilidade de serem selecionados como vizinhos.
   - No espaço de baixa dimensão, o t-SNE ajusta os pontos para que a distribuição de probabilidades de vizinhança seja semelhante à do espaço de alta dimensão.

3. **Distribuição t de Student**:
   - A técnica usa a distribuição t de Student com um grau de liberdade (t-distribution) para calcular a probabilidade de vizinhança no espaço de baixa dimensão. Isso ajuda a tratar a questão da densidade dos dados, sendo mais eficaz na separação de grupos de dados em comparação com outras distribuições, como a gaussiana.

4. **Preservação da Estrutura Local**:
   - O t-SNE é eficaz em preservar a estrutura local dos dados, o que o torna excelente para identificar agrupamentos naturais e padrões locais nos dados.

5. **Custo Computacional**:
   - Embora poderoso, o t-SNE pode ser computacionalmente intensivo, especialmente para conjuntos de dados grandes. Existem implementações otimizadas para lidar com essa limitação, como a versão “barnes-hut” que reduz a complexidade computacional.

6. **Interpretação Visual**:
   - A visualização resultante do t-SNE é muito intuitiva e ajuda a identificar padrões, agrupamentos e anomalias em dados complexos. É uma ferramenta valiosa para explorar e entender conjuntos de dados antes de aplicar outras técnicas de análise ou modelagem.

#### Como o t-SNE Funciona em Detalhes

1. **Cálculo das Probabilidades no Espaço de Alta Dimensão**:
   - Para cada ponto, calcula-se a probabilidade de que outro ponto seja um vizinho próximo. Isso é feito usando uma distribuição de probabilidade condicional baseada na distância euclidiana entre os pontos, ajustada por um parâmetro de suavização, chamado de perplexidade.

2. **Cálculo das Probabilidades no Espaço de Baixa Dimensão**:
   - No espaço de baixa dimensão, inicia-se com uma distribuição inicial de pontos e ajusta-se iterativamente para minimizar a diferença entre as probabilidades de vizinhança nos dois espaços. A diferença é medida usando a divergência de Kullback-Leibler (KL).

3. **Minimização da Divergência de Kullback-Leibler**:
   - A divergência de Kullback-Leibler é minimizada utilizando um método de descida de gradiente. O objetivo é ajustar os pontos no espaço de baixa dimensão para que as distribuições de probabilidades de vizinhança sejam o mais semelhantes possível às do espaço de alta dimensão.

#### Aplicações do t-SNE

- **Visualização de Dados em Alta Dimensão**: O t-SNE é amplamente utilizado para visualizar dados de alta dimensão, como imagens, textos e dados genômicos, de uma forma que seja compreensível e intuitiva para humanos.
- **Exploração de Clusters**: É uma ferramenta útil para explorar e identificar agrupamentos naturais em conjuntos de dados complexos.
- **Análise de Redes Neurais**: Utilizado para visualizar as ativações de camadas ocultas em redes neurais, ajudando a entender como a rede está representando os dados.

#### Considerações Finais

O t-SNE é uma ferramenta poderosa para explorar e visualizar dados complexos. No entanto, é importante ter em mente que os resultados podem variar com diferentes configurações de hiperparâmetros e que a interpretação das visualizações deve ser feita com cuidado, considerando as características específicas dos dados em análise.

### Etapas do t-SNE em Análise de Dados

O t-SNE (t-distributed Stochastic Neighbor Embedding) é uma técnica de redução de dimensionalidade que é particularmente útil na visualização de dados de alta dimensão em espaços de menor dimensão (geralmente 2D ou 3D). Para aplicar o t-SNE dentro de uma análise de dados, segue-se um conjunto de etapas que ajudam a garantir que os dados sejam preparados adequadamente e que os resultados obtidos sejam interpretáveis e úteis.

1. **Definição do Objetivo da Análise**
   - **Compreender o que se deseja visualizar**: clusters, padrões, anomalias, etc.
   - **Escolher o espaço de visualização**: 2D ou 3D, dependendo da complexidade dos dados e da visualização necessária.

2. **Coleta e Pré-processamento de Dados**
   - **Coleta de Dados**: Reunir os dados que serão analisados.
   - **Limpeza de Dados**: Remover valores ausentes, duplicados e tratar outliers.
   - **Normalização/Escalonamento**: Ajustar a escala dos dados para que diferentes variáveis tenham a mesma importância. Normalização ajuda a evitar que variáveis com grande amplitude de valores dominem a distância euclidiana entre pontos.
   - **Transformação de Dados**: Aplicar técnicas como PCA (Principal Component Analysis) para pré-reduzir a dimensionalidade se necessário, o que pode ajudar a acelerar o t-SNE e melhorar os resultados.

3. **Configuração de Hiperparâmetros do t-SNE**
   - **Perplexidade**: Controla o equilíbrio entre a preservação de estruturas locais e globais dos dados. Valores comuns variam de 5 a 50, dependendo do tamanho e da densidade dos dados.
   - **Dimensionalidade**: Escolha do número de dimensões de saída (2D ou 3D).
   - **Número de Iterações**: Define quantas vezes o algoritmo irá ajustar os pontos para minimizar a divergência de Kullback-Leibler. Um valor comum é entre 250 e 1000 iterações.
   - **Taxa de Aprendizado (Learning Rate)**: Ajusta a rapidez com que o algoritmo convergirá para uma solução estável. Valores típicos são entre 10 e 1000.
   - **Método de Inicialização**: Inicialização aleatória ou baseada em PCA.

4. **Cálculo das Probabilidades de Vizinhança no Espaço de Alta Dimensão**
   - **Distâncias Euclidianas**: Calcular distâncias entre todos os pontos no espaço de alta dimensão.
   - **Distribuição de Probabilidades**: Converter essas distâncias em probabilidades de vizinhança, usando uma distribuição gaussiana. A perplexidade influencia como essas probabilidades são distribuídas.

5. **Mapeamento das Probabilidades para o Espaço de Baixa Dimensão**
   - **Distribuição t de Student**: Utilizada no espaço de baixa dimensão para definir as probabilidades de vizinhança entre pontos.
   - **Inicialização de Posições**: Posicionar inicialmente os pontos no espaço de baixa dimensão (geralmente de forma aleatória ou baseada em PCA).

6. **Minimização da Divergência de Kullback-Leibler**
   - **Cálculo da Divergência de Kullback-Leibler**: Medir a diferença entre as distribuições de probabilidades no espaço de alta e baixa dimensão.
   - **Descida de Gradiente**: Ajustar iterativamente as posições dos pontos no espaço de baixa dimensão para minimizar a divergência de Kullback-Leibler, mantendo as relações de vizinhança o mais próximo possível das originais.

7. **Execução do t-SNE**
   - **Iteração**: Executar o algoritmo até que as posições dos pontos no espaço de baixa dimensão se estabilizem ou atinjam um critério de parada, como um número máximo de iterações ou uma convergência satisfatória.

8. **Visualização dos Resultados**
   - **Plotagem em 2D ou 3D**: Visualizar os dados no novo espaço reduzido, geralmente usando ferramentas de plotagem como matplotlib, Seaborn, ou software de visualização de dados.
   - **Análise Visual**: Identificar padrões, clusters, outliers ou outras características de interesse nos dados visualizados.

9. **Interpretação dos Resultados**
   - **Análise dos Clusters**: Identificar e interpretar agrupamentos naturais nos dados.
   - **Exploração de Padrões**: Compreender como diferentes características dos dados se relacionam no espaço reduzido.
   - **Validação de Resultados**: Comparar com outros métodos de visualização ou análise para garantir a validade das conclusões.

10. **Refinamento e Ajustes**
    - **Ajuste de Hiperparâmetros**: Se os resultados não forem satisfatórios, ajustar os hiperparâmetros (perplexidade, taxa de aprendizado, número de iterações) e reexecutar o t-SNE.
    - **Novo Pré-processamento**: Eventualmente realizar novo pré-processamento dos dados, como filtragem de outliers ou alteração da normalização.

11. **Documentação e Comunicação**
    - **Documentação dos Resultados**: Registrar os passos e configurações usadas, bem como as conclusões tiradas.
    - **Comunicação das Descobertas**: Compartilhar visualizações e insights com a equipe ou stakeholders relevantes, explicando os padrões e anomalias identificadas.

#### Exemplos de Aplicação

- **Exploração de Dados Genômicos**: Identificar padrões em dados de expressão gênica para entender doenças ou condições biológicas.
- **Análise de Dados de Imagem**: Visualizar ativações de camadas em redes neurais para compreender como a rede aprende a reconhecer imagens.
- **Análise de Dados de Clientes**: Descobrir segmentos de clientes em dados de marketing para personalizar campanhas.

O t-SNE é uma ferramenta poderosa para a análise exploratória de dados, oferecendo uma maneira intuitiva de visualizar a estrutura complexa dos dados em alta dimensão.

### Mecânica do t-SNE

A mecânica do t-SNE (t-distributed Stochastic Neighbor Embedding) envolve uma série de passos matemáticos e algoritmos que convertem dados de alta dimensão em uma representação de baixa dimensão, preservando as relações de proximidade entre os pontos de dados. Aqui, vamos detalhar como o t-SNE realiza esse mapeamento e quais são os conceitos principais envolvidos.

#### 1. **Cálculo das Probabilidades de Vizinhança no Espaço de Alta Dimensão**

- **Objetivo**: Estimar a probabilidade de que um ponto \( j \) é um vizinho próximo de um ponto \( i \) com base na distância entre eles.
  
- **Distância Euclidiana**:
  - Primeiro, calcula-se a distância euclidiana entre todos os pares de pontos no espaço de alta dimensão. 
  - A fórmula é:

    ![d_{ij} = \| x_i - x_j \|](https://latex.codecogs.com/png.image?\dpi{110}\bg{white}d_{ij}=\|x_i-x_j\|)

    onde \( x_i \) e \( x_j \) são os vetores de características dos pontos \( i \) e \( j \).

- **Distribuição Gaussiana**:
  - As distâncias são convertidas em probabilidades usando uma distribuição gaussiana centrada em cada ponto.
  - A probabilidade condicional \( p_{j|i} \) de que o ponto \( j \) é vizinho de \( i \) é dada por:

    ![p_{j|i} = \frac{\exp(-d_{ij}^2 / 2\sigma_i^2)}{\sum_{k \neq i} \exp(-d_{ik}^2 / 2\sigma_i^2)}](https://latex.codecogs.com/png.image?\dpi{110}\bg{white}p_{j|i}=\frac{\exp(-d_{ij}^2/2\sigma_i^2)}{\sum_{k\neq%20i}\exp(-d_{ik}^2/2\sigma_i^2)})

    onde \( \sigma_i \) é uma medida de escala que controla a largura da distribuição para o ponto \( i \), ajustada para manter a perplexidade constante.

- **Perplexidade**:
  - A perplexidade é um hiperparâmetro que controla a complexidade local da vizinhança. Ela é definida como:

    ![\text{Perplexidade} = 2^{H(P_i)}](https://latex.codecogs.com/png.image?\dpi{110}\bg{white}\text{Perplexidade}=2^{H(P_i)})

    onde \( H(P_i) \) é a entropia da distribuição \( P_i \).

- **Probabilidades Simétricas**:
  - As probabilidades são simetrizadas para garantir que \( p_{ij} = p_{ji} \), usando:

    ![p_{ij} = \frac{p_{j|i} + p_{i|j}}{2n}](https://latex.codecogs.com/png.image?\dpi{110}\bg{white}p_{ij}=\frac{p_{j|i}+p_{i|j}}{2n})

    onde \( n \) é o número total de pontos.

#### 2. **Mapeamento das Probabilidades para o Espaço de Baixa Dimensão**

- **Objetivo**: Representar a proximidade entre os pontos no espaço de baixa dimensão de maneira semelhante ao espaço de alta dimensão.

- **Inicialização dos Pontos**:
  - Os pontos no espaço de baixa dimensão \( y_i \) são inicializados aleatoriamente ou usando PCA para uma estimativa inicial.

- **Distribuição t de Student**:
  - As probabilidades de vizinhança no espaço de baixa dimensão são modeladas usando uma distribuição t de Student com um grau de liberdade, que tem uma cauda mais longa que a gaussiana. Isso é importante para evitar que pontos distantes influenciem a estrutura local.
  - A fórmula para a probabilidade \( q_{ij} \) é:

    ![q_{ij} = \frac{\left(1 + \| y_i - y_j \|^2 \right)^{-1}}{\sum_{k \neq l} \left(1 + \| y_k - y_l \|^2 \right)^{-1}}](https://latex.codecogs.com/png.image?\dpi{110}\bg{white}q_{ij}=\frac{\left(1+\|y_i-y_j\|^2\right)^{-1}}{\sum_{k\neq%20l}\left(1+\|y_k-y_l\|^2\right)^{-1}})

#### 3. **Minimização da Divergência de Kullback-Leibler (KL)**
  
- **Objetivo**: Ajustar os pontos no espaço de baixa dimensão para que a distribuição de probabilidades \( Q \) (no espaço de baixa dimensão) seja o mais próximo possível da distribuição \( P \) (no espaço de alta dimensão).

- **Divergência de Kullback-Leibler**:
  - A métrica utilizada para medir a diferença entre as distribuições de probabilidade \( P \) e \( Q \) é a divergência de Kullback-Leibler:

    ![KL(P \| Q) = \sum_{i \neq j} p_{ij} \log \left( \frac{p_{ij}}{q_{ij}} \right)](https://latex.codecogs.com/png.image?\dpi{110}\bg{white}KL(P\|Q)=\sum_{i\neq%20j}p_{ij}\log\left(\frac{p_{ij}}{q_{ij}}\right))

- **Descida de Gradiente**:
  - A posição dos pontos \( y \) no espaço de baixa dimensão é ajustada iterativamente para minimizar a divergência KL.
  - O ajuste é feito pela atualização dos pontos usando descida de gradiente:

    ![y_i \leftarrow y_i - \eta \frac{\partial KL}{\partial y_i}](https://latex.codecogs.com/png.image?\dpi{110}\bg{white}y_i\leftarrow%20y_i-\eta\frac{\partial%20KL}{\partial%20y_i})

    onde \( \eta \) é a taxa de aprendizado.

#### 4. **Iteração até a Convergência**

- **Objetivo**: Repetir os passos de ajuste até que os pontos no espaço de baixa dimensão se estabilizem.

- **Número de Iterações**:
  - O processo é repetido por um número pré-determinado de iterações, geralmente variando entre 250 e 1000, ou até que uma condição de convergência seja satisfeita (como mudanças mínimas nas posições dos pontos).

- **Dinâmica de Aprendizado**:
  - Para evitar o colapso de pontos e assegurar uma boa dispersão inicial, a taxa de aprendizado é ajustada dinamicamente, e a força das forças de repulsão pode ser suavizada nos primeiros estágios de treinamento.

### Exemplos de Mecanismos em Ação

- **Identificação de Clusters**: A capacidade do t-SNE de manter a estrutura local dos dados ajuda a identificar clusters naturais que podem não ser evidentes no espaço original.
- **Visualização de Redes Neurais**: No contexto de aprendizado profundo, t-SNE pode ser usado para visualizar a representação das ativações das camadas ocultas de uma rede neural, proporcionando uma visão sobre como a rede está processando os dados.

O t-SNE é uma técnica poderosa para explorar e compreender a estrutura dos dados complexos em alta dimensão, oferecendo uma visualização intuitiva que facilita a identificação de padrões e estruturas subjacentes.

### Limitações do t-SNE

O t-SNE (t-distributed Stochastic Neighbor Embedding) é uma técnica poderosa para a redução de dimensionalidade e visualização de dados complexos, mas também apresenta algumas limitações significativas que devem ser levadas em consideração ao utilizá-lo. Abaixo estão algumas das principais limitações do t-SNE:

#### 1. **Escalabilidade e Complexidade Computacional**

- **Alto Custo Computacional**:
  - O t-SNE é computacionalmente intensivo, especialmente para grandes conjuntos de dados. A sua complexidade é geralmente \(O(N^2)\), onde \(N\) é o número de pontos de dados. Isso pode tornar a aplicação de t-SNE inviável para conjuntos de dados muito grandes sem o uso de versões otimizadas ou aproximações.
  
- **Uso de Memória**:
  - A necessidade de manter as distâncias entre todos os pares de pontos em memória pode ser um desafio, exigindo considerável espaço de armazenamento e memória RAM para grandes datasets.

#### 2. **Dependência de Hiperparâmetros**

- **Ajuste Sensível**:
  - Os resultados do t-SNE podem variar significativamente com diferentes configurações de hiperparâmetros, como a perplexidade, a taxa de aprendizado e o número de iterações. Encontrar a configuração ideal pode ser um processo demorado e não trivial.
  
- **Perplexidade**:
  - A perplexidade, que controla a escala de vizinhança local, é um parâmetro crítico. Valores inadequados podem levar a uma má representação dos dados, onde um valor muito baixo foca excessivamente nos detalhes locais, enquanto um valor muito alto pode perder a estrutura local.

#### 3. **Dificuldade em Interpretar Escalas Globais**

- **Foco em Estruturas Locais**:
  - O t-SNE é muito bom em preservar a estrutura local dos dados, mas frequentemente falha em preservar as distâncias globais. Isso pode dificultar a interpretação da estrutura global dos dados e a identificação de relacionamentos de longa distância.

- **Representação não Linear**:
  - A transformação não linear aplicada pelo t-SNE significa que as distâncias no espaço reduzido não têm uma interpretação direta em relação às distâncias no espaço original.

#### 4. **Instabilidade dos Resultados**

- **Variabilidade**:
  - Pequenas variações nos dados de entrada ou nos hiperparâmetros podem resultar em representações significativamente diferentes. Essa falta de robustez pode tornar a interpretação dos resultados menos confiável.

- **Dependência da Inicialização**:
  - A inicialização aleatória dos pontos pode levar a diferentes resultados em diferentes execuções, aumentando a variabilidade nos mapas de baixa dimensão.

#### 5. **Dificuldade em Reproduzir Resultados**

- **Reprodutibilidade**:
  - Devido à sensibilidade aos hiperparâmetros e à inicialização, reproduzir exatamente os mesmos resultados em execuções diferentes pode ser difícil. Isso pode ser um problema para aplicações que exigem resultados consistentes e replicáveis.

#### 6. **Integração com Dados Novos**

- **Inserção de Novos Dados**:
  - Integrar novos dados a uma visualização t-SNE existente pode ser complicado, pois o t-SNE é um método de lotes (batch) que não se adapta facilmente à adição incremental de novos pontos de dados.

#### 7. **Comparação entre Diferentes Conjuntos de Dados**

- **Comparabilidade**:
  - Comparar resultados de t-SNE entre diferentes conjuntos de dados é problemático, uma vez que a transformação é altamente não linear e sensível a pequenas variações nos dados de entrada e nos parâmetros.

#### 8. **Visualização em Espaços Superiores a 3D**

- **Limitação de Dimensionalidade de Saída**:
  - Embora o t-SNE possa ser usado para reduzir os dados para espaços de maior dimensão (como 3D), a visualização eficaz é geralmente limitada a 2D ou 3D. Representar e interpretar espaços de maior dimensão pode não ser prático ou intuitivo.

#### 9. **Interpretação dos Resultados**

- **Interpretação Complexa**:
  - A interpretação dos gráficos gerados pelo t-SNE requer cuidado, pois a técnica não preserva proporções de distância exatas e a proximidade no espaço reduzido não corresponde necessariamente à proximidade no espaço original.

- **Clusters e Análises**:
  - A identificação de clusters em t-SNE pode ser visualmente atraente, mas é crucial validar esses clusters com métodos adicionais, pois a técnica pode introduzir artefatos que não correspondem a agrupamentos reais no espaço de alta dimensão.

#### 10. **Limitações no Tratamento de Ruído**

- **Sensibilidade a Outliers**:
  - t-SNE pode ser sensível a outliers ou ruídos nos dados, o que pode distorcer significativamente a representação no espaço de baixa dimensão.

Embora o t-SNE seja uma ferramenta valiosa para a visualização de dados complexos e de alta dimensão, é fundamental estar ciente de suas limitações. Compreender essas limitações ajuda a evitar interpretações errôneas dos resultados e a escolher os métodos apropriados para cada aplicação específica. Usar t-SNE em conjunto com outras técnicas de análise e visualização pode proporcionar uma visão mais completa e robusta dos dados.

## Transcrição das Aulas

### Bloco 1 - Introdução

#### O que é t-SNE?

Vamos começar falando agora do que é o T-SNE. T-SNE é uma sigla para T-Distributed Stochastic Neighbor Embedding. Ele é um algoritmo poderoso para redução de dimensionalidade. Ele é amplamente utilizado para visualização de conjuntos de dados de alta dimensão. Ele foi desenvolvido por Lawrence Van Den Maarten e Geoffrey Hinton em 2008. E o T-SNE é particularmente bem sucedido em capturar a estrutura local de dados complexos e revelar agrupamentos .44 em espaços de baixa dimensão, geralmente em 2D, um plano cartesiano que a gente conhece, ou em 3D. Como que o T-SNE funciona? Ele começa com a compreensão de que em um espaço de alta dimensão, cada ponto de dados pode ser visto como um ponto em um espaço multidimensional. O algoritmo transforma as distâncias entre os pontos em probabilidades condicionais que representam similaridades. A ideia é que pontos que são mais próximos uns dos outros tem uma alta probabilidade de serem vizinhos e aqueles que estão distantes tem uma probabilidade baixa. Por que usar o T-SNE? A gente utiliza o T-SNE para simplificar a complexidade dos dados, permitindo-nos observar padrões e grupos que são difíceis de visualizar em múltiplas dimensões. Por exemplo, na análise de dados genéticos, o T-SNE pode ajudar a identificar subgrupos de células com padrões de expressão gênica similares, apesar da complexidade e da grande dimensão de dados.

#### Etapas do t-SNE

Quais são as etapas do algoritmo de T-SNE? O primeiro aspecto necessário é medir as similaridades. Imaginem que cada ponto de dados em um espaço de alta dimensão seja uma pessoa em uma festa. O T-SNE começa calculando quanto cada pessoa, no caso o ponto de dados, prefere ficar perto de cada outra pessoa. Essa preferência é medida usando a distância entre eles, pessoas que são mais próximas, são mais propensas a serem amigas. Isso é convertido em probabilidades, com amigos próximos tendo alta probabilidade de ficarem juntos e conhecidos distantes tendo baixa probabilidade. A segunda parte da etapa do T-SNE é criar um mapa. Com base nessa medição de similaridades, o T-SNE cria um mapa, que é uma representação mais simples, onde as pessoas, os pontos de dados, também estão presentes, mas dessa vez o espaço é muito menor. Como mudar de um grande salão para uma sala pequena? Aqui ele tenta colocar todos no espaço pequeno, mantendo as amizades, similaridades, tanto quanto possível. Na terceira etapa do T-SNE, ele começa a fazer a comparação das similaridades. Ele olha para as probabilidades no espaço original, que é o grande salão, e no espaço pequeno, que é aquela sala que ele realocou as pessoas, e verifica quão bem as amizades foram preservadas. Se duas pessoas que eram próximas do espaço original não estiverem próximas do espaço reduzido, há uma penalidade. O T-SNE ajusta as posições do espaço pequeno, tentando minimizar essas penalidades. Quarta etapa do T-SNE é a otimização. Esse processo de ajustar as posições é um pouco como um jogo de quebra-cabeça, onde você tenta fazer todos felizes com suas posições relativas. O T-SNE faz muitos pequenos ajustes, sempre tentando manter amigos próximos, até que encontre uma configuração no espaço menor, onde as amizades sejam bem representadas de acordo com as probabilidades originais. O quinto e último passo, o resultado final, acontece onde depois de muitos ajustes, o T-SNE finaliza com uma configuração onde, em geral, pessoas que eram próximas do espaço de muitas dimensões, ainda estão próximas do espaço de poucas dimensões. Isso permite visualizar e entender padrões e grupos usados, que seriam muito difíceis de perceber em um espaço de muitas dimensões. Para ilustrar de forma diagramática, eu trouxe aqui uma visão de dois estágios, onde basicamente a gente percorre as etapas do T-SNE. Então, no primeiro ponto, imaginando que temos aqui muitas dimensões, cada ponto de dados é uma simples célula, e ele determina a similaridade entre essas células e pontos de dados, que seria a etapa 1. Depois, o que ele vai fazendo é que, de forma randômica, vai projetando essas células como pontos no espaço de baixa dimensão. Então, aqui, enquanto a gente tem várias dimensões, esse espaço aqui é um espaço unidimensional. E aí ele começa a determinar as similaridades entre esses pontos nesse espaço de baixa dimensão. Com base nessa similaridade, ele vai movendo os pontos até que as similaridades entre os pontos na baixa dimensão, de uma certa forma, lembrem ou reproduzam as similaridades encontradas em altas dimensões.

#### Mecânica do t-SNE

Vamos falar um pouco agora sobre a mecânica do T-SNE. Primeiro falando de probabilidades e similaridades, o primeiro passo no T-SNE é converter as distâncias euclidianas entre os pontos em probabilidades que somam 1 para cada ponto. Isso é feito usando a distribuição gaussiana, que é a distribuição normal, para os dados de entrada de alta dimensão e a distribuição de T-distributor, para o mapa de baixa dimensão. Coisa importante, quando a gente coloca o nome aqui T-distributed, esse T-distributed é exatamente a distribuição T-distributed que existe na estatística. Então o que na verdade ele faz, ele converte essa distância que está no espaço de uma distribuição gaussiana, representada nesse gráfico aqui por essa linha verde, e transforma esses pontos de alta dimensão numa distribuição de T-distributed para um mapa de baixa dimensão. O desafio da maldição da dimensionalidade, que é algo que a gente já comentou nos fundamentos do machine learning, ela se refere ao fenômeno onde o aumento das dimensões usadas leva a problemas como o aumento do espaço vazio e a distorção das relações de distância. O T-SNE aborda esse desafio focando nas probabilidades de similaridade, em vez de apenas replicar as distâncias métricas, o que ajuda a preservar a estrutura local dos dados. Um outro aspecto ainda sobre a mecânica do T-SNE é que é feito um processo chamado otimização por gradiente descendente. Ele usa essa técnica para minimizar a diferença entre as probabilidades no espaço de alta dimensão e no espaço projetado de baixa dimensão. Esse processo é conhecido como minimização da divergência de Kullback-Leibler entre duas distribuições de probabilidades, que mede o quão bem o espaço de baixa dimensão representa o de alta dimensão. Aqui um exemplo dessa curva de Kullback-Leibler, dessa divergência, que é onde ele faz essa otimização por gradiente descendente para minimizar a diferença entre essas duas distribuições de probabilidade da alta dimensão com a baixa dimensão.

#### Desafios e Limitações do t-SNE

Quais são os desafios e as limitações do T-SNE? Primeira delas é a sensibilidade aos hiperparâmetros. Um dos desafios ao usar o T-SNE é a sensibilidade a hiperparâmetros, como por exemplo perplexity, que indica quantos vizinhos próximos cada ponto considera. A escolha desse parâmetro pode afetar significamente a aparência dos gráficos finais e não existe um valor único ideal para todos os conjuntos de dados. Outro aspecto, um outro desafio ou limitação é a limitação na interpretação de clusters. Apesar do T-SNE ser excelente para visualizar agrupamentos, é importante não tirar conclusões precipitadas sobre os clusters visualizados. O T-SNE pode às vezes criar ou exagerar clusters devido a sua forma de otimização e não necessariamente representar divisões naturais nos dados. Um último aspecto a salientar é o custo computacional e a escalabilidade. O T-SNE pode ser computacionalmente intensivo, especialmente para conjuntos de dados muito grandes. Isso ocorre porque o algoritmo compara todos os pares de pontos para calcular as probabilidades, o que se pode tornar impraticável com o aumento do número de pontos.

#### Aplicações do t-SNE

Vou falar agora de algumas aplicações do T-SNA. A primeira delas é a biologia computacional. No campo da biologia computacional, o T-SNA é frequentemente utilizado para analisar dados de sequenciamento de células únicas, ajudando os pesquisadores a identificar tipos de células com base em seus perfis de expressão gênica. Esse tipo de análise pode revelar novos subtipos celulares e potenciais alvos terapêuticos. Um outro tipo de aplicação é o reconhecimento de imagens. O T-SNA também é aplicado ao reconhecimento de imagens, onde pode ser usado para reduzir a dimensão de representações de imagens antes de classificações ou outras análises. Visualizar essas representações pode ajudar a entender como diferentes categorias de imagens são percebidas pelo modelo de A.

### Bloco 2 - Conceitos Complementares

#### t-SNE vs. PCA

Vamos falar agora um pouco sobre o comparativo entre o T-SNE e o PCA, que foi um módulo que a gente viu anteriormente na parte de redução de dimensionalidade. No PCA, conhecido aqui em português como análise de componentes principais, o método que é usado é o linear, que significa que ele projeta os dados originais em direções que maximizam a variância, sem tentar preservar relações não lineares. Já do lado do T-SNE, o método empregado é o não-linear, ele é focado em preservar a estrutura local dos dados ao mapear pontos próximos em alta dimensão para pontos próximos em baixa dimensão. Vantagens do PCA, ele é rápido e eficiente para grandes conjuntos de dados, bom para reduzir ruídos e destacar características mais importantes dos dados. Já no T-SNE, ele é excelente para visualizar agrupamentos e padrões de dados de alta complexidade, muito eficaz em preservar a vizinhança local, permitindo visualizações intuitivas. Quais são as desvantagens do PCA? Ele não é eficaz para capturar complexidades e padrões não lineares dos dados, ele pode perder informações importantes de dados que tem estrutura intrincada. Desvantagens do T-SNE, ele é computacionalmente intensivo, especialmente com grandes datasets. Ele é sensível a parâmetros como perplexidade ou perplexity, o que pode exigir ajustes finos para resultados ótimos. Quais são os melhores usos para o PCA? Análise inicial para obter uma visão geral das principais variações dos dados, útil em campos como finanças e outras áreas onde as relações lineares são predominantes. Já no T-SNE, análise exploratória de dados para descobrir agrupamentos e padrões ocultos, especialmente útil em biologia, marketing e em qualquer campo onde as relações não lineares são mais importantes.

#### O Hiperparâmetro Perplexity

Vou falar sobre o hiperparâmetro perplexity, e aqui vou dar outro exemplo, uma outra analogia. Imagine que você está em uma festa e quer decidir com quantas pessoas você deve conversar. Se você conversar com muitas pessoas, você vai ter uma boa ideia geral de quem está na festa. Se você conversar apenas com algumas pessoas, vai entender muito bem essas poucas pessoas, mas não tanto sobre a festa como um todo. A perplexidade ou perplexity no tsn é semelhante a isso. É como se fosse um número que você escolhe para decidir quantas pessoas ou pontos de dados cada ponto deve considerar como seus vizinhos mais próximos. A perplexidade ou perplexity ajuda a determinar quantos vizinhos mais próximos cada ponto deve considerar. Um número pequeno significa que cada ponto só olha para os seus vizinhos muito próximos, ou seja, conversa apenas com seus amigos mais próximos. Um número grande significa que cada ponto considera muitos outros pontos como seus vizinhos, ou seja, conversa com muitas pessoas na festa. Como escolher a perplexidade? E aí é uma dica que parece um pouco ambígua, mas não muito alta, não muito baixa. Se a perplexidade é muito alta, o tsn pode perder os pequenos detalhes, porque cada ponto está tentando ser amigo de quase todos. Se é muito baixa, os pontos podem acabar formando muitos pequenos grupinhos isolados, o que pode não representar bem a festa, que são os dados. Não depende muito dos dados, não existe um número mágico perfeito para todos os conjuntos de dados, você normalmente tem que experimentar diversos valores para ver qual dá a melhor visão geral dos seus dados.

### Bloco 3 - Projeto Prático

#### Apresentando o Projeto Prático

Vou falar agora sobre o projeto que a gente vai fazer de TSNL, o projeto prático. Aqui eu estou trazendo um exemplo de uma empresa de cosméticos que deseja trabalhar num algoritmo de recomendação para seus clientes e comércios, de forma a indicar produtos com base em características similares, como tipo, de pele, marca e preço. Porém como a combinação entre marcas, tipos de produtos e demais características é muito grande, analisar esses dados antes de procurar um algoritmo é extremamente desafiador e a empresa não gostaria de criar um algoritmo que recomende produtos que não façam sentido. Dessa forma a gente vai desenvolver uma ferramenta visual para que essa empresa possa visualizar seus produtos num chart 2D, de forma a identificar se produtos próximos em termos de características estão próximos e para isso a gente vai usar o algoritmo TSNL. Como comentado anteriormente, esse algoritmo possui uma sensibilidade a hiperparâmetros, especialmente perplexity, então a gente vai fazer um chart de forma animada com diversos parâmetros desse hiperparâmetro perplexity, vou fazer um gráfico que vai animar de acordo com a mudança do perplexity. Falando agora da estrutura do projeto, a gente vai carregar os dados, fazer um EDA, treinar o modelo, armazenar as simulações e a gente vai alterar esse hiperparâmetro perplexity e ir armazenando essas simulações de acordo com vários hiperparâmetros, com base nesse armazenamento que a gente vai fazer dentro do projeto, a gente vai visualizar os resultados num chart animado. Então agora vamos para o código.

### Bloco 4 - AED

#### Carga de Dados

Fala Dev, a gente vai começar agora a fazer o nosso projeto prático no algoritmo de T-SNE Onde a gente vai explorar aquele caso de uso dos cosméticos Para desenvolver uma visualização usando esse algoritmo Então, aqui eu já tenho instaladas as bibliotecas que eu vou colocar aqui Inclusive no começo do nosso notebook E vamos começar aqui o projeto Primeira coisa, vamos criar um arquivo Vou chamar esse arquivo de tsnecosmeticos.ypynb Vou colocar aqui quais foram as bibliotecas que eu precisei instalar para esse notebook O pandas, o seaborn, o pyplot, o scikit-learn, o matplotlib, o ypywidgets, o ypykernel E eu não precisei usar o Streamlit Vamos começar a importar as bibliotecas agora Então, as bibliotecas são a parte de EDA e visualização de dados primeiro Então, eu vou importar o pandas.spd Vou importar o plotlib Express.spx E eu vou importar o seaborn.ssns E as bibliotecas que eu vou importar para a parte de machine learning Ou melhor dizendo, a parte do tsn Então, eu vou importar a biblioteca para fazer a transformação de colunas Que é o Column Transformer, que a gente já usou algumas vezes Vou usar dois transformadores de variáveis O One Hot Encoder e o Standard Scalar para valores numéricos E por fim, o do próprio algoritmo, o tsne, que está dentro do pacote do sklearn Subpacote aqui, manifold E vou importar aqui o módulo do tsne Então, carregamos as bibliotecas Depois de você ter feito a instalação E o primeiro ponto aqui a fazer é a gente poder carregar os dados Vou criar uma célula aqui, markdown, carregar os dados O arquivo está nessa pasta, chamada data set, chama-se cosmeticos.csv Então, a gente vai carregar data frame Vou chamar esse data frame de dfcosmeticos pd.read.csv Vou colocar o cabinho, .datasets.cosmeticos.csv Importei, e agora vou mostrar, visualizar a estrutura dfcosmeticos.info E ele mostra aqui as colunas Temos aqui um total de 11 colunas, e 1472 cosméticos, sendo que todos eles estão com valores NOT NULL Vou abrir aqui o arquivo, rapidamente, só para mostrar um pouco da estrutura dele, visualmente falando Então, a gente tem aqui o tipo, vou até deixar as duas coisas aqui, o tipo e o dicionário de dados Tipo, e o dicionário de dados aqui do lado, pronto Então, eu tenho aqui qual tipo, está em inglês, tipo Moisture, tem alguns outros tipos de cosméticos Tratamento, de limpeza, cleanser, máscara facial, creme para os olhos, protetor solar, então são alguns tipos de cosméticos aqui dentro dessa loja, dessa empresa A marca, qual a marca desse cosmético, qual o nome dele, o preço em dólares, arredondado aqui no caso para o inteiro, não tem casos decimais O Rating, que é um valor de 0 a 5, que dá como se fosse uma nota para o produto Ingredientes, que na verdade é um campo texto, que na verdade é uma lista de ingredientes E aqui, algumas variáveis binárias, booleanas, para mostrar se o produto serve ou não para esses tipos de pele Então, se ele serve para pele mista, se ele serve para pele seca, para pele normal, para pele oleosa e para pele sensível Então, os valores aqui, se a gente for olhar aqui, 1, 1, 1, 1, 1, então ele vai colocando aqui quais são os tipos de tratamento que ele serve Sempre o binário 0, 1, então basicamente aqui essa estrutura do nosso dataset Voltando aqui para o código, que apesar de eu já ter mostrado ali, vamos mostrar alguns dados Então, visualizar os primeiros registros, ndfcosméticos.red de 10 Então, tem aqui, pele mista, pele seca, pele normal, pele oleosa, pele sensível, os booleanos A lista de ingredientes, como eu falei, é um campo texto bem grande, onde cada ingrediente está separado do outro por vírgulas O preço em dólar, o rating de 0 a 5, o nome do produto, a marca e o tipo do produto Vamos visualizar agora os últimos registros, visualizar os últimos registros, ndfcosméticos.pale de 10 Então, temos aqui a mesma estrutura, também com os ingredientes separados e o tipo de produto Como eu comentei, 1472 produtos estão nesse catálogo, e 11 colunas, 11, 12 colunas para 11 colunas dentro desse DataFrame Lembrando que quando a gente for fazer o one-hot-encode das colunas categóricas, essas dimensões vão aumentar É justamente isso que a gente vai ver na hora de aplicar o algoritmo do TSE Então, começamos aqui com o processo de identificar ou entender melhor o conjunto de dados que a gente está trabalhando nesse experimento, nesse projeto prático E vamos à sequência nos próximos vídeos, ao EDA e às próximas fases.

#### Análise Univariada - parte 1

Dando sequência agora no nosso projeto prático, eu vou começar aqui com a parte de EDA e especificamente com a parte de análise univariada, ou seja, a gente vai analisar algumas variáveis desse DataFrame de forma independente. Uma coisa importante antes disso, é que a gente tem uma variável que eu comentei, que é uma variável que ela possui, é um texto que possui os ingredientes separados por vírgula, e aí como é que a gente trata isso, como é que a gente pode transformar isso em alguma coisa que depois a gente possa utilizar para fazer alguma estatística de quais são, por exemplo, os ingredientes que existem nessa base. Então antes até propriamente de a gente fazer essa análise univariada, vou propor aqui de a gente fazer uma transformação, então a gente vai fazer uma transformação aqui, a gente vai transformar os valores da coluna ingredientes em um novo DataFrame. Como é que a gente vai fazer isso? Primeira coisa, eu vou fazer uma cópia do Cosméticos para Cosméticos EDA, só para diferenciar, eu vou colocar aqui, cosméticos.copy. E aí eu vou fazer o seguinte, eu vou pegar o cosméticos EDA, que é o novo DataFrame que eu vou trabalhar só para o EDA, e vou falar para ele fazer um split da coluna ingredientes separando isso por vírgula, então eu vou pegar a coluna ingredientes, vou pegar o string dessa coluna, a parte de texto dessa coluna, e vou usar o split, e aí eu vou ter que definir no split qual que é o separador que eu quero usar para fazer esse split, para fazer essa divisão desse string. Vou fazer isso, se eu olhar o DataFrame agora, DF Cosméticos EDA, o que ele fez? Ele transformou o meu DataFrame que estava com todas as colunas, num DataFrame que agora tem apenas a coluna ingredientes, fazendo split de vírgula, então ele transformou na verdade o que era um campo texto, num campo que está como se fosse uma lista mesmo. E agora como é que eu faço para essa lista se transformar num conjunto, vamos supor que eu quero pegar esse item, esse item, esse item, e separar tudo isso em linhas separadas para que eu possa computar algumas estatísticas, então eu venho aqui e coloco DF Cosméticos EDA, que eu vou na verdade criar um DataFrame novo, é igual a DF Cosméticos EDA, ponto EXPLODE, o que o EXPLODE faz na verdade é ele vai pegar essa variável que é uma lista e vai transformar essa lista que está numa coluna em linhas, ponto EDA, DF Cosméticos ponto EDA, ponto EXPLODE, entre parênteses, o argumento. E agora se eu for olhar o DF Ingredientes, veja que ele transformou essas 1472 linhas, que é uma linha para cada cosmético, ele foi explodindo cada uma delas e ele transformou isso num DataFrame de 45 mil linhas, então depois se eu quiser fazer uma análise de quantos ingredientes tem mais ou menos em cada um dos cosméticos, eu já tenho um DataFrame para fazer isso. Voltando agora na parte das estatísticas, de fazer uma análise univariável, a primeira coisa que eu vou fazer é pegar e fazer as estatísticas das variáveis, como um todo. Então eu vou fazer aqui DF Cosméticos, ponto DESCRIBE, e eu tenho aqui preço, o preço máximo de produto é 370, preço mínimo é 3 dólares, o Rating é de 0 a 5, o Rating mínimo é 0 e o Rating máximo é 5, a pele mista aqui não tem muito sentido apesar de ser um campo inteiro, na verdade ele só vai assumir valores 0 ou 1, então aqui não faz muito sentido calcular média ou desviar o padrão, ele só vai assumir esses dois tipos de valores. Vamos começar agora a fazer uma análise da distribuição de valores entre categorias, então vou fazer uma distribuição, por exemplo, da variável TIPO, para isso eu vou fazer aqui, criar uma estrutura percentual por tipo, eu vou fazer um pouquinho diferente do que eu tenho feito anteriormente, colocando rótulos nas barras, então até aqui é ok a mesma coisa, eu pego e calculo o VALUE COUNTS da variável TIPO, que é a minha variável de interesse, e divido isso pelo tamanho do DataFrame, e multiplico por 100, aí o que eu faço? Eu dou um percentual TIPO, igual a percentual TIPO, dou um RESET INDEX, para conseguir fazer essa, para poder mostrar os percentuais lá na barra, dentro do PlotList, aí eu digo o seguinte, como agora eu estou dando um RESET INDEX, eu tenho somente duas colunas, então eu vou dar aqui, na verdade quando eu faço isso, ele cria uma série, como eu estou dando um RESET INDEX, aqui ele já criou um DataFrame, então eu tenho que dar os nomes para as colunas desse DataFrame, então eu vou dar o nome aqui de TIPO, e o nome de PERCENTUAL, então ou seja, aqui era uma série, que tinha um valor TIPO, que tinha só o percentual de cada um deles, quando eu dou o RESET INDEX, ele transformou o TIPO em uma coluna, e ele tem uma outra coluna percentual, que não foi dado nome nenhum, então eu dei o nome dessa coluna de percentual, dei o nome da primeira coluna de TIPO, e agora eu tenho um DataFrame de fato, que na verdade é %tipo.columns, um atributo de %tipo, aí o que eu vou fazer, eu vou criar a figura, eu não vou fazer o plot direto, que vai ser um gráfico de barras, onde eu vou ter aqui %tipo, o X é igual a TIPO, o Y é igual a %tipo, eu vou colocar cores diferentes de acordo com o TIPO, e o TEXT, ou seja, o que vai lá na barra, em cada uma das barras, é a coluna percentual. Feito isso, eu preciso atualizar o plot, antes de mostrar ele, para melhor visualizar os rótulos, os labels. Então o que eu faço, eu uso o comando FIG, eu uso o comando UPDATE TRACES, e eu consigo colocar um formato ao qual eu quero apresentar o label, eu uso aqui um parâmetro chamado TEXT TEMPLATE, e aí eu coloco aqui, que eu quero, eu coloco entre percentuais, a estrutura que eu quero, então eu vou colocar TEXT, eu vou receber um texto, e eu quero ele com 4 casas decimais, e eu digo a posição desse texto, então eu coloco TEXT POSITION, e digo que eu quero esse texto OUTSIDE, ou seja, fora da barra do gráfico, se eu colocasse INSIDE, ele colocaria dentro, e aí eu dou um FIG.SOW. Então veja que agora, ao invés de a gente só conseguir ver aqui o percentual, por eu posicionar na barra, nos gráficos que a gente fazia nos outros módulos, eu tenho aqui os eixos com os nomes corretos, TIPO, percentual, e tenho no INSIDE da barra, no OUTSIDE da barra, os percentuais de cada um. E aí o que a gente percebe, é que não existe uma predominância tão forte de um tipo em relação ao outro. Lógico que no caso de SunProtect, protetor solar, ele é quase metade do Moisture, que é o primeiro, mas pegando a diferença entre Moisture, Cleanser, Face Mask, eles estão todos meio dentro da mesma faixa, então não existe nenhuma predominância pelo que mostra no gráfico dessa base de dados. Uma outra coisa que a gente pode fazer, é adaptar esse mesmo gráfico para que ele mostre as marcas, e são muitas marcas, e aí a gente pode mostrar todas as marcas, mas a gente pode também fazer assim, me mostra as 10 marcas com maior relevância, com maior percentual de produtos dentro da base, eu vou mostrar como fazer isso. Então a gente pode usar até a mesma estrutura aqui, pode copiar essa estrutura, então vou fazer a distribuição da variável marca, vou chamar aqui de percentual marca, e aqui também vou trocar para marca, aqui percentual marca, marca, marca, só para deixar tudo certinho aqui, e aqui é marca. Aí tem um detalhe, como são as 10 marcas, apesar de eu querer mostrar apenas as 10 marcas mais principais, faz mais sentido a gente mostrar um gráfico de barras horizontal, ou se fosse um ranking de um gráfico de barras vertical, a gente vai mudar um pouco isso. Então aqui a gente vai usar o percentual marca, só que eu não quero todo o conjunto, eu até vou mostrar com todo o conjunto, para ver que a visualização fica bem difícil, e depois eu vou trazer só os 10 primeiros. Então como vou fazer um gráfico de barras horizontal, a primeira coisa que tem que fazer aqui é trocar a orientação para H, que é horizontal, e inverter os eixos, o que é o eixo Y, que é o eixo X, então percentual vai para o eixo X, e o Y vira a marca, o color também é a marca, e o text é o percentual. Mesma coisa a questão do layout, a gente vai usar o mesmo layout, e vou mostrar. Então veja, como são muitas marcas, fica impraticável de ver esse gráfico de uma forma interessante, inclusive os próprios labels, eles encavalam aqui nas barras, tem que posicionar muito pequenininho aqui, para você conseguir ver cada uma das marcas e o percentual que ela representa. Eu estou colocando na primeira barra, que é 5.36, que é a clinic, que nem aparece aqui no label, porque não cabe. Então como é que a gente faz para ver o top 10? Bem simples, ao invés de a gente mostrar o dataframe inteiro, a gente pode fazer um head de 10, que é exatamente o que a gente faz quando a gente quer ver as primeiras linhas do dataframe. Então eu quero que ele faça o mesmo gráfico, somente trazendo as 10 primeiras posições. A hora que eu rodo esse gráfico, ele vai mostrar, aí fica muito mais nítido, legível para eu entender. A clinic vem em primeiro lugar com 5.36, a C4H vem com 4.48, a XSEIDO vem com 4.28 e assim por diante. Então dessa forma eu consigo visualizar essas informações num gráfico de barras. Só para ver o tamanho, vamos pensar se o tamanho do problema que nós teríamos em relação a essa questão de marcas, só colocar aqui a quantidade de marcas. Se a gente tiver quantidade de marcas, a gente pode fazer o seguinte, len devcosmeticos.marca.unique. Por que isso? Porque se eu fizer UNIQUE sem o LEN, ele vai listar as marcas únicas dentro da minha base. Mas eu não quero listar, eu quero saber quantas são. Então eu venho aqui e coloco o LEN, que me traz o tamanho desse array que ele retornou aqui. Então são 116 marcas. Imagina ter que olhar 116 marcas naquele gráfico, teria que ser um outro tipo de gráfico ou quebrar alguma coisa. Então eu preferi trazer aqui as 10 primeiras. Com isso a gente conclui a primeira parte da análise univariada. A gente vai continuar com algumas outras variáveis fazendo essa análise individualizada.

#### Análise Univariada - parte 2

Dando sequência a nossa análise univariada, uma variável que gostaria de explorar aqui para entender um pouco o comportamento é a variável ingrediente, vou pegar aqui e vou fazer a mesma coisa, vou pegar essa estrutura que eu fiz aqui de marca e vou copiar ela para aplicar ela em ingrediente, porém ingrediente lembra que a gente criou um DataFrame específico para isso, então na verdade o que a gente vai fazer é criar aqui preceitual ingrediente e eu não vou usar o df-cosméticos, eu vou usar o df-ingredientes, aqui o ValueCounts eu vou contar a coluna ingredientes, continua desse jeito e vou dividir o df-ingredientes, aqui a mesma coisa, ingredientes, ingredientes, aqui é ingrediente percentual, aqui eu vou fazer a mesma coisa, depois eu vou mostrar porque, quantos ingredientes tem, então vai ser percentual, ingrediente, ingrediente aqui na cor e tudo certo, então só fiz a substituição aqui mudando também o DataFrame e mostra aqui os 10 primeiros, então ele mostra aqui que eu tenho Glicerina, Água, Fenoxetanol, Butileno Glicol, Sódio Alunico, por aí vai, então ele mostra aqui os percentuais e veja que ele colocou um micro aqui porque na verdade o percentual é bem baixo, é 0.0000 da base, ele foi dividindo isso na base então você já tem, está bastante quebrado, mas o produto que mais aparece é a Glicerina até mais do que a Água, e aí um detalhe importante, quantos ingredientes existem na base, então se a gente vier aqui e colocar DF ingredientes e colocar aqui o ingrediente Unique, cadê DF, está com o ingrediente, ah, aqui o DF ingredientes, ele está com um, vamos pegar aqui o percentual, ele está como série, percentual e ingredientes, vamos ver aqui, 7.298 ingredientes diferentes, então esse sim é um grande problema de maldição da dimensionalidade, porque a gente tem 1.000, só recuperar aqui, 1.000 e poucos registros na base, 1.472 registros, e eu tenho 7.298 ingredientes, eles estão em uma coluna que está quebrada, então imagina, se a gente explodir essas 1.000, 7.298 colunas no Unhotting Code, se eu não quisesse explodir mais nenhuma outra coluna de Unhotting Code, eu já teria um problema aqui que eu tenho mais dimensões do que registros, do que linhas de dados, ou seja, eu teria muito espaço vazio, que é um grande problema de dimensionalidade, então só para demonstrar aqui que quando a gente trabalha com esses grandes conjuntos de dados, a gente precisa de abordagens diferentes, é justamente a isso que se propõe o TSNET, você poder usar uma abordagem diferente para outros tipos, lembrando que só a questão das marcas já são 116, então a gente já precisa explodir as marcas, ele tem que explodir os tipos e mais as outras variáveis, então já vai gerar um grande volume de colunas. Então na análise univariada, eu vou colocar aqui a distribuição da variável preço, vou começar agora a falar das variáveis numéricas, preço, então no caso da variável numérica o ideal é fazer um histograma, aqui eu vou pegar DF Cosméticos, coluna preço, vou colocar aqui um título, histograma da variável preço. Ele mostra aqui o diagrama, então uma grande concentração aqui, parece uma curva normal, mas uma grande concentração de valores aqui na faixa dos US$ 35 a US$ 49, onde se concentra o maior volume, e alguns altilários, alguns produtos com valor mais elevado, acima de US$ 250 a US$ 300. Quando a gente faz a análise do rating, o rating não deveria ter muitas surpresas, porque o rating é uma variável ordinal, na verdade, não é uma variável antitativa, vamos pensar assim, porém, como ela está com uma fração 4.1 e 4.2, não dá para considerar ela ordinal, a gente teria que tirar a casa decimal dela, e vamos dizer, discretizar ela, transformar ela em uma variável contínua, para uma variável discreta, e aí de repente tratar essa variável como uma variável categórica ordinal, a gente não vai fazer isso, a gente vai trabalhar ela da forma de seu estado natural. Então, vamos colocar aqui o rating, que é aquela nota de 0 a 5, e aqui, grande parte das notas concentradas aqui nessa faixa de 4.3, 4.4, com alguns poucos altilários aqui, nota 0, nota 1, algumas poucas notas 2, 3, mas grande parte dos resultados desse marketplace, vamos pensar assim, dessa empresa, com notas bastante altas. Com isso, a gente termina essa primeira parte do EDA, agora sim, na parte de análise univariada, vejo vocês no próximo vídeo.

#### Análise Bivariada

Dando sequência a nossa parte de análise exploratória, a gente vai trabalhar agora com análise bivariada, fazer análise de duas em duas variáveis. Então vou colocar aqui, análise bivariada. Outra coisa que eu vou trazer aqui pra gente analisar é fazer um plot comparando o tipo do produto e o preço, pra ver se tem grandes discrepâncias. Pra isso a gente pode usar um boxplot, pode fazer um plot de distribuição, então aqui tem um boxplot, gráfico de caixa, por tipo e preços. Então pra fazer esse plot é bem simples, no plot, liprex.box, a gente diz o DataFrame, a coluna X, nossa coluna de valores vai ser o preço, nossa coluna Y que é a variável categórica tipo, a cor, a gente vai usar a variável tipo, a gente vai usar uma orientação também na horizontal, vai ficar mais fácil de ver, e uma coisa que dá pra ver também é o famoso HoverData, o que o HoverData faz? Se você quiser posicionar em uma barra, eventualmente em um ponto que você quer analisar com mais cuidado, o HoverData mostra esse ponto, então você consegue ver os outliers aqui. Então clico aqui, e mostra Moisture, Cleanser, Treatment, FaceMask, EyeCream e SunProtect. Então se eu quiser analisar esses pontos aqui de forma individual, por exemplo, esse ponto de dados do Moisture tem um valor aqui de 325, eu sei que a marca desse produto é Lamer, eu sei que esse produto aqui é Fresh, se eu quisesse, por exemplo, mostrar o nome do produto, então se eu quisesse aqui colocasse Nome, então eu consigo também saber que esse produto que custa 325 dólares na marca Lamer é esse produto aqui, Miracle Limited Edition Creme de Lamer, então você consegue até identificar que produtos aqui que estão sendo considerados como outliers dentro de cada um dos tipos de produtos dessa empresa. Nós vamos fazer a mesma coisa para tipo e rating, e avaliar aqui, a gente percebe uma coisa que é importante, que os produtos de acordo com a natureza deles sim tem uma variação grande de valor, uma variação de valor, então por exemplo, SunProtect tem aqui a sua mediana por volta dos 40 dólares, EyeCream por volta dos 50, FaceMask também está muito próximo da parte de protetor solar, Treatment já vai para casa dos 65, de limpeza profunda vai para a faixa dos 20, 20 e poucos, e o Moisture tem uma faixa bem grande de preços com bastante outliers, está na faixa dos 50. Olhando agora a variável rating, ver se tem alguma variação grande de rating de acordo com o tipo, então a gente pode ver aqui preço, tipo, então ao invés de ser preço e tipo, a gente vai pegar tipo e rating, e podemos até mostrar aqui marca e nome da mesma forma, então ele mostra aqui, quase todos eles tem um ponto aqui, um outlier que é o zero, que são os produtos que ganharam nota zero de cada um dos tipos, ele fala até quais são as marcas e produtos que tiveram notas altas, então tem esse Cleansonic, Refreshing e por aí vai, vários tipos de produtos, e aí a gente vê aqui uma nota, como já tinha visto na tendência normal, notas bem próximas, o único que recebe notas em geral mais baixas são os produtos, os creams para os olhos, os EyeCreams. Outra coisa para mostrar, agora fazendo uma relação de preço e rating por tipo, a gente pode pegar aqui e fazer, correlação preço e rating, ou seja, saber se o rating dos produtos mais caros ganham notas maiores, então como a gente pode fazer esse gráfico? A gente pode usar um scatterplot, um gráfico de expressão, eu posso usar aqui um scatter, eu posso colocar, vou colocar aqui Def Cosméticos, no eixo X eu vou colocar o preço, no meu eixo Y eu vou colocar o rating, a cor eu vou colocar o tipo e o HoverData, o dado que eu quero ver, eu vou colocar a marca, quando eu quiser posicionar em um certo ponto, então vou colocar aqui marca, então vai dar para ver muita informação em um gráfico só, então eu coloco o preço, de 0 a 300, de 300 e pouco, preço mais caro é 370, o rating de 0 a 5, então veja aqui, não há um padrão, ou seja, eu tenho produtos com rating alto em todas as faixas de preço e também em todos os tipos, as cores aqui mostram os tipos dos produtos, então também não há uma grande concentração, um padrão dizendo o preço de tal produto está ligado a tal rating, ou seja, não é possível traçar algum tipo de correlação direta entre essas variáveis. Por fim, para concluir essa parte de análise bivariada, antes de partir para o algoritmo, trazer uma matriz de correlação, avaliar se existe uma correlação forte entre as variáveis desse DataFrame, gerar matriz de correlação, então a gente vai gerar ela atrás do pandas, e vamos jogar isso dentro de um DataFrame, então vou chamar de matriz de correlação cosméticos, que é o dfcosmeticos.core, e aí um ponto importante, parâmetro numeric only igual a true, por quê? Porque existem variáveis categóricas dentro desse DataFrame, eu não fiz nenhum drop de coluna nesse DataFrame, ele não foi mexido, eu ia falar imexível, como eu diria antigamente, mas quando eu coloco esse numeric only igual a true, eu digo, faça o plot, crie essa matriz apenas considerando as variáveis que são numéricas no DataFrame. Então, gerei a matriz, e agora eu vou plotar a matriz, plotar matriz de correlação. Aqui eu vou usar o Seaborn, Hitmap do Seaborn, vou usar aqui a matriz de correlação cosméticos que eu gerei, vou colocar que o valor mínimo é menos 1, que seria a correlação negativa perfeita, o valor máximo igual a 1, que é a correlação positiva perfeita, e vou colocar o anot, que é de annotation, igual a true, para mostrar os valores dentro do Hitmap. Então ele apresenta aqui os valores dentro desse Hitmap, não tem nada assim muito caracterizado, a não ser, por exemplo, as correlações que existem entre essas variáveis binárias, ou seja, eu tenho aqui pele mista com pele seca, como alguns desses tipos de pele são excludentes quando ele é um, ele não é o outro, então pode gerar algum tipo de relação positiva nesse caso aqui, ou negativa, aqui ele gera só relações positivas, então pele mista com pele seca 83%, pele normal com pele mista 93%, mas apenas por conta dos tipos de pele ao qual esse produto pode ser usado, então nenhum tipo de correlação muito forte foi percebida aqui dentro. Com isso a gente conclui o nosso processo de DDA e daqui para frente a gente vai fazer o treinamento do nosso algoritmo de T-SNE e fazer a visualização dos resultados com um plot animado por conta do perplexity, que a gente quer testar diversos valores de perplexity.

### Bloco 5 - Treinamento do Modelo

#### Preparação dos Dados para Modelo

Fala Dev, vamos continuar agora nosso projeto prático e dando agora a sequência na parte do treinamento do algoritmo T-SNE. Então primeiro vou colocar aqui uma célula Markdown pra treinar o algoritmo T-SNE. Então a primeira coisa que vou fazer é copiar o DataFrame original, então vou fazer uma cópia dele em x, x igual a devcosmetics.copy. Nesse caso, pra esse projeto eu vou dropar algumas colunas que vão ser usadas. Eu vou dropar no caso a coluna nome e vou dropar a coluna de ingredientes. Ah, por que vai dropar a coluna de ingredientes? Vocês podem fazer o experimento depois, eu fiz esse experimento com ingredientes e sem ingredientes. Como é um dado de alta, são 7 mil ingredientes, na hora que você coloca isso numa matriz do OneHotEncode e vai rodar o algoritmo, primeiro que o algoritmo demora um tanto pra performar por conta dessa alta cardinalidade, dessa alta dimensão, e na hora que ele vai gerar os resultados, como a dimensão é muito alta, o resultado em si na hora que você vai visualizar não fica tão interessante. Mas vale o experimento, vou fazer o experimento colocando ingredientes também. Basicamente você vai ter que fazer um OneHotEncode dos ingredientes, como foi feito no EDA, e juntar esse OneHotEncode, concatenar ele, na verdade criar como se fosse um DataFrame com um OneHotEncode e concatenar com o DataFrame original, junto os dois, e aí você consegue ter um DataFrame pra aplicar esse T-SNE. Nesse caso a gente não vai usar ingredientes, a gente vai usar o nome, porque o nome na verdade é quase que um ID, então a gente também não vai usar ele dentro do projeto. Então, criei aqui o X, vou aplicar alguns transformadores de colunas, então vou separar aqui variáveis numéricas e categóricas, então aqui as Numeric Features, eu vou usar Rating e o Preço, são escalas diferentes, então é importante colocar essas variáveis numa mesma escala, a gente vai usar a escala normal, Standard Scalar, e vamos colocar aqui as Features, a gente vai usar a Feature Marca e Tipo. Agora a gente vai definir as transformações que nós vamos fazer para cada um desses tipos, vamos definir as transformações, a gente vai usar um Numeric Transformer, que vai ser um Standard Scalar, e a gente vai usar um Categorical Transformer, que vai ser o OneHotEncoder. Aí a gente vai criar um PreProcessor, só para transformar os dados de X para uma variável com essas transformações, então criar um PreProcessor, um PreProcessador de transformação. Então a variável que eu vou chamar aqui PreProcessor, vai ser um ColumnTransformer, onde eu vou aplicar duas transformações, então Transformers, que é o atributo, vai ser uma lista, primeira transformação, vou chamar de Num, ela vai ser uma Numeric Transformer, que vai ser aplicada nas Numeric Features, depois uma outra transformação chamada Cat, onde eu vou usar o Categorical Transformer, aplicado no Categorical Features. E aí uma coisa importante, como estou fazendo um ColumnTransformer, e passando os transformadores nessas colunas, o que acontece, por padrão, é que ele vai ignorar as outras colunas que estão no meu DataFrame. DataFrame, lembrando, eu tirei a coluna nome e ingredientes, mas além da coluna nome, preço, marca e tipo, ele tem aquelas colunas que identificam para que tipo de pele que aquele cosmético serve. Então eu preciso de alguma forma, passar essa informação para o PreProcessor, de que algumas colunas, eu não vou fazer nada, mas eu quero que elas fiquem como estão e sejam colocadas num DataFrame novo, num conjunto novo de dados transformado. Então como é que eu faço isso? Eu uso um parâmetro chamado Remainder, ou seja, o que restou, eu quero que façam PassThrough. PassThrough significa, passa direto, passa sem fazer nada, mas eu quero que no final dessa transformação, essas colunas que estavam aí, elas continuem no DataFrame, ou no conjunto de saída. Então eu crio o PreProcessor, aí eu posso transformar os dados. Então eu vou chamar de XTransformat igual a PreProcessor.fit e transform. BX. Criou o conjunto de XTransformado, fazendo um hot encoding e fazendo também a transformação para Standard Scalar. Se a gente for dar uma olhada nos dados, se eu utilizar dados e colocar aqui XTransformate, ele diz que é uma matriz esparça por conta da questão das dimensões. Então veja que é uma matriz que tem 1472 linhas por 129 colunas, porque ele criou, além das colunas que já existiam, ele criou as colunas para cada uma das colunas. São 118 ou 119 marcas, mais colunas para cada um dos tipos de cosméticos e mais as colunas que já estavam na base. Então as duas colunas numéricas, mais as outras colunas que mostrava o tipo de pele a qual aquele cosmético servia. Então como é uma matriz esparça, ele já não consegue mostrar, visualizar, tem outras formas de visualizar essa matriz esparça dentro do notebook. Ele mostra aqui que é uma matriz esparça do tipo Float64, ele converteu todos os elementos para que eles possam ser trabalhados dentro do TSNE, que vai calcular só números, só vai fazer cálculos de distância entre os pontos de dados, considerando cada uma das dimensões. Então fizemos essa primeira parte de preparação dos dados, fizemos algumas coisas só levemente diferentes, essa questão do pass-through que tem que tomar cuidado quando você não transforma todas as colunas do seu DataFrame, mas você quer que as colunas que ficaram sem transformação, elas continuem para serem usadas depois, você tem que usar o remainder igual a pass-through. E não usamos para efeito desse projeto, o ingredientes, porque o ingrediente tem uma uma dimensão muito grande e isso trariam um componente, um custo computacional, vamos dizer, bem alto. Mas fica aqui o desafio, o convite de poder fazer também uma implementação, eu já fiz essa implementação, usando a variável ingredientes como uma dimensão, fazendo um one-hot-encode dela e colocando ela no algoritmo. Terminamos essa primeira parte, depois a gente vai fazer a segunda parte, que é de como rodar o algoritmo e armazenar os dados para que a gente depois possa visualizar. Vejo vocês no próximo vídeo.

#### Treinamento do Modelo

Agora a gente vai fazer o seguinte com base nos dados que a gente já transformou a gente vai criar aqui uma rotina que vai treinar esse modelo algumas vezes, armazenar esses resultados para que a gente possa visualizar esses resultados dentro de um plot. Então como que a gente vai fazer isso? Então a gente vai aqui. Primeiro a gente precisa de uma estrutura para armazenar os dados que a gente vai obter do algoritmo considerando que a gente vai mudar um parâmetro que é o parâmetro perplexity, que como foi explicado a quantidade de vizinhos que cada ponto de dados é considera. Na literatura eu pesquisei de que um número entre 5 e 50 é o número ideal para fazer esse tipo de validação, de avaliação do algoritmo onde você conseguir avaliar. E é o que a gente vai fazer aqui, a gente vai usar esse parâmetro e vai fazer ele percorrer de 5 a 50 até que a gente vai poder ver a evolução desse parâmetro de forma animada dentro de um plot. Então se a gente precisa de uma estrutura para armazenar, então a gente vai armazenar esses resultados dentro de um dataframe. Então a gente vai armazenar resultados do t-SNE em dataframe. Então a gente vai fazer criar um dataframe vazio e a gente vai começar a popular esse dataframe ao longo do loop que a gente vai fazer para cada perplexity. Então eu vou chamar de resultsdf e vai ser um pd dataframe. Estou declarando aqui uma estrutura. Agora eu vou fazer o loop. Então aqui vai ser um loop de treinamento do algoritmo mudando o parâmetro perplexity. Para fazer um loop a gente usa estrutura, tem várias estruturas, mas a gente vai usar o for. Vamos chamar uma variável aqui de perplexity para facilitar. E ela vai estar contida no range, ela vai assumir valores num range que vai de 5 a 50. Então a gente coloca o 51 que não é incluído, de 1 em 1. Então ele vai fazer 5, 6, 7 até o 50. Ele tira da lista, no comando range, o último item aqui que é o 51. Coloco dois pontos e tudo que vem dentro desse for é o que vai ser executado com esse conjunto de valores que essa variável vai assumir. Então a primeira coisa que a gente vai fazer é criar uma variável para instanciar o modelo. Então a gente vai chamar de T-SNE. Vamos instanciar o T-SNE e nós vamos fazer o nosso T-SNE com dois componentes. Você define da mesma forma que o PCA quantos componentes você vai ter. Se você tiver dois, na hora de visualizar isso, você visualiza isso num pano cartesiano de X e Y, que é o que a gente está mais acostumado a ver. Mais poderiam ser três componentes e você veria isso num modelo 3D, como se fosse um cubo, esses três componentes. Também a adaptação que precisaria ser feita aqui para dois ou três componentes é muito pequena e vou até fazer isso para mostrar como que seria depois de dois virar três componentes, como que esse chat seria apresentado. Então eu quero dois componentes. O parâmetro perplexity vai ser igual a perplexity, que é a variável que vai receber os valores aqui em cima no loop, no começo do loop. Tem um parâmetro que tem que ser inicializado, que ele é inicializado inicialmente com o valor chamado PCA, mas que aqui no nosso caso a gente vai inicializar como random, que é a inicialização do conjunto. Então a gente vai fazer uma inicialização randômica. A gente define também a quantidade de iterações que a gente vai fazer. A gente vai definir um range, para que ele não fique tentando achar um ponto ótimo por muito tempo. Então ele vai fazer 250 iterações para cada perplexity. A gente pode definir um random state também para efeitos de de reprodutibilidade desse experimento igual a 51. Então aqui eu criei o modelo. Criado o modelo, eu tenho que fazer um fit nesse modelo. Vou fazer um fit transform, porque na verdade já vou pegar os valores e jogar em algum lugar. Então eu vou jogar isso aqui nessa variável chamada tsneResults, que vai receber tsne.fitTransform da variável xTransformate, que é a variável que foi convertida para valores. Fiz o treinamento do modelo. Vou até colocar aqui. Criar e treinar modelo. Feito isso, eu preciso agora armazenar esses resultados em algum lugar. Então vem aqui, armazenar armazenar resultados. Então eu crio um DataFrame temporário dentro do loop, que vai receber o DataFrame, que vai receber o seguinte. Os resultados do tsneResults, e eu vou dar um nome para as colunas. As colunas vão se chamar componente1 e componente2. São os dois componentes que eu quero visualizar no meu chart, x e y. Além disso, vou adicionar nesse tempdf, que é esse DataFrame temporário, o valor de perplexity que eu usei. Perplexity, ou seja, eu vou ter que armazenar o conjunto que foi gerado do algoritmo, junto com a variável que identifique qual foi o parâmetro de perplexity que eu usei. Para que eu consiga depois fazer a animação no gráfico, considerando essa variável aqui. E aí o que eu faço? Eu digo que o resultdf, que é o DataFrame vazio que eu criei aqui, ele é resultado de um concat, eu vou concatenar ele, eu vou concatenar o próprio resultdf, que está vazio nesse primeiro momento, na primeira execução, com o tempdf. Mas uma coisa importante, quando você faz concatenação em pandas, PD concat, você tem que definir o eixo. Se você quer concatenar as colunas, ou seja, você vai juntar dois DataFrames pegando as colunas de um DataFrame e somando, ou concatenando, juntando com as colunas de outro DataFrame. Ou se você quer concatenar as linhas, ou seja, você vai pegar um DataFrame e você vai colocar as linhas abaixo das linhas do DataFrame original. Então, para definir o eixo, você vai definir através do Axis. No caso, a gente vai fazer o que? A gente está concatenando linhas e não colunas. Então, a gente usa Axis igual a zero. Se fossem colunas, a gente usaria Axis igual a 1. Armazenamos esses resultados, e aí podemos até rodar esse loop. Vou dar esse loop aqui. Está dizendo que eu não posso concatenar. Aqui eu coloquei uma aspas onde não devia, que é o nome da variável. Então, vamos lá. Ele vai rodar o mesmo modelo de t-SNE, considerando o parâmetro de perplexity de 5 a 50, fazendo 250 interações em cada um desses modelos, desses parâmetros, e vai armazenar tudo isso dentro dessa variável ResultsDF. Fez aqui 24.6 segundos. E o que a gente precisa fazer agora? Apenas para ajustar o DataFrame para que a gente possa visualizar os resultados. A gente vai dar um reset no índice para realizar o plot. Mas por que isso? Vamos visualizar o DataFrame. ResultsDF.head de 10. Então, veja que ele colocou aqui, componente 1, componente 2, perplexity. Quando eu dou um reset no índice, vou colocar aqui, ResultsDF.resetIndex. Usando o parâmetro drop igual a true, em place igual a true, e eu vir aqui e mostrar, ele droppou esse índice para que ele não faça parte na hora de eu fazer o plot dos resultados. Então, para isso que serve esse drop true aqui do ResetIndex. Terminado esse passo, a gente pode agora partir para a visualização dos resultados. E é o que nós vamos ver no próximo vídeo.

### Bloco 6 - Análise de Resultados

#### Visualização dos Resultados em 2D

Para concluir esse projeto prático, agora a gente vai visualizar os resultados de uma forma um pouco diferente, é uma forma que permite que vocês possam até revisitar outros módulos, outros EDAs que vocês tenham feito e poder aplicar essa técnica para apresentar algum tipo de informação para o seu cliente de uma forma diferenciada. Então a ideia aqui vai ser criar um scatterplot que vai combinar x e y, no caso componente 1 e componente 2, de forma que ele vá variando automaticamente de acordo com o parâmetro perplexity, já que a gente tem todos os componentes para cada um dos perplexities que a gente testou. Então a gente tem aqui o perplexity 5, 5, 5, 5, componente 1, 2, 1, 2 para cada uma das linhas do nosso DataFrame original, do nosso dataset de cosméticos. Então vou colocar aqui, criar um scatterplot animado com variação no perplexity. O que a gente vai fazer primeiro é criar uma figura, porque a gente vai mexer um ponto da figura, então figura igual a px.scatter, vamos usar o resultdf, o nosso x vai ser componente 1, nosso y vai ser componente 2, aí a gente vai ter um parâmetro chamado animationFrame, e aí eu digo que o meu animationFrame vai ser feito pela variável perplexity, que está no meu DataFrame, e é maiúscula para não dar problemas, perplexity. E aí eu vou colocar um título nesse chart, visualização do T-SNE com variação do perplexity. Fecho parênteses. Eu posso vir aqui e dar um fig.show. Ele mostrou o chart, então o que ele faz? Ele primeiro mostra um perplexity bem baixo, de 5. Veja que tem muitos grupinhos separados para cada um, que é aquela história da festa. Quando você está com um valor muito pequeno, você acaba não conhecendo tanto a festa, porque você fica sempre com pouca gente ao seu redor. Quando você tem um perplexity muito alto, acaba ficando todo mundo meio misturado. Você conheceu todo mundo, você tem uma visão ampla da festa, mas você acaba não conseguindo dar atenção para tanta gente, porque você tá falando com muita gente. Então como é que a gente faz agora para animar isso? Na verdade já está pronto para ser animado. Quando você clica no botão play, o que ele faz é, ele começa a pegar os dados que estão no DataFrame e fazer o plot rodar com esses dados. E percebam o fenômeno, quanto mais o perplexity vai aumentando, mais os dados ficam aglutinados, até o ponto que eles chegam a ser quase que uma reta, onde quase todos os componentes estão dentro de uma porção só dentro do plano cartesiano. Então essa é uma forma, não necessariamente animada, poderia ser uma forma de você testar cada um deles e analisar o resultado, de você conseguir visualizar de acordo com o seu gráfico. Nesse caso aqui a gente quer visualizar, podendo separar aqui, visualizar clusters, identificar clusters de cosméticos, de acordo com o parâmetro perplexity, para ir pensar, pensando que essa ferramenta é uma ferramenta de exploração de dados, de visualização com base em uma redução de dimensionalidade, para você depois tomar uma decisão melhor em cima disso. Mas esse é um gráfico bastante interessante, mostra como você poder visualizar uma mesma informação, usando hiperparâmetros diferentes. Então, com isso a gente conclui o nosso projeto prático, mostrando essa visualização um pouco diferente, com animação, para que você consiga ver o gráfico usando diversos fatores de perplexity, que é um hiperparâmetro bastante sensível dentro desse algoritmo do TSNE. E vejo vocês no próximo módulo.

#### Visualização dos Resultados em 3D

Fiz uma pegadinha com vocês, falei pra vocês que ia fazer a visualização em 3D, então vou fazer essa mudança aqui rápida no código, não tem muita alteração pra fazer, de como mostrar esse mesmo chat que está aqui em 2D em 3D. Como que a gente faria isso? A gente trocaria aqui pra componente, como é que a gente faz isso? A gente troca isso aqui pra 3 componentes, então a gente teria aqui 3 componentes, a gente teria que adicionar mais uma coluna no DataFrame, componente 3, uma coisa que eu vou fazer antes é zerar o DataFrame ResultsDef, aqui eu já estou colocando o componente 1, 2 e 3, então eu já posso rodar esse modelo naquele execute com 3 componentes, enquanto ele está rodando eu tenho que fazer uma outra alteração aqui, então eu tenho XYZ e aí eu tenho componente 3, eu havia feito esse teste com 3D e o que acontece é que o 3D não fica tão nítida a variação do perplexity, mas eu vou trazer aqui pra vocês como que funciona, então ao invés de usar o scatter, eu vou usar um outro método chamado scatter 3D, do mesmo, do Plotly, então ele terminou aqui, se eu vier aqui um head aqui, ele mostra aqui os 3 componentes do perplexity, rodo aqui o resetIndex e agora eu vou rodar o scatter 3D, scatter 3D com underline, ele mostra aqui, praticamente um cubo, componente 1, componente 2 e componente 3, quando a gente vai pro perplexity aqui, se eu fizer assim, pra tentar mostrar um pouco melhor, e dou um perplexity, veja só, então ou seja, ele não fica tão nítido mostrar os agrupamentos como fica em 2D, então pra esse caso de uso não fez muito sentido usar 3D, mas pode ser que pra algum outro caso de uso que você queira utilizar, seja pro próprio TSNE ou pra um PCA, se você quiser testar alguma outra forma, algum outro hiperparâmetro, algum outro gráfico, faça sentido, no final também ficou aqui como um bloco, mas eu particularmente achei que pro 3D pra esse caso não funcionou muito bem, no 2D você consegue ver uma informação um pouquinho melhor, então agora sim concluímos o módulo, mostrando como a gente adaptaria o código de fazer o treinamento com 2 componentes para 3 componentes e mostrando também uma visualização em 3D que existe dentro do Plot Express, que seria o scatter 3D, então vejo vocês no próximo módulo.
