# Módulo 16 - Análise de Componentes Principais (PCA)

> [voltar](../../../README.md) para a página anterior.

## Sumário

- [Módulo 16 - Análise de Componentes Principais (PCA)](#módulo-16---análise-de-componentes-principais-pca)
  - [Sumário](#sumário)
  - [Objetivo](#objetivo)
  - [Projeto Prático](#projeto-prático)
  - [Conceitos e Explicações](#conceitos-e-explicações)
    - [O Que é Redução De Dimensionalidade](#o-que-é-redução-de-dimensionalidade)
    - [Casos de Uso](#casos-de-uso)
    - [Um Passeio Pelos Algoritmos de Redução de Dimensionalidade](#um-passeio-pelos-algoritmos-de-redução-de-dimensionalidade)
    - [O Que é o Algoritmo PCA](#o-que-é-o-algoritmo-pca)
    - [SVD e PCA](#svd-e-pca)
    - [Métricas de Algoritmos de Redução de Dimensionalidade](#métricas-de-algoritmos-de-redução-de-dimensionalidade)
  - [Transcrição das Aulas](#transcrição-das-aulas)
    - [Bloco 1 - Introdução](#bloco-1---introdução)
      - [Aula 1 - Introdução](#aula-1---introdução)
      - [Aula 2 - O Que é Redução De Dimensionalidade](#aula-2---o-que-é-redução-de-dimensionalidade)
      - [Aula 3 - Casos de Uso](#aula-3---casos-de-uso)
      - [Aula 4 - Um Passeio Pelos Algoritmos de Redução de Dimensionalidade](#aula-4---um-passeio-pelos-algoritmos-de-redução-de-dimensionalidade)
    - [Bloco 2 - Conceitos PCA](#bloco-2---conceitos-pca)
      - [Aula 5 - O Que é o Algoritmo PCA](#aula-5---o-que-é-o-algoritmo-pca)
      - [Aula 6 - SVD e PCA](#aula-6---svd-e-pca)
      - [Aula 7 - Métricas de Algoritmos de Redução de Dimensionalidade](#aula-7---métricas-de-algoritmos-de-redução-de-dimensionalidade)
    - [Bloco 3 - Projeto Prático](#bloco-3---projeto-prático)
      - [Aula 8 - Apresentação do Projeto Prático](#aula-8---apresentação-do-projeto-prático)
    - [Bloco 4 - Análise Exploratória dos Dados](#bloco-4---análise-exploratória-dos-dados)
      - [Aula 9 - Carga dos Dados](#aula-9---carga-dos-dados)
      - [Aula 10 - Análise Univariada](#aula-10---análise-univariada)
      - [Aula 11 - Análise Bivariada](#aula-11---análise-bivariada)
    - [Bloco 5 - Treinamento do Módulo](#bloco-5---treinamento-do-módulo)
      - [Aula 12 - Treinamento do Algoritmo](#aula-12---treinamento-do-algoritmo)
    - [Bloco 6 - Análise dos Resultados](#bloco-6---análise-dos-resultados)
      - [Aula 13 - Visualização dos Resultados](#aula-13---visualização-dos-resultados)

## Objetivo

Explorar algoritmos não supervisionados de redução de dimensionalidade, começando com o PCA (Principal Component Analysis). Os objetivos são simplificar os dados mantendo as informações importantes, abordar o conceito de redução de dimensionalidade, explorar diferentes algoritmos, entender o PCA e sua relação com o SVD, além de discutir métricas aplicáveis. Ao fim, realizar um projeto prático de PCA, desde EDA até a visualização dos resultados.

## Projeto Prático

O projeto do PCA envolve uma empresa de fast food que quer abrir novas lojas globalmente. Foi utilizado um algoritmo de redução de dimensionalidade para representar países em um gráfico de 3 dimensões, facilitando a visualização de variáveis macroeconômicas. O projeto incluiu etapas como descarregar dados, explorar variáveis, treinar um modelo de IPCA, visualizar resultados e salvar o modelo. O algoritmo de redução de dimensionalidade é essencial para a visualização e tomada de decisões da diretoria.

## Conceitos e Explicações

> [Acesso aos slides](../pdf/ppts_m16.pdf) do conteúdo do módulo.

### O Que é Redução De Dimensionalidade

A redução da dimensionalidade é uma técnica matemática que simplifica dados complexos, reduzindo variáveis. Algoritmos de redução de dimensionalidade têm benefícios como simplificar a análise, eliminar ruídos, melhorar eficiência computacional, desempenho de algoritmos e visualização de dados. Reduzir dimensões pode revelar estruturas ocultas nos dados, facilitando a interpretação e extração de insights. Essa técnica é essencial em Machine Learning para lidar com a maldição da dimensionalidade e melhorar a eficácia dos algoritmos.

### Casos de Uso

A redução de dimensionalidade é amplamente utilizada em diversas áreas, como finanças, marketing digital, saúde, varejo e telecomunicações. Em finanças, ajuda na análise de risco e portfólio de ações. No marketing digital, auxilia na segmentação de clientes. Na saúde, é aplicada na genômica e pesquisa de doenças. No varejo, otimiza o inventário e prevê demanda. Nas telecomunicações, auxilia no gerenciamento de tráfego e planejamento de rede, identificando padrões e tendências para melhorar a qualidade de serviço.

### Um Passeio Pelos Algoritmos de Redução de Dimensionalidade

O resumo aborda os principais algoritmos de redução de dimensionalidade, divididos em classes como lineares, não lineares, autoencoders e Manifold Learning. Destacam-se algoritmos como PCA, Factor Analysis, LDA, T-SNE, LLE, Random Projection, MDS e UMAP, explicando suas aplicações e funcionamentos. A redução de dimensionalidade difere da seleção de features, pois envolve a transformação matemática das features originais em um espaço de dimensões menor. Cada algoritmo tem sua abordagem específica para preservar a estrutura dos dados.

### O Que é o Algoritmo PCA

O algoritmo PCA (Principal Component Analysis) é uma técnica estatística para reduzir a dimensionalidade dos dados, preservando sua variância. Antes de aplicar o PCA, é essencial normalizar os dados. A matriz de covariância é calculada a partir dos dados normalizados. Os autovetores representam as direções dos novos eixos com maior variância. Os componentes principais são selecionados com base nos autovetores com os maiores autovalores. Por fim, os dados originais são projetados nos autovetores selecionados, resultando em um novo espaço com menos dimensões.

### SVD e PCA

O SVD (Singular Value Decomposition) está intimamente ligado ao PCA (Principal Component Analysis), pois o SVD fornece uma abordagem computacional para realizar o PCA. O SVD ajuda a desmontar uma pilha de livros (dados) em três pilhas menores, mostrando direções, importância e relação dos padrões. O PCA encontra os melhores ângulos para capturar informações dos dados, sendo esses ângulos os componentes principais. Ao aplicar o SVD aos dados centrados, obtemos os mesmos componentes principais buscados pelo PCA.

### Métricas de Algoritmos de Redução de Dimensionalidade

As métricas para avaliar algoritmos de redução de dimensionalidade incluem erro de reconstrução, coeficiente de correlação de distâncias, taxa de compressão de dados e precisão na classificação. O erro de reconstrução compara os dados originais com os reconstruídos. O coeficiente de correlação de distâncias avalia a preservação das relações de distância. A taxa de compressão indica a eficiência na redução de dimensionalidade, mas não garante qualidade na reconstrução. A precisão na classificação é útil em problemas de aprendizado supervisionado para comparar dados originais e reduzidos.

## Transcrição das Aulas

### Bloco 1 - Introdução

#### Aula 1 - Introdução 

Fala Dev, vamos começar agora mais um módulo da nossa triga de inteligência artificial ainda nos algoritmos não supervisionados. Nós vamos começar agora uma nova classe de algoritmos não supervisionados que são os algoritmos de redução de dimensionalidade onde a gente vai começar agora a falar do PCA. O objetivo do módulo é apresentar conceitualmente os principais algoritmos de redução de dimensionalidade para que a gente possa simplificar os dados fazendo essa redução enquanto mantemos o máximo possível das informações importantes do nosso dataset. Faremos um projeto explorando o primeiro desses algoritmos que é o PCA ou Principal Component Analysis onde faremos o processo completo desde o EDA até a visualização dos resultados. Em termos de agenda vou mostrar o que a gente vai ver. A gente vai falar primeiro do que é redução de dimensionalidade. Fazer um passeio, uma geral de todos os algoritmos ou dos principais algoritmos de redução de dimensionalidade. Falar o que é o algoritmo PCA. Como que o PCA se conecta com outro algoritmo ou outro conceito chamado SVD. Quais são as métricas que a gente pode aplicar em algoritmo de redução de dimensionalidade. Depois falar do projeto de PCA que nós exploraremos esse algoritmo de forma prática.

#### Aula 2 - O Que é Redução De Dimensionalidade

O que é a redução da dimensionalidade? Os algoritmos de redução de dimensionalidade constituem uma classe de técnicas matemáticas que são aplicadas na análise de dados, cujo propósito fundamental é a simplificação dos dados ou do conjunto de dados ao reduzir o número de variáveis ou features envolvidas. Essa redução é especialmente valiosa nos contextos onde os dados apresentam uma alta complexidade dimensional, o que não apenas dificulta a análise visual e estatística, mas também aumenta o custo computacional e pode degradar o desempenho de algoritmos de aprendizado de máquina, devido ao fenômeno conhecido como maldição da dimensionalidade que a gente discutiu no módulo de fundamentos de Machine Learning. Falando na prática, esses algoritmos trabalham transformando um grande conjunto de variáveis e features num conjunto menor, preservando o tanto quanto possível as informações essenciais. Esse processo é realizado através da identificação de padrões, correlações e estruturas fundamentais nos dados originais. Citar aqui alguns principais objetivos e benefícios dos algoritmos de redução de dimensionalidade. Primeiro deles é a redução da complexidade. Dados de alta dimensão podem ser complexos de analisar e visualizar. A redução da dimensionalidade ajuda a simplificar esses dados para facilitar a interpretação e extração de insights iniciais sobre esse conjunto de dados. Outro aspecto importante é a eliminação do ruído. Quando a gente foca em componentes principais ou características principais do nosso dataset, a redução de dimensionalidade pode ajudar a eliminar o ruído, destacando apenas as características mais significativas dos dados. Eficiência computacional. Dados com menos dimensões requerem menos recursos computacionais para o processamento. Isso é crucial em aprendizado de máquina e análise de grandes volumes de dados, onde o tempo de processamento e a capacidade de memória podem ser limitantes. Continuando ainda com benefícios e objetivos, um deles também é a melhoria no desempenho de algoritmos. Muitos algoritmos de aprendizado de máquina acabam tendo seu desempenho prejudicado pela maldição da dimensionalidade, ou seja, eu tenho um dado muito esparso, um dataset muito esparso, o que é quando eu tenho um número de dimensões e isso leva a um espaçamento maior entre esses pontos de dados. Reduzindo a dimensionalidade, a gente pode melhorar a acurácia ou a métrica que a gente está imaginando melhorar e a eficácia desses algoritmos. Outro aspecto importante é a visualização de dados. É difícil visualizar dados com muitas dimensões diretamente. A redução para duas ou três dimensões permite o uso de gráficos bidimensionais, como a gente costuma usar aqui, um gráfico num plano cartesiano de XY, ou tridimensionais, como a gente vai ver nesse módulo, para explorar e comunicar os dados de forma efetiva. Um último aspecto a comentar por aqui é a descoberta de estruturas subjacentes. A redução de dimensionalidade pode revelar uma estrutura oculta nos dados, que não são aparentes em uma análise de alta dimensão. Isso pode incluir agrupamentos ou padrões correlacionados.

#### Aula 3 - Casos de Uso

Falando agora de alguns usos que a gente pode fazer dos algoritmos de redução de dimensionalidade aí trazendo para mercado em algumas aplicações. Então trazendo aqui para aplicações de finanças e investimentos, especificamente análise de risco e portfólio de ações, a redução de dimensionalidade é frequentemente usada para simplificar os dados de mercado e identificar os principais fatores que afetam os retornos de investimento. Por exemplo, o PCA que é a análise de componentes principais que a gente vai tratar com mais detalhes aqui nesse módulo, pode ser usado para identificar as principais variáveis que influenciam a movimentação de um conjunto diversificado de ativos financeiros, permitindo as análises de risco e gerentes de portfólio entender melhor as correlações e as volatilidades desses ativos. Falando agora de um outro segmento de marketing digital, podemos usar a redução de dimensionalidade para fazer segmentação de clientes, que é algo que a gente já trabalhou em outro módulo na parte de clasterização. Os algoritmos de redução como o PCA ou o T-SNE, que a gente vai ver mais para frente, eles podem ser aplicados para entender melhor as características dos clientes com base em grandes conjuntos de dados de comportamento online. Isso permite que as empresas segmentem seus clientes de forma mais eficaz, personalizando marketing para grupos específicos com base em seus hábitos e preferências de compra. Indo para o lado da saúde e bioinformática, essa redução de dimensionalidade é bastante usada na parte de genômica e pesquisa de doenças. A redução é usada para analisar dados genômicos de alta dimensionalidade, como por exemplo a expressão gênica ou sequências de DNA e RNA. Isso ajuda os pesquisadores a identificar padrões ou marcadores biológicos relevantes para doenças específicas, facilitando o desenvolvimento de tratamentos personalizados e a medicina de precisão. No âmbito de varejo e gestão de inventário, a redução de dimensionalidade, os algoritmos de redução de dimensionalidade podem ser usados para fazer otimização de inventário. O varejo pode ser usado para analisar os padrões de compra e comportamento do consumidor, ajudando na previsão de demanda e na otimização de estoque. Os algoritmos como o PCA podem reduzir a complexidade dos dados de transações, destacando os principais fatores que influenciam as decisões de compra dos consumidores e com isso conseguir organizar e otimizar os seus estoques baseado nessas preferências. Citando um último caso aqui em termos de mercado, no ramo das telecomunicações, na parte de gerenciamento de tráfego e planejamento de rede. Empresas de telecomunicações usam a redução de dimensionalidade para analisar e gerenciar grandes volumes de dados e tráfego de rede. Isso permite a identificação de padrões e tendências do uso da rede, ajudando no planejamento e otimização da infraestrutura de rede para melhorar a qualidade de serviço e a eficiência operacional.

#### Aula 4 - Um Passeio Pelos Algoritmos de Redução de Dimensionalidade

Vou tratar agora dos algoritmos de redução de dimensionalidade, fazer um passeio para os principais algoritmos. Aqui eu estou mostrando uma hierarquia, de certa forma uma estrutura de classes de algoritmos dentro dessa grande classe que é de redução de dimensionalidade. Então a gente tem aqui os algoritmos, ou a classe dos algoritmos lineares, que eles assumem que os dados podem ser representados em subespaços lineares, eles são particularmente ineficientes quando a relação entre as variáveis são linearmente correlacionadas. Então temos alguns exemplos aqui de algoritmos lineares, o PCA, o FA e o LDA, vou explicar alguns deles mais para frente. Temos aqui também uma outra classe de algoritmos chamada de não lineares, e aí os métodos que estão aqui dentro dessa classe de algoritmos são usados quando os dados estão dispostos de uma forma não linear, então a projeção linear não seria capaz de capturar a estrutura intrínseca desses dados. Aqui temos também outros exemplos, o TSNE, que também vamos falar mais para frente, o ISOMAP e o LLE. Temos uma classe de algoritmos que tem um algoritmo único, que é o Random Projection, que ele reduz a dimensionalidade por meio da projeção dos dados em um espaço de dimensões menores, usando transformações aleatórias, sendo eficientes e escaláveis para conjuntos de dados muito grandes. Outra classe de algoritmos muito vinculada à parte de redes neurais é a classe de autoencoders. Os autoencoders são redes neurais utilizadas para aprendizado não supervisionado de codificações para fazer codificações que a gente chama de eficientes. A ideia é aprender uma representação ou uma codificação de um conjunto de dados através do treinamento dessa rede, tipicamente para redução de dimensionalidade, treinando a rede para ignorar o ruído. Nós temos dentro dessa classe de encoders os algoritmos que a gente considera como os autoencoders lineares e os autoencoders não lineares. Por último, a última classe de algoritmos de redução de dimensionalidade, o Manifold Learning. O Manifold Learning é uma abordagem que visa descobrir a estrutura de variedade subjacente em um conjunto de dados de alta dimensão. E aqui temos dois tipos dentro dessa categoria, o MDS e o MAP. Falando agora dos principais algoritmos, o que a gente vai tratar, inclusive, nesse módulo que é o PCA, é o Principal Componente Análise. O que ele faz? Ele transforma os dados para um novo sistema de coordenados, reduzindo a dimensionalidade ao escolher os primeiros componentes principais que capturam a maior variança possível. Um aspecto importante quando a gente fala de redução de dimensionalidade é que a redução de dimensionalidade é diferente do que a gente já fez em alguns outros projetos, de você fazer seleção de features. Quando você faz uma seleção de features, você está mantendo a feature de uma certa forma como ela é, do seu conjunto original, você pode até aplicar uma transformação nela, mas você está mantendo a feature original e selecionando as features que fazem mais sentido para o seu dataset. Quando a gente está falando de redução de dimensionalidade, geralmente a gente está falando de eu pegar as features que eu tenho, vamos supor que eu tenho 10, 15 features, e a combinação delas, elas são transformadas em um conjunto de features menor. Então, na verdade, eu não faço uma referência direta de uma feature nova em um processo de redução de dimensionalidade para a feature original, há um processo matemático que faz com que esse conjunto de features seja projetado em um espaço diferente de dimensões, então eu não consigo fazer essa conexão direta. Então, é bom distinguir isso, o que é feature selection, que é seleção de features, daquilo que é uma redução de dimensionalidade, que na verdade eu estou reduzindo a quantidade de features, mas num outro espaço de dimensões, num outro espaço, num outro conjunto dentro dos meus dados. Um outro tipo de algoritmo que é importante tratar aqui, ainda na linha dos algoritmos lineares, é o Factor Analysis, ou FA, ele modela as variáveis observadas em termos de um número menor de variáveis não observadas, que são os fatores, assumindo que as variáveis observadas são combinações lineares dos fatores mais o ruído. Um terceiro tipo também bastante usado é o LDA, Linear Discriminant Analysis, ele busca eixos que maximizem a separação entre múltiplas classes, o objetivo é achar uma combinação linear das características que melhor separa duas ou mais classes de objetos, podemos fazer uma referência quando a gente fala dessa combinação linear de características que melhor separa duas ou mais classes de objetos, é como por exemplo um SVM, o Support Vector Machine, então se a gente voltar lá atrás nos algoritmos de classificação, o Support Vector Machine também tem essa característica da ideia de você ter separadores entre duas ou mais classes e aí você tem esses vetores de suporte para garantir que as classes sejam bem separadas, o LDA tem, salvas as devidas proporções, imaginando aqui que é um algoritmo não supervisionado, a ideia também é ter essa combinação que separa melhor essas duas classes. Temos um quarto algoritmo a ser tratado aqui, que também vai ser tratado de forma prática durante o nosso módulo, que é o T-SNE, o T-Distributed Stochastic Neighbor Embedding, ele reduz a dimensionalidade ao converter distâncias entre pontos em probabilidades condicionais, daí que vem o estocástico, e tentando minimizar a divergência entre as probabilidades no espaço de alta e baixa dimensão. Vamos falar de mais quatro algoritmos dentro daquelas classes que eu comentei, um deles é o LLE, Local Linear Embedding, que ele reduz a dimensão a preservar distâncias locais entre os pontos, ele assume que cada ponto e seus vizinhos mais próximos estão em um plano ou um subespaço linear local. O Random Projection, como comentei, ele projeta os dados em um espaço de menor dimensionalidade de forma aleatória, mas preserva as distâncias entre os pontos de acordo com a teoria de Johnson e Lindstroth. Falando agora de dois algoritmos na classe de Manifold Learning, nós temos o MDS, que é o Multidimensional Scaling, que ele visa colocar cada objeto em um espaço de baixa dimensão de forma que as distâncias entre os pontos sejam preservadas o máximo possível. E por último, um outro algoritmo que é bastante usado, o UMAP, Uniform Manifold Approximation and Projection, ele é similar ao TSNE em seus objetivos, mas baseia-se em uma abordagem matemática diferente, e muitas vezes é mais rápido e melhor para preservar a estrutura global dos dados.

### Bloco 2 - Conceitos PCA

#### Aula 5 - O Que é o Algoritmo PCA

Vou começar a falar agora do que é o algoritmo PCA que é o algoritmo foco desse módulo. O algoritmo PCA, ou principal componente análise, é uma técnica estatística utilizada para reduzir a dimensionalidade dos dados enquanto preserva o máximo possível da sua variância. E aí eu vou explicar um pouco quais são os passos que esse algoritmo segue e aqui tem uma visualização gráfica de como esse algoritmo funciona. Ele faz dois cortes aqui nos dados e captura aqui nesses cortes qual que obtém a maior variância. Então a primeira coisa que o algoritmo precisa fazer, antes de aplicar o PCA, é normal normalizar os dados para que as características tenham média zero e variância unitária. Isso é importante para que as variáveis com maior magnitude dominem, para evitar que as variáveis com maior magnitude dominem o resultado do PCA. Então a ideia é que estando todos os valores dentro da escala padrão, a gente não vai ter o efeito da magnitude dos dados. Segunda coisa que ele faz com base nesses dados normalizados é calcular a matriz de covariança. Essa matriz de covariança é calculada a partir dos dados normalizados e essa matriz ajuda a entender como as variáveis nos dados estão variando juntas. Como terceiro passo é feito o cálculo, vamos dizer assim, daquilo que é o principal no método do PCA, que é o cálculo dos autos valores e os autovetores. Os autovalores e autovetores são calculados, os autovetores representam as direções dos novos eixos no espaço de dados original, onde os dados têm a maior variância. Os autovalores associados a cada autovetor indicam a quantidade de variância que cada um dos novos eixos captura. Então quando eu faço por exemplo, defino um algoritmo PCA para rodar, eu defino a quantidade de componentes principais que eu quero. Para cada um desses componentes eu gero um autovetor. Para cada um desses autovetores eu tenho os autos valores associados que indicam a quantidade de variância que cada um desses novos eixos captura. Então se eu tenho três componentes principais, ele vai gerar três autovetores e vai gerar um conjunto de autovalores com a quantidade de variância de cada um desses eixos. Então aqui dá um exemplo, mostrando aqui o exemplo de um dado tridimensional, eu tenho um espaço tridimensional, onde depois ele tenta buscar os dois principais componentes para representar isso dentro de um espaço bidimensional. Então o que ele busca aqui é a maior variância, ele busca a maior variância dentro de PC1, que seria o principal componente 1, e aí ele traz aqui esses dados que estão aqui plotados dentro do espaço tridimensional para serem plotados dentro do espaço bidimensional. Feito esse cálculo de autovalores e vetores é feita a seleção dos componentes principais. Os autovetores são ordenados por seus autovalores correspondentes em ordem decrescente, ou seja, do maior para o menor. Os primeiros autovetores, aqueles com os maiores autovalores, são selecionados como os componentes principais, porque eles armazenam, eles estão lá indicando uma quantidade de variância maior. O número de componentes principais selecionados depende do quanto de variância queremos ter nos dados reduzidos. O último passo feito do processo do algoritmo do IPCA é a transformação dos dados. Finalmente os dados originais são projetados nos autovetores selecionados. Então se eu tenho 17 características e eu definir que eu vou ter três componentes principais, esses 17 itens são projetados em apenas três variáveis. Eles transformam os dados do espaço original para um novo espaço com menos dimensões, que são os componentes principais, onde os dados ainda retém a maior parte da sua informação original, mas com menos ruído e redundância. Aqui mostrando de forma gráfica, como costumo dizer na nutshell, mas em uma visão geral, o que é o IPCA de forma gráfica. Então imagina que eu tenho um gráfico bidimensional, que eu tenho aqui altura e peso, altura em polegadas e aqui pode ser altura em centímetros. Ele centraliza os pontos porque ele quer obter aqui a dimensão que tem a maior variância. Com base nessa centralização de pontos dos eixos, ele computa a matriz de covariança dentro desses pontos e dela ele extrai o que no inglês é chamado de engine vectors e engine values, ou seja, autovetores e autovalores. Calculados esses autovetores e autovalores, ele pega, ele ordena os engine vectors com os maiores engine values. E aí ele projeta os dados originais dentro desses engine vectors e traz aqui para dentro de um conjunto de dados de baixa dimensão. Então sai de uma alta dimensão, aqui não é tão alta que são duas dimensões, e sai para um conjunto de dados de baixa dimensão. Então aqui ele transformou um conjunto de dados bidimensional, duas variáveis, num conjunto de dados unidimensional. Ainda de forma gráfica, mostrar aqui, por exemplo, um algoritmo trabalhando, mas para mostrar onde estão esses valores. Então se eu tenho aqui uma variável, variável B, e tem os pontos projetados dentro dessas duas variáveis num plano cartesiano. O que são os engine vectors considerando esse gráfico? São esses eixos que são obtidos dentro desse plano cartesiano. Então esses aqui são os engine vectors, e dentro de cada um desses eixos eu tenho o que são chamados os engine values. Então eu tenho os autovetores e os autovalores. Então dentro desse eixo eu tenho essa cobertura de dados. Dentro desse outro eixo eu tenho essa cobertura de dados. Então eu projeto, eu calculo os engine values baseado nos eixos que são os engine vectors para ver qual deles possui a maior variância. Legendas pela comunidade Amara.org

#### Aula 6 - SVD e PCA

Falo agora de um conceito que está muito relacionado ao PCA, que é o SVD, Singular Value Decomposition. Eles estão relacionados porque o SVD na verdade fornece uma maneira computacional de se realizar o PCA. Usar o SVD para fazer o PCA é como usar uma ferramenta sofisticada para automaticamente encontrar os melhores ângulos para capturar as características mais informativas dos dados. E aqui eu trouxe uma forma lúdica de como a gente representar o SVD. Então imagina, primeiro passo, o SVD desmontando uma matriz. Então imagina que a gente tem uma pira de livros que representam diversos aspectos dos dados, como um livro para cada variável. Então eu tenho um livro para representar a idade, um livro para representar peso, um livro para representar altura. A SVD ajuda a desmontar essa pilha de livros em três pilhas menores. Então uma pilha mostra os tipos de informações, ou seja, as direções dos dados como padrões ou tendências. Uma segunda pilha diz quão importante é cada tipo de informação, que são os valores que mostram a força ou impacto de cada um dos padrões obtidos da primeira pilha. A terceira pilha mostra como esses padrões estão relacionados aos livros originais. Segundo passo, encontrando o melhor ângulo. Agora imagine que você está tentando fotografar sua filha original de livros, mas você quer que a foto capture o máximo de informações possíveis com o mínimo de fotos possíveis. Então o PCA ajuda a encontrar o melhor ângulo para tirar essas fotos. Esses melhores ângulos são basicamente os componentes principais que capturam a maior parte da informação sobre como os livros variam uns com os outros. Ainda falando da relação entre SVD e PCA, quando você aplica a SVD aos seus dados, pensando na pilha de livros, uma das pilhas menores que você obtém mostra os melhores ângulos para olhar seus dados. Esses são os componentes principais que o PCA busca. Então, usando a SVD nos dados, você pode encontrar diretamente esses componentes principais. Ainda falando da relação entre SVD e PCA, quando você fala de dados centrados, para usar o PCA você geralmente subtrai a média de cada variável para garantir que os seus dados estejam centrados ao redor do zero. Isso é como garantir que a sua câmera esteja focada corretamente antes de tirar a foto. Aplicando a SVD para obter o PCA, depois de você ter centrado seus dados, você aplica a SVD. A matriz que você obtém, que é a pilha que mostra como os padrões estão relacionados aos livros originais, contém os mesmos componentes principais que o PCA busca. Capturando informações, as fotos que são os componentes principais tiradas pelo PCA são aquelas que retém a maior quantidade de informações variadas dos dados originais, o que nos ajuda a entender melhor os dados, com menos fotos ou com menos componentes.

#### Aula 7 - Métricas de Algoritmos de Redução de Dimensionalidade

Vou falar agora de quais métricas a gente pode usar para medir ou para avaliar algoritmos de redução de dimensionalidade. O algoritmo de redução de dimensionalidade por natureza é um algoritmo bastante usado para você fazer exploração de dados, de padrões, ou para você poder reduzir o seu dataset para poder aplicar alguma outra técnica de machine learning, eventualmente até uma técnica de machine learning de aprendizado supervisionado. Então, as métricas que eu vou comentar aqui são métricas que não necessariamente são para avaliar que a redução está sendo feita da melhor maneira possível, mas para dar uma referência entre o dado original e o dado reduzido pelo algoritmo. Então, o primeiro, a primeira classe de métricas que a gente pode obter aqui, ou trabalhar para essa parte de algoritmos de redução de dimensionalidade, é o chamado erro de reconstrução dos dados. Como que é essa métrica? Ela é a diferença entre os dados originais e os dados reconstruídos, que são obtidos aplicando a transformação inversa do algoritmo de redução de dimensionalidade. Então, quando eu aplico um algoritmo de PCA, eu saio, por exemplo, de um espaço de 15 dimensões e posso ir para um espaço de 2 dimensões. Os algoritmos permitem que você saia do 2, de 2 dimensões, e volte para o 15, só que não volta exatamente completo. Então a ideia é o que? Você comparar o quanto que os dados originais e os dados reconstruídos têm de diferença, ou seja, qual o erro dessa reconstrução. E aí a gente pode validar essa reconstrução com métricas que a gente já conhece, por exemplo, lá na regressão, o MSE, o Mean Square Error, o Root Mean Square Error, ou o Mean Absolute Error. Outra forma também de avaliar algoritmos de redução de dimensionalidade é através do coeficiente de correlação de distâncias, que é bastante usado em algoritmos como, por exemplo, o PSNE e o MDS. Eles visam preservar as relações de distância entre os pontos. Então a métrica útil aqui é avaliar o coeficiente de correlação entre as distâncias no espaço original, ou seja, no seu dataset original, e no espaço de dimensionalidade reduzida. Um coeficiente próximo de 1 indica que as distâncias relativas entre os pontos estão bem preservadas. Uma outra forma de poder medir um algoritmo de redução de dimensionalidade, especialmente para alguns tipos de aplicações, é a taxa de compressão de dados, que ela é a razão entre o tamanho dos dados originais e o tamanho dos dados reduzidos. Quanto maior a taxa de compressão dos dados, mais eficiente é o algoritmo na redução de dimensionalidade dos dados. No entanto, uma alta taxa de compressão de dados não significa necessariamente uma alta qualidade de reconstrução dos dados. Então o ideal é que a gente possa equilibrar essa métrica de taxa de compressão de dados com a taxa de reconstrução de dados quando a gente for escolher um algoritmo de redução de dimensionalidade. Por último, é a precisão na classificação dos dados em problemas de aprendizado supervisionado, como eu comentei um pouco anteriormente. Então eu posso, por exemplo, usar uma precisão como uma medida pela proporção de rótulos corretamente previstos. Então se eu tiver, por exemplo, uma regressão logística ou um KNN, que são algoritmos de classificação, eu posso usá-los para esse teste, ou seja, eu aplico esses algoritmos de forma a olhar as características originais dos dados e comparo esses mesmos algoritmos com os dados reduzidos e vejo se há uma grande diferença em relação às métricas que eu estou definindo. A precision pode ser uma dessas métricas, mas de acordo com a necessidade que você tem do problema, você pode escolher outras métricas. Então você acaba usando um aprendizado supervisionado para validar o resultado de um aprendizado não supervisionado.

### Bloco 3 - Projeto Prático

#### Aula 8 - Apresentação do Projeto Prático

Falando agora do nosso projeto do PCA. O projeto aqui é de uma empresa de fast food que deseja abrir novas lojas ao redor do mundo e precisa apresentar de uma forma simples como que os países estão organizados em termos de variáveis ou indicadores macroeconômicos como inflação, arrecadação, expectativa de vida entre outros. Dessa forma, para que seja possível representar esses países num gráfico de apenas 3 dimensões, iremos construir um algoritmo de redução de dimensionalidade que reduza a quantidade de variáveis, permitindo assim a visualização necessária para suportar a tomada de decisão para a diretoria. Qual que vai ser a estrutura do projeto? Como fundamentalmente o algoritmo de redução de dimensionalidade ajuda muito a nossa parte de visualização, esse projeto vai ter uma estrutura mais simples. Nós vamos descarregar os dados, fazer um EDA de algumas variáveis que existem hoje nesse dataset, vamos treinar um modelo de IPCA, visualizar os resultados e métricas e vamos salvar esse modelo. Então agora é hora de ir para o código.

### Bloco 4 - Análise Exploratória dos Dados

#### Aula 9 - Carga dos Dados

Fala Devs! Vamos começar agora a parte prática do nosso módulo de PCA, o primeiro dos algoritmos que a gente vai tratar de redução de dimensionalidade. Eu já estou aqui com o meu Visual Code pronto, já fiz a instalação das bibliotecas, vou começar aqui criando um arquivo pra gente começar o nosso projeto. Eu vou chamar esse arquivo de PCA que é a análise que eu vou fazer dos países, com base na solicitação daquela empresa de fast food que eu comentei. Já tenho selecionado aqui qual é o virtual environment que eu vou trabalhar, redução de dimensionalidade PCA, que é a minha pasta, e vou mostrar quais são as bibliotecas que precisam ser importadas para esse módulo a princípio. Então vou colocar aqui uma colinha das bibliotecas, tirando aqui o Streamlit que a gente vai precisar. Então vou usar o Pandas, o PyPlot, Skykit Learning, o Matplotlib, o WiPyWidgets, o WiPyKernel e o Seaborn. Alguns gráficos, vou usar o Seaborn. Vou começar importando as bibliotecas, então as bibliotecas para IDEAR e visualização de dados. Então vou importar o Pandas, vou importar o Plotly, o Express, vou importar o Seaborn, vou importar o Matplotlib, ele às vezes é usado por baixo dos Pandas, então sempre posso importar ele caso eu precise. Em termos da parte Machine Learning, vou importar o módulo do PCA, que faz parte do Skykit Learning, sobre o pacote Decomposition, então vou importar aqui o PCA, vou importar do pacote de métricas, eu vou importar o MinSquaredError, vou importar na parte de pré-processamento das variáveis que vão ser necessárias, o pré-processing, vou importar aqui o StandardScaler, e vou importar o ColumnTransformer, para fazer a transformação das variáveis numéricas, aplicando o StandardScaler, então o ColumnTransformer. Vou importar o NumPy, caso eu precise mostrar aqui alguma coisa em termos de covariança de algumas matrizes, então vou começar importando aqui, import from matplotlib, importamos as bibliotecas, primeira coisa depois de importar as bibliotecas, é a gente carregar os dados, então vou colocar aqui Markdown, carregar os dados, e aqui eu já vou trocar por uma célula de código, então vou carregar o DataFrame, vou chamar de df-counters, pd read csv, então vou importar o Dataset, que está aqui na minha pasta, vou abrir aqui, o Dataset chama-se CountryData.csv, já vou explicar ele para você, aí tem uma jogadinha aqui nesse CSV, primeiro que o separador dele não é vírgula, é ponto e vírgula, e o separador decimal é vírgula, que é o padrão brasileiro, então isso tem que ser ajustado na hora que a gente vai fazer a importação dentro do pandas, então eu coloco aqui o sep, que é o separador, ele é o ponto e vírgula, e o decimal é igual a vírgula, se você não fizer isso, ele vai considerar o padrão, que o separador é vírgula e o decimal é ponto, e aí como não é dessa forma, ele vai quebrar de forma errada os dados, carregamos aqui os dados, e agora vamos analisar a estrutura desses dados, então analisar a estrutura, df-counters.info, então temos aqui essas colunas, temos aqui a coluna Country, Child Mort, Export, Health, Import, Income, Inflation, Life Expect, Total Fare, GDPP, Income Category, Country sendo objeto, Income Category sendo objeto, e todas as outras variáveis, variáveis quantitativas, vou explicar aqui o que são essas variáveis, como está composto hoje esse banco de dados de países que a gente vai usar, aqui no dicionário, então a primeira coisa é ter uma variável que é o nome do país, uma variável única, eu tenho um registro, uma linha para cada país, eu tenho uma informação de qual o nível de mortalidade infantil a cada mil nascimentos, eu tenho aqui qual o nível de exportação que é expresso através de um percentual com base no PIB, qual o investimento em saúde também é um percentual com base no PIB, e o nível de importação, um percentual com base no PIB, aí eu tenho uma outra informação que é a renda média por habitante, a inflação, qual o nível de inflação, Life Expect que é a expectativa de vida em anos, Total Fare que é o total de crianças, é uma ideia de fertilidade, o total de crianças que poderiam ter nascido para cada mulher, se o nível de fertilidade se mantiver o mesmo ao longo do tempo, e o GDPP, que na verdade o GDP é o PIB, o que a gente conhece aqui no Brasil como PIB, e o GDPP na verdade é o PIB per capita, ou seja, é o PIB do país dividido pela população. E aqui uma outra coluna que é o que a gente chama de Income Category, ou seja, baseado na renda média por habitante, é feito uma categorização do país em quatro níveis, esse banco de dados ele veio de uma base chamada World Bank, que tem uma série de estatísticas sobre os países, se fosse juntar todas as estatísticas de vários conjuntos de dados, seria um conjunto de dados bem grande, talvez até 70 ou 80 variáveis, eu trouxe aqui um recorte desses dados de países para a gente fazer uma redução em cima ainda desse conjunto. Então vemos aqui que temos 165 entradas, 165 países constando nesse estudo, vamos dar uma visualizada nos primeiros registros, visualizar os primeiros registros, então deathcounters.head10, então mostra aqui, Afeganistão, as taxas que eu comentei, taxa de inflação, mortalidade infantil, nível de exportação, nível de importação, e aqui as categorias que eu comentei, Low Income, Upper Middle Income, Lower Middle Income e High Income, são quatro categorias associadas, são quatro categorias que são as categorias em base à renda daquele país. Vamos olhar os últimos registros, visualizar os últimos registros, então deathcounters.tail10, então podemos ver aqui no final também nenhuma novidade, a gente tem aqui até a Ucrânia, Reino Unido, até a Zâmbia, também com todas as informações aqui preenchidas. Então, essa primeira parte a gente só fez a carga dos dados, contextualizei para vocês as bibliotecas que a gente vai usar, a partir daqui para frente a gente vai começar a fazer o EDA e depois fazer ou trabalhar com o PCA propriamente dito. Então vejo vocês no próximo vídeo.

#### Aula 10 - Análise Univariada

Vamos começar agora o nosso EDA, nossa Análise Exploratória de Dados, pra gente entender melhor como está composto esse conjunto de dados e países, entender algumas correlações que existem entre essas variáveis, e depois partir para o PCA propriamente dito. Então vou colocar aqui uma marcação, aqui no Markdown, EDA. Vou fazer uma segunda marcação de análise univariada. Vou fazer análise de variáveis de forma independente, de forma individual, e depois fazer essa análise combinando algumas variáveis. Então a primeira coisa aqui é extrair as estatísticas das variáveis. Estatísticas das variáveis. DeathCountries.Describe. Describe, lembrando, ele mostra as principais estatísticas de cada uma das informações que tem no conjunto de dados, variáveis dessas numéricas. Então temos aqui contagem de dados, a média, o desvio padrão, o valor mínimo, o primeiro quartil, o segundo quartil que é a mediana, o terceiro quartil e o valor máximo. Para cada uma dessas variáveis que compõem este conjunto de dados. A primeira coisa que eu gostaria de explorar aqui é que como a gente tem uma variável categórica, é avaliar no mundo, ou baseado nesses 165 países que nós temos aqui nessa base, como está a distribuição da questão de categoria de renda, com base na renda que é aquela variável categórica. Então, sabia senhora, existe muito mais países. A minha impressão é, bom, deve ter muito poucos países que são considerados como alta renda. Como a gente faz uma aproximação disso que acontece no Brasil, por exemplo, a maioria da população está entre uma sub-renda, uma classe média, uma classe média alta, mas uma pequena mesmo parcela da população está lá, que são considerados os ricos, os milionários, pessoas que têm uma alta renda. Então, será que no mundo isso acontece também? Então a gente vai ter uma pouca fração de países desse conjunto que são considerados como alta renda ou a gente vai encontrar algum outro padrão. Então, vamos olhar isso através de um gráfico de barras. Então, distribuição da variável Income Category. Eu quero avaliar qual o percentual que eu tenho de países para cada uma das categorias, as quatro categorias. Então eu vou colocar aqui uma variável percentual Income Category e ela é calculada como dfcounts.valuecounts, a variável de interesse Income Category e eu divido isso pelo tamanho do meu data set e multiplico por 100 para expressar isso já em percentual. Eu vou gerar um gráfico de barras no Plotly, vou pegar o dado percentual e vou colocar como cor o índice desse data set para cada categoria ficar em uma cor diferente, facilitar a visualização. Mostrei aqui o gráfico e a gente percebe o seguinte, uma coisa diferente. Na verdade, por essa tabela, a grande maioria dos países na verdade é considerada como High Income, ou seja, que tem uma renda média do habitante alta, 32%. Você tem 32%, aí você tem Lower Middle Income, que seria a segunda categoria, de baixo para cima, com 28%, quase 29%. O Upper Middle Income, aqui seria classe rica, aqui seria a classe média, aqui a classe baixa e aqui já quase na linha da pobreza, que seria o Low Income com 12%. Então, na verdade, desse conjunto de dados, a grande maioria dos países, na verdade, são países considerados como High Income, ou seja, que tem uma alta renda da sua população. Quero avaliar, em cima dessa informação, como que é a distribuição da variável Income, ou seja, se a maioria dos países está dentro dessa faixa, como que a variável numérica está se comportando. Então, vou olhar aqui a distribuição da variável Income. E aí eu vou fazer um histograma, dentro do próprio Plotly, pegando a variável Income. Vou colocar um título aqui, Histograma da variável Income, que é a renda, renda média. E aqui ele mostra o seguinte, uma cauda, uma cauda à direita, tem aqui, ela é uma, há uma simetria, grande parte dos valores aqui concentrados, próximo de até 10 mil, outra parte em 20 mil, e aí alguns Outliers, ou alguns países aqui com renda na faixa de 120, na faixa de 120 a 130 mil. E aqui pensando na escala aqui de dólares. Então, ou seja, tem uma grande concentração com rendas que vão até 10 mil, mas a forma como isso é classificado, por mais que essa renda seja baixa, eu tenho a maioria dos países como sendo países com High Income. Então, pode ser que o High Income, a gente pode descobrir isso, ele não seja dado apenas pela questão do Income, as outras variáveis que estão no Dataset, elas também podem influenciar nisso. Vamos ver aqui a distribuição agora da parte do PIB, do PIB Per Capita. Então, como que é o PIB Per Capita, que é uma outra variável importante, uma variável macroeconômica do país. Então, distribuição da variável GDBP, que é o PIB Per Capita. E aí vamos fazer a mesma coisa do que a gente fez aqui. Para facilitar, a gente pode colocar aqui %GDPP, aqui a minha variável de interesse é GDPP, copio, para cá e para cá. E aí eu tenho uma distribuição, opa, GDPP... Ah não, aqui eu não preciso doar, eu coloquei a categorização, mas na verdade é o histograma. Vou categorizar aqui, fiz errado. Aqui é histograma, aqui é o histograma, então é PX.Histograma da variável DF Countries GDPP. E aí o título é Histograma da variável GDPP. E aqui também a mesma coisa, eu tenho grande parte dos valores concentrados aqui na parte inicial do gráfico. Um comportamento bastante parecido com a variável Income, mesmo comportamento. Uma outra variável, duas variáveis que eu quero avaliar ainda em relação à distribuição, é a variável Inflação. Então eu quero ver a distribuição da variável Inflação, para ver se ela segue algum comportamento próximo dessas outras duas. Então aqui eu posso usar também o histograma. Então aqui eu vou usar o Inflation, histograma da variável Inflation. E aí ele mostra um comportamento um pouco diferente. Eu tenho aqui, não a inflação mais baixa no início, mas eu tenho uma grande concentração de inflação nessa faixa aqui. Depois seguido de, aqui na verdade é de menos 5 a menos 1, como está nos dados. De 0, que seria a deflação. Aqui de 0 a 5%, aqui de 5 a 10%, onde concentra a maior parte dos dados. Eu tenho aqui algumas com uma inflação bem alta, uns outliers com inflações bem altas. Provavelmente de países de baixa renda ou países subdesenvolvidos. E uma outra variável que eu gostaria de estourar aqui é a questão da expectativa de vida. Quanto que a expectativa de vida também influencia na questão da categoria ou como ela se relaciona com as outras variáveis. A gente vai fazer isso na análise bivariada, mas eu gostaria de analisar ela de forma independente. Então aqui o Life Expectation. Então eu vou colocar aqui Life Expectation. E aí o que eu mostro já é um gráfico ao contrário, na verdade uma assimetria. A esquerda então tem um país que tem uma expectativa de vida bem baixa, de 30 a 35 anos. Uma pequena parte ainda de 45 a 55 e a grande maioria aqui concentrada nessa parte entre 70 e 80 anos de idade. Então mostra que apesar da renda, da grande concentração de renda e de TPPC baixa, ainda assim a grande parte dos países tem uma expectativa de vida bastante alta. Então fecho aqui essa parte do EDA, especificamente a parte de análise univariada, onde eu quis avaliar algumas variáveis de forma independente. Vejo vocês no próximo vídeo.

#### Aula 11 - Análise Bivariada

Dando continuidade a nosso EDA, agora a gente vai fazer algumas análises bivariadas, combinando algumas variáveis e avaliando a correlação entre elas. Então vou marcar aqui no notebook. Markdown. Análise bivariada. A primeira análise que eu gostaria de fazer é, com base na categoria, avaliar o Income, para ver se há uma correlação entre quanto maior o valor médio da renda da população, mais enquadrado o nível alto vai estar aquele país. A gente consegue fazer isso através de um gráfico de Box Plot. Então vou fazer um plot de distribuição. E nós vamos usar aqui um Box Plot. Por Income. Income Category. E aí como nós vamos fazer esse gráfico? A gente vai fazê-lo invertido, ao invés de ele ser na vertical, na horizontal. PxBox, que é o Box Plot. Olha o nome do DataFrame. A variável x, na verdade ela vai virar a y, porque a gente vai inverter os eixos. É a variável Income. A variável y é a variável Income Category, que também vai ficar invertida. A cor vai ser dada pelo Income Category. A orientação desse gráfico vai ser horizontal e não a vertical, que é o padrão. E aí tem uma coisa interessante aqui, que é um parâmetro chamado HoverData. Onde você pode colocar aqui que informação que você quer ver no plotly, além das informações padrões que aparecem naquele gráfico. Então quando a gente quer, por exemplo, detectar outliers, se a gente tem outliers no Box Plot, se você posicionar em cima do outlier, sem o HoverData, você não sabe quem é aquele outlier. Ele vai te mostrar as estatísticas, mas você não sabe quem é. Quando você coloca o HoverData, ele vai mostrar exatamente qual é o ponto de dados, no caso aqui eu estou trazendo a variável Count, que é o nome do país, que representa aquele outlier dentro do Box Plot. Então isso é bastante interessante. Então eu girei o gráfico, e a gente consegue ver aqui o seguinte. Ele não está ordenado, de certa forma, pela categoria, então o correto seria Low Income, Lower Middle Income, Upper Middle Income e High Income. Mas aqui fica fácil de perceber que as médias de Income dos países que são Low Income, são as menores, seguido do Lower Middle Income, seguido do Upper Middle Income e seguido depois do High Income, que tem uma amplitude bem grande com uma série de outliers. A gente consegue ver quem são esses outliers. Eu consigo ver que, por exemplo, o Catar tem um Income de 125 mil, é uma renda média. Eu tenho o Luxemburgo com quase 92 mil dólares. Eu tenho o Brunei com 80 mil dólares, quase 81 mil dólares. Aí do lado dos países com Low Income, eu tenho o Sudão e o Iêmen como sendo outliers. Tenho aqui no Upper Middle Income, eu tenho a Líbia e a Guiné Equatorial. E no Lower Middle Income eu tenho a Algéria, o Líbano e o Irã. Então é interessante conseguir perceber, além das métricas, mas também tem essa visualização de outliers nomeados, o que permite ter uma informação mais completa para a sua tomada de decisão. Da mesma forma que eu fiz esse Box Plot, eu quero explorar também a questão do GDPP. Da mesma forma que eu fiz na análise univariada, eu vou trazer isso também para a análise bivariada. Então, ao invés de eu trabalhar com Income, Income Category, eu vou trabalhar com o GDPP. Eu quero ver se segue o mesmo comportamento do Income. Então eu vou colocar aqui GDPP, só trocar a variável e vou mostrar. E ele segue basicamente o mesmo padrão, Low Income com o menor, seguido do Lower Middle Income, Upper Middle Income e depois High Income. E aí de novo a gente encontra aqui os outliers. No caso aqui o outlier do High Income é Luxemburgo, que aparece lá como um dos outliers, não o maior. O Upper Middle Income de novo aparece Guiné Equatorial. Aqui aparece de novo o Líbano e o Irã. E aqui no Low Income o Sudão, o Iêmen e o Chad. Como três outliers. Então segue a mesma lógica, séries estranhas. E aí seria interessante se a gente visse algo diferente. Poderia ser alguma coisa da base ou ele leva em consideração alguma outra coisa. Mas está acompanhando a lógica de que países categorizados como High Income vão ter maior GDPP e vão ter uma maior variável de renda média da população. E vou fazer uma avaliação agora de mais duas variáveis. Também seguindo o conceito da análise univariada, que seria a parte de inflação. Então analisar se a inflação muda muito de acordo com a categoria de Income. Então vou colocar aqui Inflation. E aí a gente percebe no Inflation que em termos de Low Income, Upper Middle Income e Lower Middle Income elas estão equiparadas. Então se a gente aplicasse um teste de uma nova para avaliar a média, talvez a diferença não seria tão significativa das médias entre esses três grupos. A gente encontra também alguns outliers. Aqui a gente encontra outros outliers. A gente encontra a Argentina como outlier na parte de Upper Middle Income. A gente encontra a Mongólia no Lower Middle Income, que não tinha aparecido. E a Nigéria também com uma inflação bem alta. E aqui nos países que são como High Income, a gente tem países aqui da América do Sul, o Chile, temos o Kuwait, temos os Emirados Árabes Unidos, Oman, que também são países árabes, a Arábia Saudita, e Brunei também já tinha aparecido em uma das listas. E não aparece nenhum país como outlier nos países com baixa renda, considerados categorizados com baixa renda. A gente já percebe um comportamento diferente. Ou seja, esses três grupos estão bem equiparados em termos de inflação e países com maior desenvolvimento, maior renda, em geral possuem uma inflação mais baixa. Para fechar esse bloco de EDA, com essa parte de Box Plot, quero trazer uma visão agora da expectativa de vida. Então, qual seria se a expectativa de vida, a gente percebeu pelo outro gráfico, que parece que tudo está muito concentrado ali entre 70 e 80 anos, mas vamos olhar isso fatiado por Income Category. Então, vou colocar aqui Life Spec, que é a expectativa de vida. E aqui a gente percebe nitidamente uma diferença. Então, países com Low Income, com uma expectativa de vida mais baixa, o máximo aqui de 67 anos, tendo um outlier aqui com 47,5, que é a República Africana, República Central da África. Temos aqui outros países que aparecem, como por exemplo, a África do Sul, Botsuana, Namíbia, como Upper Middle Income. Temos aqui Lower Middle Income, então aqui também está seguindo um padrão, esse, depois esse, depois esse, depois esse, onde aparece aqui o Haiti e Lesotho. E temos aqui um país com High Income, também fora da curva, que é a Guiana. Ou seja, a Guiana, apesar de ela estar enquadrada como High Income, ela tem uma expectativa de 65,5 anos, que é praticamente a mediana dos países Lower Middle Income. Então, é considerado aqui um outlier, mas aqui a gente percebe que os países mais ricos, vamos pensar assim, com mais renda, tem uma expectativa de vida maior. Seguindo nessa linha ainda de análise bivariada, antes de partir para o PCA, quero trazer também uma outra visualização em termos de combinar as duas variáveis e avaliá-las ao longo do tempo, ao longo das categorias. Duas variáveis que tem uma correlação, parecem ter uma correlação forte, seriam Income e GDPP. Então, o que eu vou fazer aqui é um Scatter Plot, combinando essas duas variáveis no plano cartesiano, eu vou pintar a cor dos pontos com a cor da categoria para ver se a gente encontra algum padrão também de correlação entre essas variáveis. Então, vou usar aqui o PX Scatter, Scatter Plot, Death Countries, variável X eu vou colocar Income, a variável Y eu vou usar o GDPP, a cor vai ser dada pelo Income Category, e eu vou usar o Hover Data país. Então, a gente vai conseguir também, aqui no caso, a gente vai conseguir ver todos os pontinhos de dados, lógico que alguns vão ficar em cima dos outros, mas ao contrário do Box Plot, que você só consegue apontar para os Outliers, aqui você vai conseguir apontar para mais pontos e conseguir ver eles no gráfico, ver alguns Outliers, alguns pontos realmente fora da curva. Então, gerando esse plot aqui, a gente vê, bom, a maioria dos países que a gente viu estão no High Income, então a gente consegue ver o seguinte, sim, quanto maior o Income, há uma tendência, não é uma tendência, talvez não seja uma correlação linear, mas há uma correlação entre Income e GDPP, então quanto mais os países vão tendo de Income, mais GDP eles têm. A gente encontra aqui de novo alguns Outliers, então Qatar, com Income de 125, GDP de 70, Luxemburgo, com Income de 91, com GDP de 105, você tem Brunei, que tem um GDP baixo, um PIB baixo comparado aos demais, mas com um Income bem alto, aqui o Kuwait a mesma coisa, Singapura, então na faixa dos 50, enquanto esses aqui estão em uma faixa bem alta, e aqui vários países meio que misturados, também na faixa, e já mais equilibrados, entre GDP de 30 a 35, Income de 30 a 35, então a gente tem aqui o Chipre, a gente tem a Espanha, a gente tem a Nova Zelândia, a gente tem Israel, Bahamas, aqui um pouco mais para cima a Dinamarca, os países nórdicos, a Holanda, os Estados Unidos está aqui posicionado, e começa a ser o primeiro país que sai um pouco desse bolo aqui em termos de Income e GDP, então na faixa ali próximo dos 50, 1000 dólares tanto de Income como de GDP, Bahrein, aí olhando os países de Low Income, quase não aparecem aqui, mas a gente tem aqui o Níger, a gente tem o Haiti aqui bem baixo como Lower Middle Income, temos a Ucrânia, que também está como Lower Middle, e nos vermelhinhos, Upper Middle Income, Bielorrússia, o Brasil aparece aqui também como Upper Middle Income, na faixa de 11, no GDP de 14,5, em termos de Income de Renda Média do país. Então aqui é uma visão melhor para correlacionar essas duas variáveis e ver como elas também se comportam em comparação ou atreladas também à categoria que o país foi enquadrado em termos de Income. E aí para fechar essa parte do EDA, fazer uma matriz de correlação entre todas as variáveis numéricas. Então gerar uma matriz de correlação. Se a gente gerar essa matriz, eu gero uma variável, vou colocar numa variável, matriz, correlação, countries, e aí eu uso um método do próprio Pandas, que é o .core, e é uma coisa interessante, eu tenho aqui dentro do DataFrame variáveis que não são numéricas. Então como é que eu faço para ele só trazer a correlação das variáveis numéricas e não dar erro? Existe um parâmetro para isso, eu coloco numeric only igual a true. Então ele resolve pegar as variáveis numéricas que eu tenho. Gera essa matriz de correlação, e para que eu mostre esse Hitmap, essa matriz de uma forma mais interessante, nesse caso é melhor usar o Seaborn. O Seaborn traz uma... O Plotly faz isso, mas ele faz com muito mais, vamos dizer, você precisa de muito mais coisas para fazer um gráfico que fique bom dentro do Plotly. Nesse caso eu prefiro usar o Hitmap do Seaborn, onde eu informo a matriz, coloco a matriz, coloco aqui o vmin igual a menos um, que seria qual o valor mínimo que eu quero, eu quero de menos um a mais um, o vmax igual a um, e a note, que no caso é, eu quero que traga os valores dentro da tabela, que é a note igual a true. Então eu clico aqui, e aí ele vai mostrar para mim a matriz de correlação. Então ele mostra aqui algumas correlações negativas fortes, por exemplo, expectativa de vida com mortalidade infantil, enquanto uma sobe a outra desce, aqui também em relação à mortalidade infantil, a questão de fertilidade, então quanto a fertilidade sobe, também sobe a quantidade de filhos que uma mãe poderia ter, no caso dos níveis de fertilidade estarem ok, estarem constantes. A gente tem uma outra correlação aqui também forte, negativa de expectativa de vida com essa questão da quantidade de crianças por mulher. Temos uma outra aqui de income com mortalidade infantil, também negativa, quanto maior, menor o income, ou quanto menor o income, maior a mortalidade. Aquela que a gente estava analisando várias vezes, que é o income com GDP muito forte, o income com GDP 0.9, então realmente há uma forte relação entre o PIB per capita e a renda média da população. O que não vemos na questão da inflação, na inflação não tem nenhum tipo de correlação bastante forte com nenhuma outra variável, mas o income tem, com o GDP, com expectativa de vida, e o GDP também tem, com expectativa, o income moderado aqui com relação à exportação e mortalidade infantil. Então aqui confirma algumas coisas que a gente viu nos gráficos anteriores. A ideia de mostrar esses gráficos é tentar encontrar algumas relações e ver, comparar depois como que isso vai se traduzir na hora que a gente reduzir a dimensionalidade dos dados. Ou seja, hoje a gente tem esse conjunto de variáveis, a gente tem uma, duas, três, quatro, cinco, seis, sete, oito, nove variáveis, na verdade tem uma décima, que é a variável de income category, que a gente não vai usar, mas com nove variáveis, reduzindo isso para três, a gente vai avaliar se ele consegue ainda manter o agrupamento dos países, a gente consegue encontrar ali os clusters, aparecer os clusters mesmo com um conjunto, uma dimensão menor do que a gente está vendo aqui. Então encerramos aqui a parte do EDA, e vamos partir para a parte do PCA, vejo vocês no próximo vídeo.

### Bloco 5 - Treinamento do Módulo

#### Aula 12 - Treinamento do Algoritmo

Iniciando agora a parte do PCA propriamente dito, de rodarmos o algoritmo, seguindo aquilo que eu comentei em relação a parte teórica, o primeiro aspecto importante de a gente fazer dentro do PCA, o primeiro passo é a gente normalizar as variáveis, colocá-las dentro de uma escala normal para garantir que uma variável não puxe muito na hora do PCA, lembrando que ela não contribua mais do que as outras, a gente vai colocar todas essas variáveis em uma mesma escala, então vou colocar aqui um markdown, a gente vai começar agora a treinar o algoritmo PCA, a primeira coisa que a gente vai fazer é selecionar as variáveis que a gente vai usar, então eu vou fazer uma cópia do data set, do data frame, para manter o data frame original, vocês vão entender também para que a gente já fez isso na classeirização, então selecionar as colunas para o PCA, então eu vou fazer uma cópia em x do defcounters, então defcounters.copy, e vou remover desse x as colunas desnecessárias, que colunas são essas, por exemplo, eu não vou trazer colunas que sejam índices, colunas com valores únicos, como por exemplo o país, o país é um índice, é o que determina, e eu não vou levar ele para o PCA, porque isso vai gerar uma alta cardinalidade nos dados, então não vou ter diferenciação, então a coluna country é uma coluna que eu vou remover, uma outra coluna que eu vou remover é a coluna income category, e também essa coluna na verdade é uma coluna que eu poderia transformar ela em uma forma original, eu tenho lá os níveis, o low, o low income, o lower income, o upper middle income e o high income, então na verdade eu poderia codificar isso como sendo 0, 1, 2 e 3, aí depois desse 0, 1, 2 e 3, talvez eu teria que transformar ele para uma escala normal, para que ele também não puxasse muito em relação às outras variáveis, então como a ideia aqui, a minha ideia é a seguinte, eu quero treinar o algoritmo PCA sem levar em conta a questão da categoria do income, e saber se ele consegue capturar, agrupar esses dados em três dimensões, de forma que eu consiga ver a mesma coisa que eu vi nos gráficos dois a dois, nos gráficos bidimensionais de correlação, essa é a minha ideia, é a ideia que eu tenho aqui para poder apresentar, por exemplo, esse trabalho mostrando os países agrupados nessas três dimensões, então eu vou remover a coluna country e vou remover a coluna income category, aqui faltou o axis="to", então para ele já fazer a remoção e jogar no próprio x os dados, feito isso, eu preciso agora aplicar o transformer, aplicar uma transformação de dados nas variáveis numéricas, então aqui eu vou separar variáveis numéricas, ou quantitativas, vou colocar aqui numeric features, o mesmo conceito que eu já usei em alguns outros notebooks, e vou começar a relacionar quais são essas features, então são o Child Mort, mortalidade infantil, o Export, Health, Imports, o Income, o Inflation, o Life, Spec, Total Fare, e por último o GDPP, criei as variáveis, agora eu posso definir, definir transformações, eu vou criar um transformador aqui numeric transformer, standard scalar, e vou criar um preprocessor para esses dados, preprocessador de transformações, vou criar aqui um preprocessor, vou aplicar um column transformer, onde eu vou ter transformers, que é a variável que eu vou popular, e que é uma lista, então essa lista vai ter um transformer, que na verdade ele é configurado dessa forma, o nome dele num, numérico, qual é o transformer que eu vou criar, e em que features ele vai ser aplicado, numeric features, pronto, criei o preprocessor, agora eu posso transformar os dados, então, transformar os dados, nesses dados eu vou aplicar o PCI, então vou chamar de X-Transformer, vou aplicar o preprocessor e vou dar um Fit Transform, ou seja, ele treina o transformador e já transforma os dados com base no meu DataFrame X, tá, criei aqui o X-Transformer, e agora eu posso visualizar esse X-Transformer, então se pegar aqui o X-Transformer, tá aqui todos os números, os valores já levados para a escala, para uma estándar esquerda, para uma escala normal, para não ter nenhuma variável puxando, vamos dizer, carregando muito o modelo do PCI, uma coisa só a título de curiosidade, é que eu comentei que dentro dos processos que ele faz, o PCI faz, para poder calcular o PCI, realizar o PCI, uma dessas, uma das coisas que ele faz é calcular uma matriz de covariança, tá, então aqui, vou deixar isso aqui no córrego, que é interessante, porque se em algum momento, em algum outro, existem vários algoritmos que dependem da matriz de covariança, tá, aí se eu quisesse calcular uma matriz de covariança para aplicar essa matriz de covariança num cálculo matemático, ou se eu quisesse desenvolver o PCI, ou desenvolver o SVD, o Single Value Decomposition, usando uma matriz de covariança, como é que eu calculo essa matriz de covariança? Então, eu uso um comando numpy, np, eu já importei ele lá em cima, e existe um método chamado cov, que é de covariança, e aí eu coloco aqui qual é o meu array de dados para fazer essa matriz, então se eu fizer isso, ele vai mostrar aqui a matriz de covariança com base nos meus dados já transformados na escala normal, tá, então eu só fiquei com uma curiosidade, uma forma para vocês obterem essa matriz de covariança em qualquer necessidade que tiverem em outros algoritmos. E agora, vamos fazer a criação do modelo PCA, né, do modelo PCA, a gente vai criar aqui uma variável chamada modelo PCA, e eu vou fazer um PCA com 3 componentes, que foi o que eu coloquei no enunciado do projeto, ou seja, eu quero mostrar um gráfico tridimensional, eu estou saindo de 9 variáveis para 3 variáveis, então estou reduzindo para 1 terço das variáveis a apresentação dos meus dados, eu vou encontrar 3 componentes principais das 9 variáveis que foram apresentadas, então eu crio esse modelo, executo agora o PCA propriamente dito, o algoritmo, então eu venho aqui e coloco, e vou jogar isso em uma outra variável chamada XPCA, e eu vou executar, modelo PCA, ponto, fit transform, aqui não tem um predict, você vai fazer um fit, e já vai transformar o dado para dentro daquilo que você precisa, vejam que aqui não tem um predict, então aqui é fit transform, e vou pegar o XTransformer como entrada. Executei o PCA, e aí, qual é o ideal agora? A gente poder pegar esses componentes principais que ele obteve, vou chamar de PC1, PC2 e PC3, e incorporar eles dentro do meu DataFrame original, mas para isso eu vou primeiro gerar um DataFrame desse resultado, então, vou criar aqui, gerar um DataFrame com base nos componentes principais, então vou chamar de DFPCA, igual a PDDataFrame, e eu vou ler o XPCA, e vou dar o nome das colunas, colunas, vou chamar PC1, principal componente 1, principal componente 2, principal componente 3, girei, se eu quiser até olhar esse DataFrame, DFEdge10, ele mostra aqui, PC1, PC2 e PC3, ele reduziu a dimensionalidade para esses três componentes principais, vou colocar aqui, visualizar componentes principais, e agora eu vou incluir, como eles estão na ordem, se eu der um len aqui, verificar o tamanho do DataFrame, se eu colocar aqui, len de DFPCA, DFPCA, está minúsculo, DFPCA, ele tem as mesmas 165 registros, então ele está alinhado, então agora a gente faz o que, incluir colunas, então posso aqui incluir componentes principais no DataFrame original, então vou fazer o que, DFCountries de PC1, é igual a DFPCA de PC1, e vou fazer a mesma coisa, o PC2 e o PC3, então estou levando esses três componentes principais, que é a representação nesse espaço de dimensão menor, para dentro do DataFrame original, para eu correlacionar também com os países, e com a informação também de Income Category, gerei isso, então se eu vier aqui agora, mostrar o DataFrame original, com componentes principais, DFCountries.head e 10, ele está mostrando os mesmos dados, aqui no final ele está colocando PC1, PC2 e PC3, então com isso, a gente treinou o modelo, preparou os dados, treinamos o modelo do PCA e incorporamos os dados que, resultantes do PCA, das três dimensões que a gente escolheu, os três componentes principais, dentro do DataFrame original, e agora a gente na sequência vai mostrar, visualizar os dados com base nesses componentes principais, e vamos fazer calcular uma métrica, usar uma das métricas que foram apresentadas na parte teórica, para mostrar se ele conseguiu capturar bem, se essa redução foi boa no ponto de vista de erro de reconstrução de dados, que foi a primeira métrica que eu trouxe na parte teórica, então vejo vocês no próximo vídeo.

### Bloco 6 - Análise dos Resultados

#### Aula 13 - Visualização dos Resultados

Para que a gente possa agora cumprir com aquilo que a gente colocou como projeto, que era dar uma visualização simples, e aqui no caso a gente vai usar uma visualização 3D para ver os países, plotar ali o Income Category dentro de 3 componentes, que foram os escolhidos que a gente escolheu, eu vou criar uma outra seção aqui no nosso notebook chamada visualização de dados, então vou colocar aqui, visualizar resultados, uma coisa que eu gostaria de trazer da mesma forma que a matriz de covariança, de como calcular a matriz de covariança, o próximo passo depois da matriz de covariança é que ela é usada para calcular os autovetores e os autovalores, ou os End Vectors e End Values, mas e se eu quisesse obter os End Vectors e os End Vectors do algoritmo que eu acabei de treinar, como é que eu obtenho isso? Então vou mostrar aqui também, autovalores, para obter os autovalores, então vou criar até uma variável aqui, autovalores, eu vou lá no meu modelo, modelo OPCA, eu tenho uma propriedade chamada Explained Variance, lembrando que os autovalores são a variância que ele capturou para cada um dos autovetores, onde ele ordena aqueles autovetores que tiveram a maior variância nos seus autovalores, e aqui autovetores, para obter os autovetores que são os componentes principais, os valores que representam esses componentes, a gente tem aqui modelo OPCA.componentes, é uma outra propriedade, e eu vou imprimir essas duas informações, vou colocar primeiro autovetores, dar um print aqui, dar um print, autovetores, depois eu vou dar um print aqui, eu não estou fazendo porque eu estou no markdown aqui, só vou trocar aqui para Python, e aí eu vou dar aqui um print, vou dar um salto de linha, barra N, autovalores, print autovalores, vou fechar esse parênteses aqui, e ele mostra aqui, os autovetores, na verdade eu inverti, autovalores, autovetores, autovetores, autovalores, eu inverti aqui, que na verdade os autovetores são os 3, aqui explained variance, na verdade eu inverti aqui, são 3 autovetores, autovetores, autovalores, 4.18, 1.55 e 115, e os autovalores que são os valores das varianças dentro de cada um dos autovetores, então só corrigindo aqui e invertendo as nomenclaturas. Podemos agora partir para mostrar um gráfico 3D desses componentes principais, como é que a gente vai fazer esse gráfico, ele é bem simples de fazer usando o Plotly, então a gente vai mostrar um chart 3D com os componentes principais, então aqui eu preciso criar uma figura do Matplotlib, onde eu vou colocar o px scatter 3D, então isso é um diagrama de pontos 3D, onde eu coloco o countries, qual o eixo x, eu vou colocar aqui o PC1, o eixo y o PC2, e o eixo z o PC3, aí eu coloco o color, vai ser o Income Category para ver como é que os países estão se agrupando nesse conjunto de 3 dimensões, vou colocar um título, título visualização PCA, vou colocar um gráfico grande, então vou colocar um widget aqui de 800x600 e Height 600, e vou colocar um HoverData para mostrar o país, agora eu preciso dar um show, fig.show, e ele mostra aqui um gráfico 3D, como ele monta de primeira não fica estranho, mas na verdade o que ele está montando aqui é um cubo, é quase que um cubo, então você consegue ver aqui em 3 dimensões os países, percebam que sim, os países de High Income estão ali meio que agrupados, então essa informação da categoria não foi codificada na hora de gerar o PDA, o PCA, mas na hora que ela aparece aqui de forma nítida, ela foi capturada mesmo com 3 dimensões, então vejam que os países aqui estão todos próximos, é interessante que a gente vê o PC, vê o Income Category e vê quais são os países, a gente consegue ver de novo os Outliers aqui, Luxemburgo, Singapura, Qatar, Noruega, Suíça, Kuwait, Brunei, Emirados Árabes, Estados Unidos, a gente vê aqui um Outlier do Lower Middle Income que é a Nigéria, que a gente já tinha visto, o Haiti, a gente vê aqui no Low Income Liberia, Rwanda, Togo, você consegue girar esse gráfico aqui, essas 3 dimensões, e bem misturado, na verdade o Lower é o que está mais misturado e o Vermelho, que é o Upper Middle Income, onde está o Brasil, também está aqui, está bem aglutinado, então você tem aqui Geórgia, Bosa-Hergovina, Costa Rica, Sérvia, Turquia, Tunísia, se você girar o gráfico aqui, você pega os países que estão aqui embaixo, você pode girar aqui, pegar aqui os de cima, olhar por cima, então dessa forma você consegue ver se com essas 3 dimensões esse Income Category foi bem representado e está bem aglutinado. Como ponto final dessa parte de visualização dos resultados, a gente vai olhar uma métrica, conseguir avaliar se a gente reconstruindo os dados a partir do PCA e comparando esses dados reconstruídos com os dados originais, no caso já os transformados, já na Standard Scalar, qual a diferença desses dados para ver se ele conseguiu fazer uma boa reconstrução. Como que a gente faz isso? Então reconstruir os dados com base no PCA, então eu vou chamar de X-Recovered, recuperado, e aí eu tenho um método dentro do modelo que chama-se Inverse Transform, onde eu pego o meu PCA, que é aquele array com 3 colunas, que são os componentes principais, e eu retorno ele para aquilo que eu chamo de Recovered, então faço isso aqui, se eu quiser visualizar o Recovered, então visualizar, Recovered, X-Recovered, veja que ele trouxe os dados de novo para o mesmo conjunto de dimensões que tinham originalmente, se eu colocar aqui o shape 0, shape 0, 165, shape de 1, 9 colunas que foi o que entrou, então ele voltou para as 9 colunas. E aí como é que eu recalculo o erro de reconstrução? Basicamente eu vou fazer uma medida do erro do dado recuperado versus o dado transformado e ver qual é a diferença desses dois valores, então calcular o erro de reconstrução. Então eu vou chamar isso aqui de ReconstructionError, e eu vou fazer o que, uma MinSquaredError do X-Transformed, que é o que entrou no meu PCA, versus o X-Recovered, que é aquele reconstruído. E vou dar um print aqui, print erro de reconstrução, dois pontos, ReconstructionError. Ele mostra um erro aqui de reconstrução de 0.23, que é um erro considerado muito baixo, ou seja, ele conseguiu recuperar bastante das informações fazendo o caminho inverso, saindo do PCA e voltando para os dados originais. Uma curiosidade, uma coisa que a gente pode fazer aqui só para avaliar como ficaria, sem muitas mexidas no código, é avaliar se a gente estivesse com duas dimensões. Aqui eu fiz um gráfico de três dimensões, ou seja, eu trouxe aqui para três dimensões, mas como seria se fossem duas dimensões? Então a gente mudaria o que do código, para não ter que reconstruir o código na necessidade, na verdade a única coisa que a gente tem que medir aqui é a gente poder colocar o PCA igual a 2, então a gente tem um PCA com dois componentes, a gente cria um modelo novo com dois componentes, a gente vai fazer o PCA em cima do Transformer, da mesma forma, ao invés de gerar um DataFrame com três colunas, a gente vai gerar um DataFrame com duas, então só para manter aqui o outro, eu vou comentar aqui essa linha, e vou deixar ele criar um DataFrame com dois componentes principais, vou visualizar esses dois componentes, aqui eu vou comentar essa linha e vou colocar no DFCountries apenas PC1 e PC2, porém tem um problema, o PC1 e PC2 já existem essas colunas lá, então eu vou colocar e vai sobrepor, aqui a PC3 vai continuar aparecendo da outra vez, mas a gente não vai usar ela, e vamos visualizar os resultados, se a gente visualizar os resultados aqui, aparecem dois autovetores com seus respectivos altos valores, e aqui ao invés de eu fazer um Chart 3D, eu posso fazer um Chart 2D, eu posso fazer um Scatter normal, um Scatter, colocando o DFCountries, o X vai ser PC1, o Y vai ser PC2, ai eu posso usar o Color como o Income, Category, posso usar o HoverData igual a Country, deixa eu colocar aqui, gráfico 2D, ou seja, um PCA com dois componentes principais, vou dar de novo, então aqui ele já consegue mostrar, mas aqui já vê que os dados já ficam bem mais misturados, a gente já vê o Lower aqui misturado dentro dos eixos, o Azul misturado, o que a gente vê é uma separação mais clara do conjunto do High Income, dos países mais ricos, até o próprio vermelho que é o Upper, Middle Income, ele está concentrado aqui, mas ele espalha um pouco aqui, o que dificulta um pouco a visualização, da mesma forma a gente pode fazer um teste aqui, fazer um Recovered para os dados originais, temos aqui as nove colunas, as nove colunas originais, e a gente percebe o seguinte, com dois componentes principais o erro de reconstrução aumenta, ou seja, representando com dois componentes principais, não é tão bom quanto representar com três, e provavelmente se a gente fosse colocando mais componentes principais, esse erro de reconstrução iria caindo. Então, fecho aqui essa parte, aliás, antes de fechar vamos salvar esse modelo, a gente já pode fazer o Save do modelo, então, colocar aqui um código, colocar um Markdown na verdade, aqui primeiro, salvar modelo, e aí para salvar esse modelo a gente faz o que? Dá um Import JobLib, e aí a gente usa aqui, salvar modelo, salvar modelo. A gente está com o modelo de duas, tá? Eu vou deixar o dois, mas só por jeito de curiosidade, se você não quiser fazer a simulação com dois, você deixa com três, e salva o modelo com três. Modelo PCA, vou salvar aqui como ModeloPCACountries.pickle, e vou salvar também o preprocessor, JobLib.dump preprocessor PCACountries.pickle, salvar os dois modelos, se eu olhar aqui nos dados, eu tenho aqui o modelo PCA e o preprocessor. Com isso, então, fechamos esse projeto prático para demonstrar o algoritmo de PCA, a ideia foi mostrar como que o PCA ajuda na identificação, na visualização melhor dos dados para outros algoritmos ou para análises avançadas, ou até com o objetivo de poder até clusterizar algumas coisas usando o algoritmo, não é o foco dele, o foco dele é reduzir dimensionalidade, mas a gente conseguiu aqui avaliar o quanto ele conseguiu, de uma certa forma, agrupar algumas coisas com três dimensões, de forma que eu consiga ver de forma bem separada as características desses países com base na coluna Income Category, que foi uma coluna que a gente avaliou como importante para dentro da classificação dos países que essa empresa de fast food vai querer montar suas novas lojas. Então, concluo aqui mais esse módulo, vejo vocês no próximo módulo.

> [voltar](../../../README.md) para a página anterior.